//
// Generated by LLVM NVPTX Back-End
//

.version 8.7
.target sm_90a
.address_size 64

	// .globl	_dropout                // -- Begin function _dropout
                                        // @_dropout
.visible .entry _dropout(
	.param .u64 .ptr .global .align 1 _dropout_param_0,
	.param .u64 .ptr .global .align 1 _dropout_param_1,
	.param .u64 .ptr .global .align 1 _dropout_param_2,
	.param .u32 _dropout_param_3,
	.param .f32 _dropout_param_4,
	.param .u64 .ptr .global .align 1 _dropout_param_5,
	.param .u64 .ptr .global .align 1 _dropout_param_6
)
.reqntid 128
{
	.reg .pred 	%p<33>;
	.reg .b32 	%r<50>;
	.reg .b64 	%rd<29>;

// %bb.0:
	ld.param.b64 	%rd25, [_dropout_param_0];
	ld.param.b64 	%rd26, [_dropout_param_1];
	mov.u32 	%r25, %ctaid.x;
	shl.b32 	%r26, %r25, 10;
	ld.param.b64 	%rd27, [_dropout_param_2];
	ld.param.b32 	%r27, [_dropout_param_3];
	mov.u32 	%r28, %tid.x;
	shl.b32 	%r29, %r28, 2;
	ld.param.b32 	%r30, [_dropout_param_4];
	and.b32 	%r31, %r29, 508;
	or.b32 	%r32, %r31, %r26;
	or.b32 	%r33, %r32, 1;
	or.b32 	%r34, %r32, 2;
	or.b32 	%r35, %r32, 3;
	or.b32 	%r36, %r32, 512;
	or.b32 	%r37, %r32, 513;
	or.b32 	%r38, %r32, 514;
	or.b32 	%r39, %r32, 515;
	setp.lt.s32 	%p1, %r32, %r27;
	setp.lt.s32 	%p2, %r33, %r27;
	setp.lt.s32 	%p3, %r34, %r27;
	setp.lt.s32 	%p4, %r35, %r27;
	setp.lt.s32 	%p5, %r36, %r27;
	setp.lt.s32 	%p6, %r37, %r27;
	setp.lt.s32 	%p7, %r38, %r27;
	setp.lt.s32 	%p8, %r39, %r27;
	mul.wide.s32 	%rd28, %r32, 4;
	add.s64 	%rd1, %rd25, %rd28;
	add.s64 	%rd2, %rd1, 4;
	add.s64 	%rd3, %rd1, 8;
	add.s64 	%rd4, %rd1, 12;
	add.s64 	%rd5, %rd1, 2048;
	add.s64 	%rd6, %rd1, 2052;
	add.s64 	%rd7, %rd1, 2056;
	add.s64 	%rd8, %rd1, 2060;
	// begin inline asm
	mov.u32 %r1, 0x0;
	@%p1 ld.global.b32 { %r1 }, [ %rd1 + 0 ];
	// end inline asm
	// begin inline asm
	mov.u32 %r2, 0x0;
	@%p2 ld.global.b32 { %r2 }, [ %rd2 + 0 ];
	// end inline asm
	// begin inline asm
	mov.u32 %r3, 0x0;
	@%p3 ld.global.b32 { %r3 }, [ %rd3 + 0 ];
	// end inline asm
	// begin inline asm
	mov.u32 %r4, 0x0;
	@%p4 ld.global.b32 { %r4 }, [ %rd4 + 0 ];
	// end inline asm
	// begin inline asm
	mov.u32 %r5, 0x0;
	@%p5 ld.global.b32 { %r5 }, [ %rd5 + 0 ];
	// end inline asm
	// begin inline asm
	mov.u32 %r6, 0x0;
	@%p6 ld.global.b32 { %r6 }, [ %rd6 + 0 ];
	// end inline asm
	// begin inline asm
	mov.u32 %r7, 0x0;
	@%p7 ld.global.b32 { %r7 }, [ %rd7 + 0 ];
	// end inline asm
	// begin inline asm
	mov.u32 %r8, 0x0;
	@%p8 ld.global.b32 { %r8 }, [ %rd8 + 0 ];
	// end inline asm
	add.s64 	%rd9, %rd26, %rd28;
	add.s64 	%rd10, %rd9, 4;
	add.s64 	%rd11, %rd9, 8;
	add.s64 	%rd12, %rd9, 12;
	add.s64 	%rd13, %rd9, 2048;
	add.s64 	%rd14, %rd9, 2052;
	add.s64 	%rd15, %rd9, 2056;
	add.s64 	%rd16, %rd9, 2060;
	// begin inline asm
	mov.u32 %r9, 0x0;
	@%p1 ld.global.b32 { %r9 }, [ %rd9 + 0 ];
	// end inline asm
	// begin inline asm
	mov.u32 %r10, 0x0;
	@%p2 ld.global.b32 { %r10 }, [ %rd10 + 0 ];
	// end inline asm
	// begin inline asm
	mov.u32 %r11, 0x0;
	@%p3 ld.global.b32 { %r11 }, [ %rd11 + 0 ];
	// end inline asm
	// begin inline asm
	mov.u32 %r12, 0x0;
	@%p4 ld.global.b32 { %r12 }, [ %rd12 + 0 ];
	// end inline asm
	// begin inline asm
	mov.u32 %r13, 0x0;
	@%p5 ld.global.b32 { %r13 }, [ %rd13 + 0 ];
	// end inline asm
	// begin inline asm
	mov.u32 %r14, 0x0;
	@%p6 ld.global.b32 { %r14 }, [ %rd14 + 0 ];
	// end inline asm
	// begin inline asm
	mov.u32 %r15, 0x0;
	@%p7 ld.global.b32 { %r15 }, [ %rd15 + 0 ];
	// end inline asm
	// begin inline asm
	mov.u32 %r16, 0x0;
	@%p8 ld.global.b32 { %r16 }, [ %rd16 + 0 ];
	// end inline asm
	mov.b32 	%r40, 0f3F800000;
	sub.f32 	%r41, %r40, %r30;
	div.full.f32 	%r42, %r1, %r41;
	div.full.f32 	%r43, %r2, %r41;
	div.full.f32 	%r44, %r3, %r41;
	div.full.f32 	%r45, %r4, %r41;
	div.full.f32 	%r46, %r5, %r41;
	div.full.f32 	%r47, %r6, %r41;
	div.full.f32 	%r48, %r7, %r41;
	div.full.f32 	%r49, %r8, %r41;
	setp.eq.b32 	%p25, %r9, 0;
	setp.eq.b32 	%p26, %r10, 0;
	setp.eq.b32 	%p27, %r11, 0;
	setp.eq.b32 	%p28, %r12, 0;
	setp.eq.b32 	%p29, %r13, 0;
	setp.eq.b32 	%p30, %r14, 0;
	setp.eq.b32 	%p31, %r15, 0;
	setp.eq.b32 	%p32, %r16, 0;
	add.s64 	%rd17, %rd27, %rd28;
	add.s64 	%rd18, %rd17, 4;
	add.s64 	%rd19, %rd17, 8;
	add.s64 	%rd20, %rd17, 12;
	add.s64 	%rd21, %rd17, 2048;
	add.s64 	%rd22, %rd17, 2052;
	add.s64 	%rd23, %rd17, 2056;
	add.s64 	%rd24, %rd17, 2060;
	selp.b32 	%r17, 0, %r42, %p25;
	// begin inline asm
	@%p1 st.global.b32 [ %rd17 + 0 ], { %r17 };
	// end inline asm
	selp.b32 	%r18, 0, %r43, %p26;
	// begin inline asm
	@%p2 st.global.b32 [ %rd18 + 0 ], { %r18 };
	// end inline asm
	selp.b32 	%r19, 0, %r44, %p27;
	// begin inline asm
	@%p3 st.global.b32 [ %rd19 + 0 ], { %r19 };
	// end inline asm
	selp.b32 	%r20, 0, %r45, %p28;
	// begin inline asm
	@%p4 st.global.b32 [ %rd20 + 0 ], { %r20 };
	// end inline asm
	selp.b32 	%r21, 0, %r46, %p29;
	// begin inline asm
	@%p5 st.global.b32 [ %rd21 + 0 ], { %r21 };
	// end inline asm
	selp.b32 	%r22, 0, %r47, %p30;
	// begin inline asm
	@%p6 st.global.b32 [ %rd22 + 0 ], { %r22 };
	// end inline asm
	selp.b32 	%r23, 0, %r48, %p31;
	// begin inline asm
	@%p7 st.global.b32 [ %rd23 + 0 ], { %r23 };
	// end inline asm
	selp.b32 	%r24, 0, %r49, %p32;
	// begin inline asm
	@%p8 st.global.b32 [ %rd24 + 0 ], { %r24 };
	// end inline asm
	ret;
                                        // -- End function
}
