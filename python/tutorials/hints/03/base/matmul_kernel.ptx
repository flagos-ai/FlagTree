//
// Generated by LLVM NVPTX Back-End
//

.version 8.7
.target sm_90a
.address_size 64

	// .globl	matmul_kernel           // -- Begin function matmul_kernel
.extern .shared .align 16 .b8 global_smem[];
                                        // @matmul_kernel
.visible .entry matmul_kernel(
	.param .u64 .ptr .global .align 1 matmul_kernel_param_0,
	.param .u64 .ptr .global .align 1 matmul_kernel_param_1,
	.param .u64 .ptr .global .align 1 matmul_kernel_param_2,
	.param .u32 matmul_kernel_param_3,
	.param .u32 matmul_kernel_param_4,
	.param .u32 matmul_kernel_param_5,
	.param .u32 matmul_kernel_param_6,
	.param .u32 matmul_kernel_param_7,
	.param .u32 matmul_kernel_param_8,
	.param .u64 .ptr .global .align 1 matmul_kernel_param_9,
	.param .u64 .ptr .global .align 1 matmul_kernel_param_10
)
.reqntid 128
{
	.reg .pred 	%p<66>;
	.reg .b32 	%r<1505>;
	.reg .b64 	%rd<136>;

// %bb.0:
	ld.param.b32 	%r223, [matmul_kernel_param_8];
	ld.param.b32 	%r222, [matmul_kernel_param_5];
	ld.param.b32 	%r221, [matmul_kernel_param_4];
	ld.param.b32 	%r220, [matmul_kernel_param_3];
	ld.param.b64 	%rd40, [matmul_kernel_param_2];
	ld.param.b64 	%rd39, [matmul_kernel_param_1];
	ld.param.b64 	%rd38, [matmul_kernel_param_0];
	mov.u32 	%r297, %ctaid.x;
	add.s32 	%r298, %r220, 63;
	shr.s32 	%r299, %r298, 31;
	shr.u32 	%r300, %r299, 26;
	add.s32 	%r301, %r298, %r300;
	shr.s32 	%r302, %r301, 6;
	add.s32 	%r303, %r221, 127;
	shr.s32 	%r304, %r303, 31;
	shr.u32 	%r305, %r304, 25;
	add.s32 	%r306, %r303, %r305;
	shr.s32 	%r307, %r306, 7;
	shl.b32 	%r309, %r307, 3;
	ld.param.b32 	%r310, [matmul_kernel_param_6];
	ld.param.b32 	%r311, [matmul_kernel_param_7];
	div.s32 	%r312, %r297, %r309;
	shl.b32 	%r313, %r312, 3;
	sub.s32 	%r314, %r302, %r313;
	min.s32 	%r316, %r314, 8;
	mul.lo.s32 	%r317, %r312, %r309;
	sub.s32 	%r318, %r297, %r317;
	div.s32 	%r319, %r318, %r316;
	mul.lo.s32 	%r320, %r319, %r316;
	sub.s32 	%r321, %r318, %r320;
	add.s32 	%r322, %r321, %r313;
	shl.b32 	%r1, %r322, 6;
	mov.u32 	%r2, %tid.x;
	and.b32 	%r4, %r2, 120;
	bfe.u32 	%r323, %r2, 3, 4;
	and.b32 	%r324, %r2, 112;
	bfe.u32 	%r5, %r2, 4, 3;
	or.b32 	%r6, %r5, 8;
	or.b32 	%r7, %r5, 16;
	or.b32 	%r8, %r5, 24;
	or.b32 	%r9, %r5, 32;
	or.b32 	%r10, %r5, 40;
	or.b32 	%r11, %r5, 48;
	or.b32 	%r12, %r5, 56;
	or.b32 	%r325, %r1, %r323;
	or.b32 	%r326, %r325, 16;
	or.b32 	%r327, %r325, 32;
	or.b32 	%r328, %r325, 48;
	rem.s32 	%r329, %r325, %r220;
	rem.s32 	%r330, %r326, %r220;
	rem.s32 	%r331, %r327, %r220;
	rem.s32 	%r332, %r328, %r220;
	shl.b32 	%r333, %r319, 7;
	shl.b32 	%r334, %r2, 3;
	and.b32 	%r335, %r334, 120;
	or.b32 	%r13, %r333, %r335;
	rem.s32 	%r336, %r13, %r221;
	and.b32 	%r337, %r2, 7;
	shl.b32 	%r14, %r337, 3;
	mad.lo.s32 	%r338, %r329, %r310, %r14;
	mad.lo.s32 	%r339, %r330, %r310, %r14;
	mad.lo.s32 	%r340, %r331, %r310, %r14;
	mad.lo.s32 	%r341, %r332, %r310, %r14;
	mad.wide.s32 	%rd41, %r338, 2, %rd38;
	mad.wide.s32 	%rd42, %r339, 2, %rd38;
	mad.wide.s32 	%rd43, %r340, 2, %rd38;
	mad.wide.s32 	%rd44, %r341, 2, %rd38;
	shl.b32 	%r342, %r311, 3;
	mad.lo.s32 	%r343, %r311, %r5, %r336;
	add.s32 	%r344, %r343, %r342;
	add.s32 	%r345, %r344, %r342;
	add.s32 	%r346, %r345, %r342;
	add.s32 	%r347, %r346, %r342;
	add.s32 	%r348, %r347, %r342;
	add.s32 	%r349, %r348, %r342;
	add.s32 	%r350, %r349, %r342;
	mad.wide.s32 	%rd45, %r343, 2, %rd39;
	mad.wide.s32 	%rd46, %r344, 2, %rd39;
	mad.wide.s32 	%rd47, %r345, 2, %rd39;
	mad.wide.s32 	%rd48, %r346, 2, %rd39;
	mad.wide.s32 	%rd49, %r347, 2, %rd39;
	mad.wide.s32 	%rd50, %r348, 2, %rd39;
	mad.wide.s32 	%rd51, %r349, 2, %rd39;
	mad.wide.s32 	%rd52, %r350, 2, %rd39;
	add.s32 	%r351, %r222, 63;
	shl.b32 	%r355, %r311, 6;
	setp.lt.s32 	%p1, %r351, 64;
	setp.gt.s32 	%p2, %r351, 63;
	setp.lt.s32 	%p3, %r14, %r222;
	shl.b32 	%r356, %r2, 4;
	and.b32 	%r357, %r356, 2032;
	shl.b32 	%r358, %r2, 1;
	and.b32 	%r359, %r358, 112;
	xor.b32 	%r16, %r357, %r359;
	mov.b32 	%r360, global_smem;
	add.s32 	%r361, %r360, %r16;
	add.s32 	%r224, %r361, 65536;
	selp.b32 	%r362, 16, 0, %p2;
	selp.b32 	%r227, %r362, 0, %p3;
	// begin inline asm
	cp.async.cg.shared.global [ %r224 + 0 ], [ %rd41 + 0 ], 0x10, %r227;
	// end inline asm
	add.s32 	%r226, %r361, 67584;
	// begin inline asm
	cp.async.cg.shared.global [ %r226 + 0 ], [ %rd42 + 0 ], 0x10, %r227;
	// end inline asm
	add.s32 	%r228, %r361, 69632;
	// begin inline asm
	cp.async.cg.shared.global [ %r228 + 0 ], [ %rd43 + 0 ], 0x10, %r227;
	// end inline asm
	add.s32 	%r230, %r361, 71680;
	// begin inline asm
	cp.async.cg.shared.global [ %r230 + 0 ], [ %rd44 + 0 ], 0x10, %r227;
	// end inline asm
	cp.async.commit_group;
	setp.lt.s32 	%p4, %r5, %r222;
	setp.lt.s32 	%p5, %r6, %r222;
	setp.lt.s32 	%p6, %r7, %r222;
	setp.lt.s32 	%p7, %r8, %r222;
	setp.lt.s32 	%p8, %r9, %r222;
	setp.lt.s32 	%p9, %r10, %r222;
	setp.lt.s32 	%p10, %r11, %r222;
	setp.lt.s32 	%p11, %r12, %r222;
	shl.b32 	%r363, %r337, 4;
	shl.b32 	%r364, %r324, 3;
	shl.b32 	%r365, %r2, 10;
	and.b32 	%r366, %r365, 8192;
	or.b32 	%r367, %r363, %r364;
	xor.b32 	%r368, %r367, %r324;
	or.b32 	%r17, %r368, %r366;
	add.s32 	%r232, %r360, %r17;
	selp.b32 	%r233, %r362, 0, %p4;
	// begin inline asm
	cp.async.cg.shared.global [ %r232 + 0 ], [ %rd45 + 0 ], 0x10, %r233;
	// end inline asm
	add.s32 	%r234, %r232, 1024;
	selp.b32 	%r235, %r362, 0, %p5;
	// begin inline asm
	cp.async.cg.shared.global [ %r234 + 0 ], [ %rd46 + 0 ], 0x10, %r235;
	// end inline asm
	add.s32 	%r236, %r232, 2048;
	selp.b32 	%r237, %r362, 0, %p6;
	// begin inline asm
	cp.async.cg.shared.global [ %r236 + 0 ], [ %rd47 + 0 ], 0x10, %r237;
	// end inline asm
	add.s32 	%r238, %r232, 3072;
	selp.b32 	%r239, %r362, 0, %p7;
	// begin inline asm
	cp.async.cg.shared.global [ %r238 + 0 ], [ %rd48 + 0 ], 0x10, %r239;
	// end inline asm
	add.s32 	%r240, %r232, 4096;
	selp.b32 	%r241, %r362, 0, %p8;
	// begin inline asm
	cp.async.cg.shared.global [ %r240 + 0 ], [ %rd49 + 0 ], 0x10, %r241;
	// end inline asm
	add.s32 	%r242, %r232, 5120;
	selp.b32 	%r243, %r362, 0, %p9;
	// begin inline asm
	cp.async.cg.shared.global [ %r242 + 0 ], [ %rd50 + 0 ], 0x10, %r243;
	// end inline asm
	add.s32 	%r244, %r232, 6144;
	selp.b32 	%r245, %r362, 0, %p10;
	// begin inline asm
	cp.async.cg.shared.global [ %r244 + 0 ], [ %rd51 + 0 ], 0x10, %r245;
	// end inline asm
	add.s32 	%r246, %r232, 7168;
	selp.b32 	%r247, %r362, 0, %p11;
	// begin inline asm
	cp.async.cg.shared.global [ %r246 + 0 ], [ %rd52 + 0 ], 0x10, %r247;
	// end inline asm
	cp.async.commit_group;
	setp.gt.s32 	%p12, %r351, 127;
	add.s64 	%rd53, %rd41, 128;
	add.s64 	%rd54, %rd42, 128;
	add.s64 	%rd55, %rd43, 128;
	add.s64 	%rd56, %rd44, 128;
	mul.wide.s32 	%rd77, %r355, 2;
	add.s64 	%rd57, %rd45, %rd77;
	add.s64 	%rd58, %rd46, %rd77;
	add.s64 	%rd59, %rd47, %rd77;
	add.s64 	%rd60, %rd48, %rd77;
	add.s64 	%rd61, %rd49, %rd77;
	add.s64 	%rd62, %rd50, %rd77;
	add.s64 	%rd63, %rd51, %rd77;
	add.s64 	%rd64, %rd52, %rd77;
	add.s32 	%r369, %r222, -64;
	setp.lt.s32 	%p13, %r14, %r369;
	bar.sync 	0;
	add.s32 	%r248, %r361, 73728;
	selp.b32 	%r370, 16, 0, %p13;
	selp.b32 	%r251, %r370, 0, %p12;
	// begin inline asm
	cp.async.cg.shared.global [ %r248 + 0 ], [ %rd53 + 0 ], 0x10, %r251;
	// end inline asm
	add.s32 	%r250, %r361, 75776;
	// begin inline asm
	cp.async.cg.shared.global [ %r250 + 0 ], [ %rd54 + 0 ], 0x10, %r251;
	// end inline asm
	add.s32 	%r252, %r361, 77824;
	// begin inline asm
	cp.async.cg.shared.global [ %r252 + 0 ], [ %rd55 + 0 ], 0x10, %r251;
	// end inline asm
	add.s32 	%r254, %r361, 79872;
	// begin inline asm
	cp.async.cg.shared.global [ %r254 + 0 ], [ %rd56 + 0 ], 0x10, %r251;
	// end inline asm
	cp.async.commit_group;
	setp.lt.s32 	%p14, %r5, %r369;
	setp.lt.s32 	%p15, %r6, %r369;
	setp.lt.s32 	%p16, %r7, %r369;
	setp.lt.s32 	%p17, %r8, %r369;
	setp.lt.s32 	%p18, %r9, %r369;
	setp.lt.s32 	%p19, %r10, %r369;
	setp.lt.s32 	%p20, %r11, %r369;
	setp.lt.s32 	%p21, %r12, %r369;
	add.s32 	%r256, %r232, 16384;
	selp.b32 	%r371, 16, 0, %p14;
	selp.b32 	%r257, %r371, 0, %p12;
	// begin inline asm
	cp.async.cg.shared.global [ %r256 + 0 ], [ %rd57 + 0 ], 0x10, %r257;
	// end inline asm
	add.s32 	%r258, %r232, 17408;
	selp.b32 	%r372, 16, 0, %p15;
	selp.b32 	%r259, %r372, 0, %p12;
	// begin inline asm
	cp.async.cg.shared.global [ %r258 + 0 ], [ %rd58 + 0 ], 0x10, %r259;
	// end inline asm
	add.s32 	%r260, %r232, 18432;
	selp.b32 	%r373, 16, 0, %p16;
	selp.b32 	%r261, %r373, 0, %p12;
	// begin inline asm
	cp.async.cg.shared.global [ %r260 + 0 ], [ %rd59 + 0 ], 0x10, %r261;
	// end inline asm
	add.s32 	%r262, %r232, 19456;
	selp.b32 	%r374, 16, 0, %p17;
	selp.b32 	%r263, %r374, 0, %p12;
	// begin inline asm
	cp.async.cg.shared.global [ %r262 + 0 ], [ %rd60 + 0 ], 0x10, %r263;
	// end inline asm
	add.s32 	%r264, %r232, 20480;
	selp.b32 	%r375, 16, 0, %p18;
	selp.b32 	%r265, %r375, 0, %p12;
	// begin inline asm
	cp.async.cg.shared.global [ %r264 + 0 ], [ %rd61 + 0 ], 0x10, %r265;
	// end inline asm
	add.s32 	%r266, %r232, 21504;
	selp.b32 	%r376, 16, 0, %p19;
	selp.b32 	%r267, %r376, 0, %p12;
	// begin inline asm
	cp.async.cg.shared.global [ %r266 + 0 ], [ %rd62 + 0 ], 0x10, %r267;
	// end inline asm
	add.s32 	%r268, %r232, 22528;
	selp.b32 	%r377, 16, 0, %p20;
	selp.b32 	%r269, %r377, 0, %p12;
	// begin inline asm
	cp.async.cg.shared.global [ %r268 + 0 ], [ %rd63 + 0 ], 0x10, %r269;
	// end inline asm
	add.s32 	%r270, %r232, 23552;
	selp.b32 	%r378, 16, 0, %p21;
	selp.b32 	%r271, %r378, 0, %p12;
	// begin inline asm
	cp.async.cg.shared.global [ %r270 + 0 ], [ %rd64 + 0 ], 0x10, %r271;
	// end inline asm
	cp.async.commit_group;
	setp.gt.s32 	%p22, %r351, 191;
	add.s64 	%rd65, %rd41, 256;
	add.s64 	%rd66, %rd42, 256;
	add.s64 	%rd67, %rd43, 256;
	add.s64 	%rd68, %rd44, 256;
	add.s64 	%rd69, %rd57, %rd77;
	add.s64 	%rd70, %rd58, %rd77;
	add.s64 	%rd71, %rd59, %rd77;
	add.s64 	%rd72, %rd60, %rd77;
	add.s64 	%rd73, %rd61, %rd77;
	add.s64 	%rd74, %rd62, %rd77;
	add.s64 	%rd75, %rd63, %rd77;
	add.s64 	%rd76, %rd64, %rd77;
	add.s32 	%r379, %r222, -128;
	setp.lt.s32 	%p23, %r14, %r379;
	bar.sync 	0;
	add.s32 	%r272, %r361, 81920;
	selp.b32 	%r380, 16, 0, %p23;
	selp.b32 	%r275, %r380, 0, %p22;
	// begin inline asm
	cp.async.cg.shared.global [ %r272 + 0 ], [ %rd65 + 0 ], 0x10, %r275;
	// end inline asm
	add.s32 	%r274, %r361, 83968;
	// begin inline asm
	cp.async.cg.shared.global [ %r274 + 0 ], [ %rd66 + 0 ], 0x10, %r275;
	// end inline asm
	add.s32 	%r276, %r361, 86016;
	// begin inline asm
	cp.async.cg.shared.global [ %r276 + 0 ], [ %rd67 + 0 ], 0x10, %r275;
	// end inline asm
	add.s32 	%r278, %r361, 88064;
	// begin inline asm
	cp.async.cg.shared.global [ %r278 + 0 ], [ %rd68 + 0 ], 0x10, %r275;
	// end inline asm
	cp.async.commit_group;
	setp.lt.s32 	%p24, %r5, %r379;
	setp.lt.s32 	%p25, %r6, %r379;
	setp.lt.s32 	%p26, %r7, %r379;
	setp.lt.s32 	%p27, %r8, %r379;
	setp.lt.s32 	%p28, %r9, %r379;
	setp.lt.s32 	%p29, %r10, %r379;
	setp.lt.s32 	%p30, %r11, %r379;
	setp.lt.s32 	%p31, %r12, %r379;
	add.s32 	%r280, %r232, 32768;
	selp.b32 	%r381, 16, 0, %p24;
	selp.b32 	%r281, %r381, 0, %p22;
	// begin inline asm
	cp.async.cg.shared.global [ %r280 + 0 ], [ %rd69 + 0 ], 0x10, %r281;
	// end inline asm
	add.s32 	%r282, %r232, 33792;
	selp.b32 	%r382, 16, 0, %p25;
	selp.b32 	%r283, %r382, 0, %p22;
	// begin inline asm
	cp.async.cg.shared.global [ %r282 + 0 ], [ %rd70 + 0 ], 0x10, %r283;
	// end inline asm
	add.s32 	%r284, %r232, 34816;
	selp.b32 	%r383, 16, 0, %p26;
	selp.b32 	%r285, %r383, 0, %p22;
	// begin inline asm
	cp.async.cg.shared.global [ %r284 + 0 ], [ %rd71 + 0 ], 0x10, %r285;
	// end inline asm
	add.s32 	%r286, %r232, 35840;
	selp.b32 	%r384, 16, 0, %p27;
	selp.b32 	%r287, %r384, 0, %p22;
	// begin inline asm
	cp.async.cg.shared.global [ %r286 + 0 ], [ %rd72 + 0 ], 0x10, %r287;
	// end inline asm
	add.s32 	%r288, %r232, 36864;
	selp.b32 	%r385, 16, 0, %p28;
	selp.b32 	%r289, %r385, 0, %p22;
	// begin inline asm
	cp.async.cg.shared.global [ %r288 + 0 ], [ %rd73 + 0 ], 0x10, %r289;
	// end inline asm
	add.s32 	%r290, %r232, 37888;
	selp.b32 	%r386, 16, 0, %p29;
	selp.b32 	%r291, %r386, 0, %p22;
	// begin inline asm
	cp.async.cg.shared.global [ %r290 + 0 ], [ %rd74 + 0 ], 0x10, %r291;
	// end inline asm
	add.s32 	%r292, %r232, 38912;
	selp.b32 	%r387, 16, 0, %p30;
	selp.b32 	%r293, %r387, 0, %p22;
	// begin inline asm
	cp.async.cg.shared.global [ %r292 + 0 ], [ %rd75 + 0 ], 0x10, %r293;
	// end inline asm
	add.s32 	%r294, %r232, 39936;
	selp.b32 	%r388, 16, 0, %p31;
	selp.b32 	%r295, %r388, 0, %p22;
	// begin inline asm
	cp.async.cg.shared.global [ %r294 + 0 ], [ %rd76 + 0 ], 0x10, %r295;
	// end inline asm
	cp.async.commit_group;
	mov.b32 	%r1376, 0f00000000;
	mov.b32 	%r1377, %r1376;
	mov.b32 	%r1378, %r1376;
	mov.b32 	%r1379, %r1376;
	mov.b32 	%r1380, %r1376;
	mov.b32 	%r1381, %r1376;
	mov.b32 	%r1382, %r1376;
	mov.b32 	%r1383, %r1376;
	mov.b32 	%r1384, %r1376;
	mov.b32 	%r1385, %r1376;
	mov.b32 	%r1386, %r1376;
	mov.b32 	%r1387, %r1376;
	mov.b32 	%r1388, %r1376;
	mov.b32 	%r1389, %r1376;
	mov.b32 	%r1390, %r1376;
	mov.b32 	%r1391, %r1376;
	mov.b32 	%r1392, %r1376;
	mov.b32 	%r1393, %r1376;
	mov.b32 	%r1394, %r1376;
	mov.b32 	%r1395, %r1376;
	mov.b32 	%r1396, %r1376;
	mov.b32 	%r1397, %r1376;
	mov.b32 	%r1398, %r1376;
	mov.b32 	%r1399, %r1376;
	mov.b32 	%r1400, %r1376;
	mov.b32 	%r1401, %r1376;
	mov.b32 	%r1402, %r1376;
	mov.b32 	%r1403, %r1376;
	mov.b32 	%r1404, %r1376;
	mov.b32 	%r1405, %r1376;
	mov.b32 	%r1406, %r1376;
	mov.b32 	%r1407, %r1376;
	mov.b32 	%r1408, %r1376;
	mov.b32 	%r1409, %r1376;
	mov.b32 	%r1410, %r1376;
	mov.b32 	%r1411, %r1376;
	mov.b32 	%r1412, %r1376;
	mov.b32 	%r1413, %r1376;
	mov.b32 	%r1414, %r1376;
	mov.b32 	%r1415, %r1376;
	mov.b32 	%r1416, %r1376;
	mov.b32 	%r1417, %r1376;
	mov.b32 	%r1418, %r1376;
	mov.b32 	%r1419, %r1376;
	mov.b32 	%r1420, %r1376;
	mov.b32 	%r1421, %r1376;
	mov.b32 	%r1422, %r1376;
	mov.b32 	%r1423, %r1376;
	mov.b32 	%r1424, %r1376;
	mov.b32 	%r1425, %r1376;
	mov.b32 	%r1426, %r1376;
	mov.b32 	%r1427, %r1376;
	mov.b32 	%r1428, %r1376;
	mov.b32 	%r1429, %r1376;
	mov.b32 	%r1430, %r1376;
	mov.b32 	%r1431, %r1376;
	mov.b32 	%r1432, %r1376;
	mov.b32 	%r1433, %r1376;
	mov.b32 	%r1434, %r1376;
	mov.b32 	%r1435, %r1376;
	mov.b32 	%r1436, %r1376;
	mov.b32 	%r1437, %r1376;
	mov.b32 	%r1438, %r1376;
	mov.b32 	%r1439, %r1376;
	@%p1 bra 	$L__BB0_3;
// %bb.1:                               // %.lr.ph
	shr.u32 	%r3, %r2, 5;
	cvt.s64.s32 	%rd1, %r338;
	cvt.s64.s32 	%rd2, %r339;
	cvt.s64.s32 	%rd3, %r340;
	cvt.s64.s32 	%rd4, %r341;
	cvt.s64.s32 	%rd5, %r343;
	cvt.s64.s32 	%rd6, %r344;
	cvt.s64.s32 	%rd7, %r345;
	cvt.s64.s32 	%rd8, %r346;
	cvt.s64.s32 	%rd9, %r347;
	cvt.s64.s32 	%rd10, %r348;
	cvt.s64.s32 	%rd11, %r349;
	cvt.s64.s32 	%rd12, %r350;
	shr.s32 	%r352, %r351, 31;
	shr.u32 	%r353, %r352, 26;
	add.s32 	%r354, %r351, %r353;
	shr.s32 	%r15, %r354, 6;
	cvt.s64.s32 	%rd13, %r355;
	add.s32 	%r18, %r15, -3;
	add.s32 	%r1373, %r222, -192;
	shl.b64 	%rd14, %rd12, 1;
	mad.lo.s64 	%rd135, %rd13, 6, %rd39;
	shl.b64 	%rd16, %rd13, 1;
	shl.b64 	%rd17, %rd11, 1;
	shl.b64 	%rd18, %rd10, 1;
	shl.b64 	%rd19, %rd9, 1;
	shl.b64 	%rd20, %rd8, 1;
	shl.b64 	%rd21, %rd7, 1;
	shl.b64 	%rd22, %rd6, 1;
	shl.b64 	%rd23, %rd5, 1;
	shl.b64 	%rd78, %rd4, 1;
	add.s64 	%rd79, %rd78, %rd38;
	add.s64 	%rd134, %rd79, 384;
	shl.b64 	%rd80, %rd3, 1;
	add.s64 	%rd81, %rd80, %rd38;
	add.s64 	%rd133, %rd81, 384;
	shl.b64 	%rd82, %rd2, 1;
	add.s64 	%rd83, %rd82, %rd38;
	add.s64 	%rd132, %rd83, 384;
	shl.b64 	%rd84, %rd1, 1;
	add.s64 	%rd85, %rd84, %rd38;
	add.s64 	%rd131, %rd85, 384;
	mov.b32 	%r1040, 0;
	mov.b32 	%r1376, 0f00000000;
	mov.b32 	%r1375, 2;
	mov.b32 	%r1374, -1;
	mov.b32 	%r1377, %r1376;
	mov.b32 	%r1378, %r1376;
	mov.b32 	%r1379, %r1376;
	mov.b32 	%r1380, %r1376;
	mov.b32 	%r1381, %r1376;
	mov.b32 	%r1382, %r1376;
	mov.b32 	%r1383, %r1376;
	mov.b32 	%r1384, %r1376;
	mov.b32 	%r1385, %r1376;
	mov.b32 	%r1386, %r1376;
	mov.b32 	%r1387, %r1376;
	mov.b32 	%r1388, %r1376;
	mov.b32 	%r1389, %r1376;
	mov.b32 	%r1390, %r1376;
	mov.b32 	%r1391, %r1376;
	mov.b32 	%r1392, %r1376;
	mov.b32 	%r1393, %r1376;
	mov.b32 	%r1394, %r1376;
	mov.b32 	%r1395, %r1376;
	mov.b32 	%r1396, %r1376;
	mov.b32 	%r1397, %r1376;
	mov.b32 	%r1398, %r1376;
	mov.b32 	%r1399, %r1376;
	mov.b32 	%r1400, %r1376;
	mov.b32 	%r1401, %r1376;
	mov.b32 	%r1402, %r1376;
	mov.b32 	%r1403, %r1376;
	mov.b32 	%r1404, %r1376;
	mov.b32 	%r1405, %r1376;
	mov.b32 	%r1406, %r1376;
	mov.b32 	%r1407, %r1376;
	mov.b32 	%r1408, %r1376;
	mov.b32 	%r1409, %r1376;
	mov.b32 	%r1410, %r1376;
	mov.b32 	%r1411, %r1376;
	mov.b32 	%r1412, %r1376;
	mov.b32 	%r1413, %r1376;
	mov.b32 	%r1414, %r1376;
	mov.b32 	%r1415, %r1376;
	mov.b32 	%r1416, %r1376;
	mov.b32 	%r1417, %r1376;
	mov.b32 	%r1418, %r1376;
	mov.b32 	%r1419, %r1376;
	mov.b32 	%r1420, %r1376;
	mov.b32 	%r1421, %r1376;
	mov.b32 	%r1422, %r1376;
	mov.b32 	%r1423, %r1376;
	mov.b32 	%r1424, %r1376;
	mov.b32 	%r1425, %r1376;
	mov.b32 	%r1426, %r1376;
	mov.b32 	%r1427, %r1376;
	mov.b32 	%r1428, %r1376;
	mov.b32 	%r1429, %r1376;
	mov.b32 	%r1430, %r1376;
	mov.b32 	%r1431, %r1376;
	mov.b32 	%r1432, %r1376;
	mov.b32 	%r1433, %r1376;
	mov.b32 	%r1434, %r1376;
	mov.b32 	%r1435, %r1376;
	mov.b32 	%r1436, %r1376;
	mov.b32 	%r1437, %r1376;
	mov.b32 	%r1438, %r1376;
	mov.b32 	%r1439, %r1376;
	mov.b32 	%r1440, %r1040;
$L__BB0_2:                              // =>This Inner Loop Header: Depth=1
	setp.lt.s32 	%p36, %r1440, %r18;
	add.s32 	%r1069, %r1374, 1;
	setp.gt.s32 	%p37, %r1069, 3;
	selp.b32 	%r1374, 0, %r1069, %p37;
	cp.async.wait_group 	4;
	bar.sync 	0;
	shl.b32 	%r1070, %r1374, 13;
	add.s32 	%r1072, %r360, 65536;
	add.s32 	%r969, %r1072, %r1070;
	shl.b32 	%r1073, %r1374, 14;
	add.s32 	%r972, %r360, %r1073;
	shfl.sync.idx.b32 	%r1074, %r3, 0, 31, -1;
	wgmma.fence.sync.aligned;
	bfe.u32 	%r1075, %r969, 4, 14;
	cvt.u64.u32 	%rd106, %r1075;
	or.b64 	%rd86, %rd106, 4611686293338849280;
	bfe.u32 	%r1076, %r972, 4, 14;
	cvt.u64.u32 	%rd107, %r1076;
	or.b64 	%rd87, %rd107, 4611686293338849280;
	mov.pred 	%p32, -1;
	// begin inline asm
	wgmma.mma_async.sync.aligned.m64n128k16.f32.f16.f16 {%r1376,%r1377,%r1378,%r1379,%r1380,%r1381,%r1382,%r1383,%r1384,%r1385,%r1386,%r1387,%r1388,%r1389,%r1390,%r1391,%r1392,%r1393,%r1394,%r1395,%r1396,%r1397,%r1398,%r1399,%r1400,%r1401,%r1402,%r1403,%r1404,%r1405,%r1406,%r1407,%r1408,%r1409,%r1410,%r1411,%r1412,%r1413,%r1414,%r1415,%r1416,%r1417,%r1418,%r1419,%r1420,%r1421,%r1422,%r1423,%r1424,%r1425,%r1426,%r1427,%r1428,%r1429,%r1430,%r1431,%r1432,%r1433,%r1434,%r1435,%r1436,%r1437,%r1438,%r1439}, %rd86, %rd87, %p32, 1, 1, 0, 1;
	// end inline asm
	add.s32 	%r1077, %r969, 32;
	bfe.u32 	%r1078, %r1077, 4, 14;
	cvt.u64.u32 	%rd108, %r1078;
	or.b64 	%rd88, %rd108, 4611686293338849280;
	add.s32 	%r1079, %r972, 2048;
	bfe.u32 	%r1080, %r1079, 4, 14;
	cvt.u64.u32 	%rd109, %r1080;
	or.b64 	%rd89, %rd109, 4611686293338849280;
	// begin inline asm
	wgmma.mma_async.sync.aligned.m64n128k16.f32.f16.f16 {%r1376,%r1377,%r1378,%r1379,%r1380,%r1381,%r1382,%r1383,%r1384,%r1385,%r1386,%r1387,%r1388,%r1389,%r1390,%r1391,%r1392,%r1393,%r1394,%r1395,%r1396,%r1397,%r1398,%r1399,%r1400,%r1401,%r1402,%r1403,%r1404,%r1405,%r1406,%r1407,%r1408,%r1409,%r1410,%r1411,%r1412,%r1413,%r1414,%r1415,%r1416,%r1417,%r1418,%r1419,%r1420,%r1421,%r1422,%r1423,%r1424,%r1425,%r1426,%r1427,%r1428,%r1429,%r1430,%r1431,%r1432,%r1433,%r1434,%r1435,%r1436,%r1437,%r1438,%r1439}, %rd88, %rd89, %p32, 1, 1, 0, 1;
	// end inline asm
	add.s32 	%r1081, %r969, 64;
	bfe.u32 	%r1082, %r1081, 4, 14;
	cvt.u64.u32 	%rd110, %r1082;
	or.b64 	%rd90, %rd110, 4611686293338849280;
	add.s32 	%r1083, %r972, 4096;
	bfe.u32 	%r1084, %r1083, 4, 14;
	cvt.u64.u32 	%rd111, %r1084;
	or.b64 	%rd91, %rd111, 4611686293338849280;
	// begin inline asm
	wgmma.mma_async.sync.aligned.m64n128k16.f32.f16.f16 {%r1376,%r1377,%r1378,%r1379,%r1380,%r1381,%r1382,%r1383,%r1384,%r1385,%r1386,%r1387,%r1388,%r1389,%r1390,%r1391,%r1392,%r1393,%r1394,%r1395,%r1396,%r1397,%r1398,%r1399,%r1400,%r1401,%r1402,%r1403,%r1404,%r1405,%r1406,%r1407,%r1408,%r1409,%r1410,%r1411,%r1412,%r1413,%r1414,%r1415,%r1416,%r1417,%r1418,%r1419,%r1420,%r1421,%r1422,%r1423,%r1424,%r1425,%r1426,%r1427,%r1428,%r1429,%r1430,%r1431,%r1432,%r1433,%r1434,%r1435,%r1436,%r1437,%r1438,%r1439}, %rd90, %rd91, %p32, 1, 1, 0, 1;
	// end inline asm
	add.s32 	%r1085, %r969, 96;
	bfe.u32 	%r1086, %r1085, 4, 14;
	cvt.u64.u32 	%rd112, %r1086;
	or.b64 	%rd92, %rd112, 4611686293338849280;
	add.s32 	%r1087, %r972, 6144;
	bfe.u32 	%r1088, %r1087, 4, 14;
	cvt.u64.u32 	%rd113, %r1088;
	or.b64 	%rd93, %rd113, 4611686293338849280;
	// begin inline asm
	wgmma.mma_async.sync.aligned.m64n128k16.f32.f16.f16 {%r1376,%r1377,%r1378,%r1379,%r1380,%r1381,%r1382,%r1383,%r1384,%r1385,%r1386,%r1387,%r1388,%r1389,%r1390,%r1391,%r1392,%r1393,%r1394,%r1395,%r1396,%r1397,%r1398,%r1399,%r1400,%r1401,%r1402,%r1403,%r1404,%r1405,%r1406,%r1407,%r1408,%r1409,%r1410,%r1411,%r1412,%r1413,%r1414,%r1415,%r1416,%r1417,%r1418,%r1419,%r1420,%r1421,%r1422,%r1423,%r1424,%r1425,%r1426,%r1427,%r1428,%r1429,%r1430,%r1431,%r1432,%r1433,%r1434,%r1435,%r1436,%r1437,%r1438,%r1439}, %rd92, %rd93, %p32, 1, 1, 0, 1;
	// end inline asm
	wgmma.commit_group.sync.aligned;
	mov.b32 	%r974, %r1040;
	mov.b32 	%r970, %r1040;
	mov.b32 	%r971, %r1040;
	mov.b32 	%r973, %r1040;
	// begin inline asm
	// wait for regs: %r1376,%r1377,%r1378,%r1379,%r1380,%r1381,%r1382,%r1383,%r1384,%r1385,%r1386,%r1387,%r1388,%r1389,%r1390,%r1391,%r1392,%r1393,%r1394,%r1395,%r1396,%r1397,%r1398,%r1399,%r1400,%r1401,%r1402,%r1403,%r1404,%r1405,%r1406,%r1407,%r1408,%r1409,%r1410,%r1411,%r1412,%r1413,%r1414,%r1415,%r1416,%r1417,%r1418,%r1419,%r1420,%r1421,%r1422,%r1423,%r1424,%r1425,%r1426,%r1427,%r1428,%r1429,%r1430,%r1431,%r1432,%r1433,%r1434,%r1435,%r1436,%r1437,%r1438,%r1439,%r969,%r970,%r971,%r972,%r973,%r974
	wgmma.wait_group.sync.aligned 1;
	// end inline asm
	add.s64 	%rd98, %rd135, %rd23;
	add.s64 	%rd99, %rd135, %rd22;
	add.s64 	%rd100, %rd135, %rd21;
	add.s64 	%rd101, %rd135, %rd20;
	add.s64 	%rd102, %rd135, %rd19;
	add.s64 	%rd103, %rd135, %rd18;
	add.s64 	%rd104, %rd135, %rd17;
	add.s64 	%rd105, %rd135, %rd14;
	add.s32 	%r1089, %r1375, 1;
	setp.gt.s32 	%p38, %r1089, 3;
	selp.b32 	%r1375, 0, %r1089, %p38;
	setp.lt.s32 	%p39, %r14, %r1373;
	shl.b32 	%r1090, %r1375, 13;
	add.s32 	%r1091, %r1072, %r1090;
	bar.sync 	0;
	add.s32 	%r1045, %r1091, %r16;
	selp.b32 	%r1092, 16, 0, %p39;
	selp.b32 	%r1048, %r1092, 0, %p36;
	// begin inline asm
	cp.async.cg.shared.global [ %r1045 + 0 ], [ %rd131 + 0 ], 0x10, %r1048;
	// end inline asm
	add.s32 	%r1047, %r1045, 2048;
	// begin inline asm
	cp.async.cg.shared.global [ %r1047 + 0 ], [ %rd132 + 0 ], 0x10, %r1048;
	// end inline asm
	add.s32 	%r1049, %r1045, 4096;
	// begin inline asm
	cp.async.cg.shared.global [ %r1049 + 0 ], [ %rd133 + 0 ], 0x10, %r1048;
	// end inline asm
	add.s32 	%r1051, %r1045, 6144;
	// begin inline asm
	cp.async.cg.shared.global [ %r1051 + 0 ], [ %rd134 + 0 ], 0x10, %r1048;
	// end inline asm
	cp.async.commit_group;
	setp.lt.s32 	%p40, %r5, %r1373;
	setp.lt.s32 	%p41, %r6, %r1373;
	setp.lt.s32 	%p42, %r7, %r1373;
	setp.lt.s32 	%p43, %r8, %r1373;
	setp.lt.s32 	%p44, %r9, %r1373;
	setp.lt.s32 	%p45, %r10, %r1373;
	setp.lt.s32 	%p46, %r11, %r1373;
	setp.lt.s32 	%p47, %r12, %r1373;
	shl.b32 	%r1093, %r1375, 14;
	add.s32 	%r1094, %r360, %r1093;
	add.s32 	%r1053, %r1094, %r17;
	selp.b32 	%r1095, 16, 0, %p40;
	selp.b32 	%r1054, %r1095, 0, %p36;
	// begin inline asm
	cp.async.cg.shared.global [ %r1053 + 0 ], [ %rd98 + 0 ], 0x10, %r1054;
	// end inline asm
	add.s32 	%r1055, %r1053, 1024;
	selp.b32 	%r1096, 16, 0, %p41;
	selp.b32 	%r1056, %r1096, 0, %p36;
	// begin inline asm
	cp.async.cg.shared.global [ %r1055 + 0 ], [ %rd99 + 0 ], 0x10, %r1056;
	// end inline asm
	add.s32 	%r1057, %r1053, 2048;
	selp.b32 	%r1097, 16, 0, %p42;
	selp.b32 	%r1058, %r1097, 0, %p36;
	// begin inline asm
	cp.async.cg.shared.global [ %r1057 + 0 ], [ %rd100 + 0 ], 0x10, %r1058;
	// end inline asm
	add.s32 	%r1059, %r1053, 3072;
	selp.b32 	%r1098, 16, 0, %p43;
	selp.b32 	%r1060, %r1098, 0, %p36;
	// begin inline asm
	cp.async.cg.shared.global [ %r1059 + 0 ], [ %rd101 + 0 ], 0x10, %r1060;
	// end inline asm
	add.s32 	%r1061, %r1053, 4096;
	selp.b32 	%r1099, 16, 0, %p44;
	selp.b32 	%r1062, %r1099, 0, %p36;
	// begin inline asm
	cp.async.cg.shared.global [ %r1061 + 0 ], [ %rd102 + 0 ], 0x10, %r1062;
	// end inline asm
	add.s32 	%r1063, %r1053, 5120;
	selp.b32 	%r1100, 16, 0, %p45;
	selp.b32 	%r1064, %r1100, 0, %p36;
	// begin inline asm
	cp.async.cg.shared.global [ %r1063 + 0 ], [ %rd103 + 0 ], 0x10, %r1064;
	// end inline asm
	add.s32 	%r1065, %r1053, 6144;
	selp.b32 	%r1101, 16, 0, %p46;
	selp.b32 	%r1066, %r1101, 0, %p36;
	// begin inline asm
	cp.async.cg.shared.global [ %r1065 + 0 ], [ %rd104 + 0 ], 0x10, %r1066;
	// end inline asm
	add.s32 	%r1067, %r1053, 7168;
	selp.b32 	%r1102, 16, 0, %p47;
	selp.b32 	%r1068, %r1102, 0, %p36;
	// begin inline asm
	cp.async.cg.shared.global [ %r1067 + 0 ], [ %rd105 + 0 ], 0x10, %r1068;
	// end inline asm
	cp.async.commit_group;
	add.s32 	%r1440, %r1440, 1;
	add.s64 	%rd135, %rd135, %rd16;
	add.s64 	%rd134, %rd134, 128;
	add.s64 	%rd133, %rd133, 128;
	add.s64 	%rd132, %rd132, 128;
	add.s64 	%rd131, %rd131, 128;
	add.s32 	%r1373, %r1373, -64;
	setp.ne.b32 	%p48, %r15, %r1440;
	@%p48 bra 	$L__BB0_2;
$L__BB0_3:                              // %._crit_edge
	or.b32 	%r1303, %r1, %r12;
	or.b32 	%r1304, %r1, %r11;
	or.b32 	%r1305, %r1, %r10;
	or.b32 	%r1306, %r1, %r9;
	or.b32 	%r1307, %r1, %r8;
	or.b32 	%r1308, %r1, %r7;
	or.b32 	%r1309, %r1, %r6;
	or.b32 	%r1310, %r1, %r5;
	// begin inline asm
	// wait for regs: %r1376,%r1377,%r1378,%r1379,%r1380,%r1381,%r1382,%r1383,%r1384,%r1385,%r1386,%r1387,%r1388,%r1389,%r1390,%r1391,%r1392,%r1393,%r1394,%r1395,%r1396,%r1397,%r1398,%r1399,%r1400,%r1401,%r1402,%r1403,%r1404,%r1405,%r1406,%r1407,%r1408,%r1409,%r1410,%r1411,%r1412,%r1413,%r1414,%r1415,%r1416,%r1417,%r1418,%r1419,%r1420,%r1421,%r1422,%r1423,%r1424,%r1425,%r1426,%r1427,%r1428,%r1429,%r1430,%r1431,%r1432,%r1433,%r1434,%r1435,%r1436,%r1437,%r1438,%r1439
	wgmma.wait_group.sync.aligned 0;
	// end inline asm
	cp.async.wait_group 	0;
	bar.sync 	0;
	cvt.rn.f16x2.f32 	%r1311, %r1377, %r1376;
	cvt.rn.f16x2.f32 	%r1312, %r1379, %r1378;
	cvt.rn.f16x2.f32 	%r1313, %r1381, %r1380;
	cvt.rn.f16x2.f32 	%r1314, %r1383, %r1382;
	cvt.rn.f16x2.f32 	%r1315, %r1385, %r1384;
	cvt.rn.f16x2.f32 	%r1316, %r1387, %r1386;
	cvt.rn.f16x2.f32 	%r1317, %r1389, %r1388;
	cvt.rn.f16x2.f32 	%r1318, %r1391, %r1390;
	cvt.rn.f16x2.f32 	%r1319, %r1393, %r1392;
	cvt.rn.f16x2.f32 	%r1320, %r1395, %r1394;
	cvt.rn.f16x2.f32 	%r1321, %r1397, %r1396;
	cvt.rn.f16x2.f32 	%r1322, %r1399, %r1398;
	cvt.rn.f16x2.f32 	%r1323, %r1401, %r1400;
	cvt.rn.f16x2.f32 	%r1324, %r1403, %r1402;
	cvt.rn.f16x2.f32 	%r1325, %r1405, %r1404;
	cvt.rn.f16x2.f32 	%r1326, %r1407, %r1406;
	cvt.rn.f16x2.f32 	%r1327, %r1409, %r1408;
	cvt.rn.f16x2.f32 	%r1328, %r1411, %r1410;
	cvt.rn.f16x2.f32 	%r1329, %r1413, %r1412;
	cvt.rn.f16x2.f32 	%r1330, %r1415, %r1414;
	cvt.rn.f16x2.f32 	%r1331, %r1417, %r1416;
	cvt.rn.f16x2.f32 	%r1332, %r1419, %r1418;
	cvt.rn.f16x2.f32 	%r1333, %r1421, %r1420;
	cvt.rn.f16x2.f32 	%r1334, %r1423, %r1422;
	cvt.rn.f16x2.f32 	%r1335, %r1425, %r1424;
	cvt.rn.f16x2.f32 	%r1336, %r1427, %r1426;
	cvt.rn.f16x2.f32 	%r1337, %r1429, %r1428;
	cvt.rn.f16x2.f32 	%r1338, %r1431, %r1430;
	cvt.rn.f16x2.f32 	%r1339, %r1433, %r1432;
	cvt.rn.f16x2.f32 	%r1340, %r1435, %r1434;
	cvt.rn.f16x2.f32 	%r1341, %r1437, %r1436;
	cvt.rn.f16x2.f32 	%r1342, %r1439, %r1438;
	mul.lo.s32 	%r1343, %r1310, %r223;
	mul.lo.s32 	%r1344, %r1309, %r223;
	mul.lo.s32 	%r1345, %r1308, %r223;
	mul.lo.s32 	%r1346, %r1307, %r223;
	mul.lo.s32 	%r1347, %r1306, %r223;
	mul.lo.s32 	%r1348, %r1305, %r223;
	mul.lo.s32 	%r1349, %r1304, %r223;
	mul.lo.s32 	%r1350, %r1303, %r223;
	mad.wide.s32 	%rd122, %r1343, 2, %rd40;
	mad.wide.s32 	%rd123, %r1344, 2, %rd40;
	mad.wide.s32 	%rd124, %r1345, 2, %rd40;
	mad.wide.s32 	%rd125, %r1346, 2, %rd40;
	mad.wide.s32 	%rd126, %r1347, 2, %rd40;
	mad.wide.s32 	%rd127, %r1348, 2, %rd40;
	mad.wide.s32 	%rd128, %r1349, 2, %rd40;
	mad.wide.s32 	%rd129, %r1350, 2, %rd40;
	mul.wide.s32 	%rd130, %r13, 2;
	add.s64 	%rd114, %rd122, %rd130;
	add.s64 	%rd115, %rd123, %rd130;
	add.s64 	%rd116, %rd124, %rd130;
	add.s64 	%rd117, %rd125, %rd130;
	add.s64 	%rd118, %rd126, %rd130;
	add.s64 	%rd119, %rd127, %rd130;
	add.s64 	%rd120, %rd128, %rd130;
	add.s64 	%rd121, %rd129, %rd130;
	setp.lt.s32 	%p57, %r1310, %r220;
	setp.lt.s32 	%p58, %r1309, %r220;
	setp.lt.s32 	%p59, %r1308, %r220;
	setp.lt.s32 	%p60, %r1307, %r220;
	setp.lt.s32 	%p61, %r1306, %r220;
	setp.lt.s32 	%p62, %r1305, %r220;
	setp.lt.s32 	%p63, %r1304, %r220;
	setp.lt.s32 	%p64, %r1303, %r220;
	setp.lt.s32 	%p65, %r13, %r221;
	and.pred 	%p49, %p57, %p65;
	and.pred 	%p50, %p58, %p65;
	and.pred 	%p51, %p59, %p65;
	and.pred 	%p52, %p60, %p65;
	and.pred 	%p53, %p61, %p65;
	and.pred 	%p54, %p62, %p65;
	and.pred 	%p55, %p63, %p65;
	and.pred 	%p56, %p64, %p65;
	and.b32 	%r1351, %r2, 3;
	shl.b32 	%r1352, %r1351, 11;
	shl.b32 	%r1353, %r1351, 5;
	shl.b32 	%r1354, %r4, 4;
	shl.b32 	%r1355, %r2, 2;
	and.b32 	%r1356, %r1355, 16;
	or.b32 	%r1357, %r1354, %r1356;
	or.b32 	%r1358, %r1357, %r1352;
	or.b32 	%r1359, %r1358, %r1353;
	add.s32 	%r1361, %r360, %r1359;
	st.shared.v4.b32 	[%r1361], {%r1311, %r1313, %r1315, %r1317};
	xor.b32 	%r1362, %r1359, 32;
	add.s32 	%r1363, %r360, %r1362;
	st.shared.v4.b32 	[%r1363], {%r1319, %r1321, %r1323, %r1325};
	xor.b32 	%r1364, %r1359, 64;
	add.s32 	%r1365, %r360, %r1364;
	st.shared.v4.b32 	[%r1365], {%r1327, %r1329, %r1331, %r1333};
	xor.b32 	%r1366, %r1359, 96;
	add.s32 	%r1367, %r360, %r1366;
	st.shared.v4.b32 	[%r1367], {%r1335, %r1337, %r1339, %r1341};
	bar.sync 	0;
	shl.b32 	%r1368, %r2, 8;
	and.b32 	%r1369, %r1368, 6144;
	and.b32 	%r1370, %r1355, 496;
	or.b32 	%r1371, %r1369, %r1353;
	xor.b32 	%r1372, %r1371, %r1370;
	add.s32 	%r1235, %r360, %r1372;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r1271, %r1272, %r1273, %r1274}, [%r1235];
	// end inline asm
	add.s32 	%r1240, %r1235, 512;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r1279, %r1280, %r1281, %r1282}, [%r1240];
	// end inline asm
	add.s32 	%r1245, %r1235, 1024;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r1287, %r1288, %r1289, %r1290}, [%r1245];
	// end inline asm
	add.s32 	%r1250, %r1235, 1536;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r1295, %r1296, %r1297, %r1298}, [%r1250];
	// end inline asm
	bar.sync 	0;
	st.shared.v4.b32 	[%r1361], {%r1312, %r1314, %r1316, %r1318};
	st.shared.v4.b32 	[%r1363], {%r1320, %r1322, %r1324, %r1326};
	st.shared.v4.b32 	[%r1365], {%r1328, %r1330, %r1332, %r1334};
	st.shared.v4.b32 	[%r1367], {%r1336, %r1338, %r1340, %r1342};
	bar.sync 	0;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r1275, %r1276, %r1277, %r1278}, [%r1235];
	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r1283, %r1284, %r1285, %r1286}, [%r1240];
	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r1291, %r1292, %r1293, %r1294}, [%r1245];
	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r1299, %r1300, %r1301, %r1302}, [%r1250];
	// end inline asm
	// begin inline asm
	@%p49 st.global.v4.b32 [ %rd114 + 0 ], { %r1271, %r1272, %r1273, %r1274 };
	// end inline asm
	// begin inline asm
	@%p50 st.global.v4.b32 [ %rd115 + 0 ], { %r1275, %r1276, %r1277, %r1278 };
	// end inline asm
	// begin inline asm
	@%p51 st.global.v4.b32 [ %rd116 + 0 ], { %r1279, %r1280, %r1281, %r1282 };
	// end inline asm
	// begin inline asm
	@%p52 st.global.v4.b32 [ %rd117 + 0 ], { %r1283, %r1284, %r1285, %r1286 };
	// end inline asm
	// begin inline asm
	@%p53 st.global.v4.b32 [ %rd118 + 0 ], { %r1287, %r1288, %r1289, %r1290 };
	// end inline asm
	// begin inline asm
	@%p54 st.global.v4.b32 [ %rd119 + 0 ], { %r1291, %r1292, %r1293, %r1294 };
	// end inline asm
	// begin inline asm
	@%p55 st.global.v4.b32 [ %rd120 + 0 ], { %r1295, %r1296, %r1297, %r1298 };
	// end inline asm
	// begin inline asm
	@%p56 st.global.v4.b32 [ %rd121 + 0 ], { %r1299, %r1300, %r1301, %r1302 };
	// end inline asm
	ret;
                                        // -- End function
}
