#loc1 = loc("a_ptr")
#loc2 = loc("b_ptr")
#loc3 = loc("c_ptr")
#loc4 = loc("M")
#loc5 = loc("N")
#loc6 = loc("K")
#loc7 = loc("stride_am")
#loc8 = loc("stride_bk")
#loc9 = loc("stride_cm")
module {
  tt.func public @matmul_kernel(%a_ptr: !tt.ptr<f16> {tt.divisibility = 16 : i32} loc("a_ptr"), %b_ptr: !tt.ptr<f16> {tt.divisibility = 16 : i32} loc("b_ptr"), %c_ptr: !tt.ptr<f16> {tt.divisibility = 16 : i32} loc("c_ptr"), %M: i32 {tt.divisibility = 16 : i32} loc("M"), %N: i32 {tt.divisibility = 16 : i32} loc("N"), %K: i32 {tt.divisibility = 16 : i32} loc("K"), %stride_am: i32 {tt.divisibility = 16 : i32} loc("stride_am"), %stride_bk: i32 {tt.divisibility = 16 : i32} loc("stride_bk"), %stride_cm: i32 {tt.divisibility = 16 : i32} loc("stride_cm")) attributes {noinline = false} {
    %cst = arith.constant dense<0.000000e+00> : tensor<64x128xf32> loc(#loc)
    %c127_i32 = arith.constant 127 : i32 loc(#loc)
    %c63_i32 = arith.constant 63 : i32 loc(#loc)
    %cst_0 = arith.constant dense<0.000000e+00> : tensor<64x128xf16> loc(#loc)
    %cst_1 = arith.constant dense<0.000000e+00> : tensor<64x64xf16> loc(#loc)
    %c1_i32 = arith.constant 1 : i32 loc(#loc)
    %cst_2 = arith.constant dense<64> : tensor<64x64xi32> loc(#loc)
    %c128_i32 = arith.constant 128 : i32 loc(#loc)
    %c64_i32 = arith.constant 64 : i32 loc(#loc)
    %true = arith.constant true loc(#loc)
    %c0_i32 = arith.constant 0 : i32 loc(#loc)
    %c8_i32 = arith.constant 8 : i32 loc(#loc)
    %0 = tt.get_program_id x : i32 loc(#loc)
    %1 = arith.addi %M, %c63_i32 : i32 loc(#loc)
    %2 = arith.divsi %1, %c64_i32 : i32 loc(#loc)
    %3 = arith.addi %N, %c127_i32 : i32 loc(#loc)
    %4 = arith.divsi %3, %c128_i32 : i32 loc(#loc)
    %5 = arith.muli %4, %c8_i32 : i32 loc(#loc)
    %6 = arith.divsi %0, %5 : i32 loc(#loc)
    %7 = arith.muli %6, %c8_i32 : i32 loc(#loc)
    %8 = arith.subi %2, %7 : i32 loc(#loc)
    %9 = arith.minsi %8, %c8_i32 : i32 loc(#loc)
    %10 = arith.remsi %0, %5 : i32 loc(#loc)
    %11 = arith.remsi %10, %9 : i32 loc(#loc)
    %12 = arith.addi %7, %11 : i32 loc(#loc)
    %13 = arith.divsi %10, %9 : i32 loc(#loc)
    %14 = arith.cmpi sge, %12, %c0_i32 : i32 loc(#loc)
    llvm.intr.assume %14 : i1 loc(#loc)
    %15 = arith.cmpi sge, %13, %c0_i32 : i32 loc(#loc)
    llvm.intr.assume %15 : i1 loc(#loc)
    %16 = arith.cmpi sgt, %stride_am, %c0_i32 : i32 loc(#loc)
    llvm.intr.assume %16 : i1 loc(#loc)
    llvm.intr.assume %true : i1 loc(#loc)
    llvm.intr.assume %true : i1 loc(#loc)
    %17 = arith.cmpi sgt, %stride_bk, %c0_i32 : i32 loc(#loc)
    llvm.intr.assume %17 : i1 loc(#loc)
    %18 = arith.cmpi sgt, %stride_cm, %c0_i32 : i32 loc(#loc)
    llvm.intr.assume %18 : i1 loc(#loc)
    llvm.intr.assume %true : i1 loc(#loc)
    %19 = arith.muli %12, %c64_i32 : i32 loc(#loc)
    %20 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32> loc(#loc)
    %21 = tt.splat %19 : i32 -> tensor<64xi32> loc(#loc)
    %22 = arith.addi %21, %20 : tensor<64xi32> loc(#loc)
    %23 = tt.splat %M : i32 -> tensor<64xi32> loc(#loc)
    %24 = arith.remsi %22, %23 : tensor<64xi32> loc(#loc)
    %25 = arith.muli %13, %c128_i32 : i32 loc(#loc)
    %26 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32> loc(#loc)
    %27 = tt.splat %25 : i32 -> tensor<128xi32> loc(#loc)
    %28 = arith.addi %27, %26 : tensor<128xi32> loc(#loc)
    %29 = tt.splat %N : i32 -> tensor<128xi32> loc(#loc)
    %30 = arith.remsi %28, %29 : tensor<128xi32> loc(#loc)
    %31 = tt.expand_dims %24 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32> loc(#loc)
    %32 = tt.splat %stride_am : i32 -> tensor<64x1xi32> loc(#loc)
    %33 = arith.muli %31, %32 : tensor<64x1xi32> loc(#loc)
    %34 = tt.expand_dims %20 {axis = 0 : i32} : tensor<64xi32> -> tensor<1x64xi32> loc(#loc)
    %35 = tt.broadcast %33 : tensor<64x1xi32> -> tensor<64x64xi32> loc(#loc)
    %36 = tt.broadcast %34 : tensor<1x64xi32> -> tensor<64x64xi32> loc(#loc)
    %37 = arith.addi %35, %36 : tensor<64x64xi32> loc(#loc)
    %38 = tt.splat %a_ptr : !tt.ptr<f16> -> tensor<64x64x!tt.ptr<f16>> loc(#loc)
    %39 = tt.addptr %38, %37 : tensor<64x64x!tt.ptr<f16>>, tensor<64x64xi32> loc(#loc)
    %40 = tt.expand_dims %20 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32> loc(#loc)
    %41 = tt.splat %stride_bk : i32 -> tensor<64x1xi32> loc(#loc)
    %42 = arith.muli %40, %41 : tensor<64x1xi32> loc(#loc)
    %43 = tt.expand_dims %30 {axis = 0 : i32} : tensor<128xi32> -> tensor<1x128xi32> loc(#loc)
    %44 = tt.broadcast %42 : tensor<64x1xi32> -> tensor<64x128xi32> loc(#loc)
    %45 = tt.broadcast %43 : tensor<1x128xi32> -> tensor<64x128xi32> loc(#loc)
    %46 = arith.addi %44, %45 : tensor<64x128xi32> loc(#loc)
    %47 = tt.splat %b_ptr : !tt.ptr<f16> -> tensor<64x128x!tt.ptr<f16>> loc(#loc)
    %48 = tt.addptr %47, %46 : tensor<64x128x!tt.ptr<f16>>, tensor<64x128xi32> loc(#loc)
    %49 = arith.addi %K, %c63_i32 : i32 loc(#loc)
    %50 = arith.divsi %49, %c64_i32 : i32 loc(#loc)
    %accumulator:3 = scf.for %k = %c0_i32 to %50 step %c1_i32 iter_args(%a_ptrs = %39, %b_ptrs = %48, %accumulator_3 = %cst) -> (tensor<64x64x!tt.ptr<f16>>, tensor<64x128x!tt.ptr<f16>>, tensor<64x128xf32>)  : i32 {
      %68 = arith.muli %k, %c64_i32 : i32 loc(#loc)
      %69 = arith.subi %K, %68 : i32 loc(#loc)
      %70 = tt.splat %69 : i32 -> tensor<1x64xi32> loc(#loc)
      %71 = arith.cmpi slt, %34, %70 : tensor<1x64xi32> loc(#loc)
      %72 = tt.broadcast %71 : tensor<1x64xi1> -> tensor<64x64xi1> loc(#loc)
      %73 = tt.load %a_ptrs, %72, %cst_1 {flagtree_hints = "shared_memory"} : tensor<64x64x!tt.ptr<f16>> loc(#loc)
      %74 = tt.splat %69 : i32 -> tensor<64x1xi32> loc(#loc)
      %75 = arith.cmpi slt, %40, %74 : tensor<64x1xi32> loc(#loc)
      %76 = tt.broadcast %75 : tensor<64x1xi1> -> tensor<64x128xi1> loc(#loc)
      %77 = tt.load %b_ptrs, %76, %cst_0 {flagtree_hints = "shared_memory"} : tensor<64x128x!tt.ptr<f16>> loc(#loc)
      %78 = tt.dot %73, %77, %accumulator_3, inputPrecision = tf32 : tensor<64x64xf16> * tensor<64x128xf16> -> tensor<64x128xf32> loc(#loc)
      %79 = tt.addptr %a_ptrs, %cst_2 : tensor<64x64x!tt.ptr<f16>>, tensor<64x64xi32> loc(#loc)
      %80 = arith.muli %stride_bk, %c64_i32 : i32 loc(#loc)
      %81 = tt.splat %80 : i32 -> tensor<64x128xi32> loc(#loc)
      %82 = tt.addptr %b_ptrs, %81 : tensor<64x128x!tt.ptr<f16>>, tensor<64x128xi32> loc(#loc)
      scf.yield %79, %82, %78 : tensor<64x64x!tt.ptr<f16>>, tensor<64x128x!tt.ptr<f16>>, tensor<64x128xf32> loc(#loc)
    } loc(#loc12)
    %51 = arith.truncf %accumulator#2 : tensor<64x128xf32> to tensor<64x128xf16> loc(#loc)
    %52 = tt.expand_dims %22 {axis = 1 : i32} : tensor<64xi32> -> tensor<64x1xi32> loc(#loc)
    %53 = tt.splat %stride_cm : i32 -> tensor<64x1xi32> loc(#loc)
    %54 = arith.muli %53, %52 : tensor<64x1xi32> loc(#loc)
    %55 = tt.splat %c_ptr : !tt.ptr<f16> -> tensor<64x1x!tt.ptr<f16>> loc(#loc)
    %56 = tt.addptr %55, %54 : tensor<64x1x!tt.ptr<f16>>, tensor<64x1xi32> loc(#loc)
    %57 = tt.expand_dims %28 {axis = 0 : i32} : tensor<128xi32> -> tensor<1x128xi32> loc(#loc)
    %58 = tt.broadcast %56 : tensor<64x1x!tt.ptr<f16>> -> tensor<64x128x!tt.ptr<f16>> loc(#loc)
    %59 = tt.broadcast %57 : tensor<1x128xi32> -> tensor<64x128xi32> loc(#loc)
    %60 = tt.addptr %58, %59 : tensor<64x128x!tt.ptr<f16>>, tensor<64x128xi32> loc(#loc)
    %61 = tt.splat %M : i32 -> tensor<64x1xi32> loc(#loc)
    %62 = arith.cmpi slt, %52, %61 : tensor<64x1xi32> loc(#loc)
    %63 = tt.splat %N : i32 -> tensor<1x128xi32> loc(#loc)
    %64 = arith.cmpi slt, %57, %63 : tensor<1x128xi32> loc(#loc)
    %65 = tt.broadcast %62 : tensor<64x1xi1> -> tensor<64x128xi1> loc(#loc)
    %66 = tt.broadcast %64 : tensor<1x128xi1> -> tensor<64x128xi1> loc(#loc)
    %67 = arith.andi %65, %66 : tensor<64x128xi1> loc(#loc)
    tt.store %60, %51, %67 : tensor<64x128x!tt.ptr<f16>> loc(#loc)
    tt.return loc(#loc)
  } loc(#loc)
} loc(#loc)
#loc = loc(unknown)
#loc10 = loc("a_ptrs")
#loc11 = loc("b_ptrs"(#loc10))
#loc12 = loc("accumulator"(#loc11))
