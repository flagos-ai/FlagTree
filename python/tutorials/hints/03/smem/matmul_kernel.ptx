//
// Generated by LLVM NVPTX Back-End
//

.version 8.7
.target sm_90a
.address_size 64

	// .globl	matmul_kernel           // -- Begin function matmul_kernel
.extern .shared .align 16 .b8 global_smem[];
                                        // @matmul_kernel
.visible .entry matmul_kernel(
	.param .u64 .ptr .global .align 1 matmul_kernel_param_0,
	.param .u64 .ptr .global .align 1 matmul_kernel_param_1,
	.param .u64 .ptr .global .align 1 matmul_kernel_param_2,
	.param .u32 matmul_kernel_param_3,
	.param .u32 matmul_kernel_param_4,
	.param .u32 matmul_kernel_param_5,
	.param .u32 matmul_kernel_param_6,
	.param .u32 matmul_kernel_param_7,
	.param .u32 matmul_kernel_param_8,
	.param .u64 .ptr .global .align 1 matmul_kernel_param_9,
	.param .u64 .ptr .global .align 1 matmul_kernel_param_10
)
.reqntid 128
{
	.reg .pred 	%p<33>;
	.reg .b32 	%r<1409>;
	.reg .b64 	%rd<73>;

// %bb.0:
	ld.param.b32 	%r243, [matmul_kernel_param_8];
	ld.param.b32 	%r1311, [matmul_kernel_param_5];
	ld.param.b32 	%r239, [matmul_kernel_param_4];
	ld.param.b32 	%r238, [matmul_kernel_param_3];
	ld.param.b64 	%rd30, [matmul_kernel_param_2];
	mov.u32 	%r276, %ctaid.x;
	add.s32 	%r277, %r238, 63;
	shr.s32 	%r278, %r277, 31;
	shr.u32 	%r279, %r278, 26;
	add.s32 	%r280, %r277, %r279;
	shr.s32 	%r281, %r280, 6;
	add.s32 	%r282, %r239, 127;
	shr.s32 	%r283, %r282, 31;
	shr.u32 	%r284, %r283, 25;
	add.s32 	%r285, %r282, %r284;
	shr.s32 	%r286, %r285, 7;
	shl.b32 	%r288, %r286, 3;
	div.s32 	%r289, %r276, %r288;
	shl.b32 	%r290, %r289, 3;
	sub.s32 	%r291, %r281, %r290;
	min.s32 	%r293, %r291, 8;
	mul.lo.s32 	%r294, %r289, %r288;
	sub.s32 	%r295, %r276, %r294;
	div.s32 	%r296, %r295, %r293;
	mul.lo.s32 	%r297, %r296, %r293;
	sub.s32 	%r298, %r295, %r297;
	add.s32 	%r299, %r298, %r290;
	shl.b32 	%r1, %r299, 6;
	mov.u32 	%r2, %tid.x;
	and.b32 	%r4, %r2, 120;
	shr.u32 	%r300, %r2, 4;
	bfe.u32 	%r5, %r2, 4, 3;
	or.b32 	%r6, %r5, 8;
	or.b32 	%r7, %r5, 16;
	or.b32 	%r8, %r5, 24;
	or.b32 	%r9, %r5, 32;
	or.b32 	%r10, %r5, 40;
	or.b32 	%r11, %r5, 48;
	or.b32 	%r12, %r300, 56;
	shl.b32 	%r301, %r296, 7;
	and.b32 	%r13, %r2, 15;
	shl.b32 	%r14, %r13, 3;
	or.b32 	%r15, %r301, %r14;
	add.s32 	%r302, %r1311, 63;
	setp.lt.s32 	%p1, %r302, 64;
	mov.b32 	%r1377, 0;
	mov.b32 	%r1310, global_smem;
	mov.b32 	%r1378, %r1377;
	mov.b32 	%r1379, %r1377;
	mov.b32 	%r1380, %r1377;
	mov.b32 	%r1381, %r1377;
	mov.b32 	%r1382, %r1377;
	mov.b32 	%r1383, %r1377;
	mov.b32 	%r1384, %r1377;
	mov.b32 	%r1385, %r1377;
	mov.b32 	%r1386, %r1377;
	mov.b32 	%r1387, %r1377;
	mov.b32 	%r1388, %r1377;
	mov.b32 	%r1389, %r1377;
	mov.b32 	%r1390, %r1377;
	mov.b32 	%r1391, %r1377;
	mov.b32 	%r1392, %r1377;
	mov.b32 	%r1393, %r1377;
	mov.b32 	%r1394, %r1377;
	mov.b32 	%r1395, %r1377;
	mov.b32 	%r1396, %r1377;
	mov.b32 	%r1397, %r1377;
	mov.b32 	%r1398, %r1377;
	mov.b32 	%r1399, %r1377;
	mov.b32 	%r1400, %r1377;
	mov.b32 	%r1401, %r1377;
	mov.b32 	%r1402, %r1377;
	mov.b32 	%r1403, %r1377;
	mov.b32 	%r1404, %r1377;
	mov.b32 	%r1405, %r1377;
	mov.b32 	%r1406, %r1377;
	mov.b32 	%r1407, %r1377;
	mov.b32 	%r1408, %r1377;
	@%p1 bra 	$L__BB0_4;
// %bb.1:                               // %.lr.ph
	ld.param.b32 	%r242, [matmul_kernel_param_7];
	ld.param.b32 	%r241, [matmul_kernel_param_6];
	ld.param.b64 	%rd68, [matmul_kernel_param_1];
	ld.param.b64 	%rd28, [matmul_kernel_param_0];
	shr.u32 	%r3, %r2, 5;
	shl.b32 	%r16, %r2, 3;
	and.b32 	%r17, %r16, 56;
	shr.s32 	%r303, %r302, 31;
	shr.u32 	%r304, %r303, 26;
	add.s32 	%r305, %r302, %r304;
	shr.s32 	%r1312, %r305, 6;
	shl.b32 	%r307, %r242, 6;
	rem.s32 	%r308, %r15, %r239;
	mad.lo.s32 	%r309, %r242, %r12, %r308;
	mad.lo.s32 	%r310, %r242, %r5, %r308;
	shr.u32 	%r311, %r4, 3;
	or.b32 	%r312, %r1, %r311;
	or.b32 	%r313, %r312, 48;
	rem.s32 	%r314, %r313, %r238;
	mad.lo.s32 	%r315, %r314, %r241, %r17;
	mad.wide.s32 	%rd69, %r315, 2, %rd28;
	or.b32 	%r316, %r312, 32;
	rem.s32 	%r317, %r316, %r238;
	mad.lo.s32 	%r318, %r317, %r241, %r17;
	mad.wide.s32 	%rd70, %r318, 2, %rd28;
	or.b32 	%r319, %r312, 16;
	rem.s32 	%r320, %r319, %r238;
	mad.lo.s32 	%r321, %r320, %r241, %r17;
	mad.wide.s32 	%rd71, %r321, 2, %rd28;
	rem.s32 	%r322, %r312, %r238;
	mad.lo.s32 	%r323, %r322, %r241, %r17;
	mad.wide.s32 	%rd72, %r323, 2, %rd28;
	shl.b32 	%r324, %r2, 4;
	and.b32 	%r325, %r324, 2032;
	add.s32 	%r365, %r1310, %r325;
	add.s32 	%r367, %r365, 2048;
	add.s32 	%r369, %r365, 4096;
	add.s32 	%r371, %r365, 6144;
	shl.b32 	%r327, %r13, 7;
	shl.b32 	%r328, %r2, 6;
	and.b32 	%r329, %r328, 6144;
	and.b32 	%r330, %r2, 16;
	add.s32 	%r331, %r1310, %r327;
	add.s32 	%r332, %r331, %r329;
	add.s32 	%r377, %r332, %r330;
	add.s32 	%r382, %r377, 32;
	add.s32 	%r387, %r377, 64;
	add.s32 	%r392, %r377, 96;
	add.s32 	%r401, %r365, 8192;
	add.s32 	%r403, %r365, 10240;
	add.s32 	%r405, %r365, 12288;
	or.b32 	%r333, %r324, 14336;
	add.s32 	%r407, %r1310, %r333;
	and.b32 	%r334, %r16, 248;
	add.s32 	%r31, %r1310, %r334;
	or.b32 	%r335, %r16, 7936;
	add.s32 	%r32, %r1310, %r335;
	or.b32 	%r336, %r16, 16128;
	add.s32 	%r33, %r1310, %r336;
	shl.b32 	%r337, %r330, 9;
	or.b32 	%r338, %r337, %r14;
	add.s32 	%r34, %r1310, %r338;
	xor.b32 	%r339, %r338, 16;
	add.s32 	%r35, %r1310, %r339;
	xor.b32 	%r340, %r338, 32;
	add.s32 	%r36, %r1310, %r340;
	xor.b32 	%r341, %r338, 48;
	add.s32 	%r37, %r1310, %r341;
	xor.b32 	%r342, %r338, 64;
	add.s32 	%r38, %r1310, %r342;
	xor.b32 	%r343, %r338, 80;
	add.s32 	%r39, %r1310, %r343;
	xor.b32 	%r344, %r338, 96;
	add.s32 	%r40, %r1310, %r344;
	xor.b32 	%r345, %r338, 112;
	add.s32 	%r41, %r1310, %r345;
	bfe.u32 	%r346, %r1310, 4, 14;
	cvt.u64.u32 	%rd31, %r346;
	or.b64 	%rd47, %rd31, 4611686293338849280;
	add.s32 	%r347, %r1310, 2048;
	bfe.u32 	%r348, %r347, 4, 14;
	cvt.u64.u32 	%rd32, %r348;
	or.b64 	%rd48, %rd32, 4611686293338849280;
	add.s32 	%r349, %r1310, 4096;
	bfe.u32 	%r350, %r349, 4, 14;
	cvt.u64.u32 	%rd33, %r350;
	or.b64 	%rd49, %rd33, 4611686293338849280;
	add.s32 	%r351, %r1310, 6144;
	bfe.u32 	%r352, %r351, 4, 14;
	cvt.u64.u32 	%rd34, %r352;
	or.b64 	%rd50, %rd34, 4611686293338849280;
	mul.wide.s32 	%rd9, %r309, 2;
	mul.wide.s32 	%rd10, %r307, 2;
	mad.lo.s32 	%r354, %r242, %r11, %r308;
	mul.wide.s32 	%rd11, %r354, 2;
	mad.lo.s32 	%r356, %r242, %r10, %r308;
	mul.wide.s32 	%rd12, %r356, 2;
	mad.lo.s32 	%r358, %r242, %r9, %r308;
	mul.wide.s32 	%rd13, %r358, 2;
	mad.lo.s32 	%r360, %r242, %r8, %r308;
	mul.wide.s32 	%rd14, %r360, 2;
	mad.lo.s32 	%r362, %r242, %r7, %r308;
	mul.wide.s32 	%rd15, %r362, 2;
	mad.lo.s32 	%r364, %r242, %r6, %r308;
	mul.wide.s32 	%rd16, %r364, 2;
	mul.wide.s32 	%rd17, %r310, 2;
	mov.b32 	%r1313, 0f00000000;
	mov.b32 	%r1314, %r1313;
	mov.b32 	%r1315, %r1313;
	mov.b32 	%r1316, %r1313;
	mov.b32 	%r1317, %r1313;
	mov.b32 	%r1318, %r1313;
	mov.b32 	%r1319, %r1313;
	mov.b32 	%r1320, %r1313;
	mov.b32 	%r1321, %r1313;
	mov.b32 	%r1322, %r1313;
	mov.b32 	%r1323, %r1313;
	mov.b32 	%r1324, %r1313;
	mov.b32 	%r1325, %r1313;
	mov.b32 	%r1326, %r1313;
	mov.b32 	%r1327, %r1313;
	mov.b32 	%r1328, %r1313;
	mov.b32 	%r1329, %r1313;
	mov.b32 	%r1330, %r1313;
	mov.b32 	%r1331, %r1313;
	mov.b32 	%r1332, %r1313;
	mov.b32 	%r1333, %r1313;
	mov.b32 	%r1334, %r1313;
	mov.b32 	%r1335, %r1313;
	mov.b32 	%r1336, %r1313;
	mov.b32 	%r1337, %r1313;
	mov.b32 	%r1338, %r1313;
	mov.b32 	%r1339, %r1313;
	mov.b32 	%r1340, %r1313;
	mov.b32 	%r1341, %r1313;
	mov.b32 	%r1342, %r1313;
	mov.b32 	%r1343, %r1313;
	mov.b32 	%r1344, %r1313;
	mov.b32 	%r1345, %r1313;
	mov.b32 	%r1346, %r1313;
	mov.b32 	%r1347, %r1313;
	mov.b32 	%r1348, %r1313;
	mov.b32 	%r1349, %r1313;
	mov.b32 	%r1350, %r1313;
	mov.b32 	%r1351, %r1313;
	mov.b32 	%r1352, %r1313;
	mov.b32 	%r1353, %r1313;
	mov.b32 	%r1354, %r1313;
	mov.b32 	%r1355, %r1313;
	mov.b32 	%r1356, %r1313;
	mov.b32 	%r1357, %r1313;
	mov.b32 	%r1358, %r1313;
	mov.b32 	%r1359, %r1313;
	mov.b32 	%r1360, %r1313;
	mov.b32 	%r1361, %r1313;
	mov.b32 	%r1362, %r1313;
	mov.b32 	%r1363, %r1313;
	mov.b32 	%r1364, %r1313;
	mov.b32 	%r1365, %r1313;
	mov.b32 	%r1366, %r1313;
	mov.b32 	%r1367, %r1313;
	mov.b32 	%r1368, %r1313;
	mov.b32 	%r1369, %r1313;
	mov.b32 	%r1370, %r1313;
	mov.b32 	%r1371, %r1313;
	mov.b32 	%r1372, %r1313;
	mov.b32 	%r1373, %r1313;
	mov.b32 	%r1374, %r1313;
	mov.b32 	%r1375, %r1313;
	mov.b32 	%r1376, %r1313;
$L__BB0_2:                              // =>This Inner Loop Header: Depth=1
	add.s64 	%rd46, %rd68, %rd9;
	add.s64 	%rd45, %rd68, %rd11;
	add.s64 	%rd44, %rd68, %rd12;
	add.s64 	%rd43, %rd68, %rd13;
	add.s64 	%rd42, %rd68, %rd14;
	add.s64 	%rd41, %rd68, %rd15;
	add.s64 	%rd40, %rd68, %rd16;
	add.s64 	%rd39, %rd68, %rd17;
	setp.lt.s32 	%p6, %r17, %r1311;
	bar.sync 	0;
	selp.b32 	%r366, 16, 0, %p6;
	// begin inline asm
	cp.async.cg.shared.global [ %r365 + 0 ], [ %rd72 + 0 ], 0x10, %r366;
	// end inline asm
	// begin inline asm
	cp.async.cg.shared.global [ %r367 + 0 ], [ %rd71 + 0 ], 0x10, %r366;
	// end inline asm
	// begin inline asm
	cp.async.cg.shared.global [ %r369 + 0 ], [ %rd70 + 0 ], 0x10, %r366;
	// end inline asm
	// begin inline asm
	cp.async.cg.shared.global [ %r371 + 0 ], [ %rd69 + 0 ], 0x10, %r366;
	// end inline asm
	cp.async.commit_group;
	cp.async.wait_group 	0;
	bar.sync 	0;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r373, %r374, %r375, %r376}, [%r377];
	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r378, %r379, %r380, %r381}, [%r382];
	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r383, %r384, %r385, %r386}, [%r387];
	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r388, %r389, %r390, %r391}, [%r392];
	// end inline asm
	setp.lt.s32 	%p7, %r5, %r1311;
	setp.lt.s32 	%p8, %r6, %r1311;
	setp.lt.s32 	%p9, %r7, %r1311;
	setp.lt.s32 	%p10, %r8, %r1311;
	setp.lt.s32 	%p11, %r9, %r1311;
	setp.lt.s32 	%p12, %r10, %r1311;
	setp.lt.s32 	%p13, %r11, %r1311;
	setp.lt.s32 	%p14, %r12, %r1311;
	bar.sync 	0;
	selp.b32 	%r394, 16, 0, %p7;
	// begin inline asm
	cp.async.cg.shared.global [ %r365 + 0 ], [ %rd39 + 0 ], 0x10, %r394;
	// end inline asm
	selp.b32 	%r396, 16, 0, %p8;
	// begin inline asm
	cp.async.cg.shared.global [ %r367 + 0 ], [ %rd40 + 0 ], 0x10, %r396;
	// end inline asm
	selp.b32 	%r398, 16, 0, %p9;
	// begin inline asm
	cp.async.cg.shared.global [ %r369 + 0 ], [ %rd41 + 0 ], 0x10, %r398;
	// end inline asm
	selp.b32 	%r400, 16, 0, %p10;
	// begin inline asm
	cp.async.cg.shared.global [ %r371 + 0 ], [ %rd42 + 0 ], 0x10, %r400;
	// end inline asm
	selp.b32 	%r402, 16, 0, %p11;
	// begin inline asm
	cp.async.cg.shared.global [ %r401 + 0 ], [ %rd43 + 0 ], 0x10, %r402;
	// end inline asm
	selp.b32 	%r404, 16, 0, %p12;
	// begin inline asm
	cp.async.cg.shared.global [ %r403 + 0 ], [ %rd44 + 0 ], 0x10, %r404;
	// end inline asm
	selp.b32 	%r406, 16, 0, %p13;
	// begin inline asm
	cp.async.cg.shared.global [ %r405 + 0 ], [ %rd45 + 0 ], 0x10, %r406;
	// end inline asm
	selp.b32 	%r408, 16, 0, %p14;
	// begin inline asm
	cp.async.cg.shared.global [ %r407 + 0 ], [ %rd46 + 0 ], 0x10, %r408;
	// end inline asm
	cp.async.commit_group;
	cp.async.wait_group 	0;
	bar.sync 	0;
	ld.shared.v2.b32 	{%r1071, %r1072}, [%r31];
	ld.shared.v2.b32 	{%r1073, %r1074}, [%r31+256];
	ld.shared.v2.b32 	{%r1075, %r1076}, [%r31+512];
	ld.shared.v2.b32 	{%r1077, %r1078}, [%r31+768];
	ld.shared.v2.b32 	{%r1079, %r1080}, [%r31+1024];
	ld.shared.v2.b32 	{%r1081, %r1082}, [%r31+1280];
	ld.shared.v2.b32 	{%r1083, %r1084}, [%r31+1536];
	ld.shared.v2.b32 	{%r1085, %r1086}, [%r31+1792];
	ld.shared.v2.b32 	{%r1087, %r1088}, [%r31+2048];
	ld.shared.v2.b32 	{%r1089, %r1090}, [%r31+2304];
	ld.shared.v2.b32 	{%r1091, %r1092}, [%r31+2560];
	ld.shared.v2.b32 	{%r1093, %r1094}, [%r31+2816];
	ld.shared.v2.b32 	{%r1095, %r1096}, [%r31+3072];
	ld.shared.v2.b32 	{%r1097, %r1098}, [%r31+3328];
	ld.shared.v2.b32 	{%r1099, %r1100}, [%r31+3584];
	ld.shared.v2.b32 	{%r1101, %r1102}, [%r31+3840];
	ld.shared.v2.b32 	{%r1103, %r1104}, [%r31+4096];
	ld.shared.v2.b32 	{%r1105, %r1106}, [%r31+4352];
	ld.shared.v2.b32 	{%r1107, %r1108}, [%r31+4608];
	ld.shared.v2.b32 	{%r1109, %r1110}, [%r31+4864];
	ld.shared.v2.b32 	{%r1111, %r1112}, [%r31+5120];
	ld.shared.v2.b32 	{%r1113, %r1114}, [%r31+5376];
	ld.shared.v2.b32 	{%r1115, %r1116}, [%r31+5632];
	ld.shared.v2.b32 	{%r1117, %r1118}, [%r31+5888];
	ld.shared.v2.b32 	{%r1119, %r1120}, [%r31+6144];
	ld.shared.v2.b32 	{%r1121, %r1122}, [%r31+6400];
	ld.shared.v2.b32 	{%r1123, %r1124}, [%r31+6656];
	ld.shared.v2.b32 	{%r1125, %r1126}, [%r31+6912];
	ld.shared.v2.b32 	{%r1127, %r1128}, [%r31+7168];
	ld.shared.v2.b32 	{%r1129, %r1130}, [%r31+7424];
	ld.shared.v2.b32 	{%r1131, %r1132}, [%r31+7680];
	ld.shared.v2.b32 	{%r1133, %r1134}, [%r32];
	ld.shared.v2.b32 	{%r1135, %r1136}, [%r31+8192];
	ld.shared.v2.b32 	{%r1137, %r1138}, [%r31+8448];
	ld.shared.v2.b32 	{%r1139, %r1140}, [%r31+8704];
	ld.shared.v2.b32 	{%r1141, %r1142}, [%r31+8960];
	ld.shared.v2.b32 	{%r1143, %r1144}, [%r31+9216];
	ld.shared.v2.b32 	{%r1145, %r1146}, [%r31+9472];
	ld.shared.v2.b32 	{%r1147, %r1148}, [%r31+9728];
	ld.shared.v2.b32 	{%r1149, %r1150}, [%r31+9984];
	ld.shared.v2.b32 	{%r1151, %r1152}, [%r31+10240];
	ld.shared.v2.b32 	{%r1153, %r1154}, [%r31+10496];
	ld.shared.v2.b32 	{%r1155, %r1156}, [%r31+10752];
	ld.shared.v2.b32 	{%r1157, %r1158}, [%r31+11008];
	ld.shared.v2.b32 	{%r1159, %r1160}, [%r31+11264];
	ld.shared.v2.b32 	{%r1161, %r1162}, [%r31+11520];
	ld.shared.v2.b32 	{%r1163, %r1164}, [%r31+11776];
	ld.shared.v2.b32 	{%r1165, %r1166}, [%r31+12032];
	ld.shared.v2.b32 	{%r1167, %r1168}, [%r31+12288];
	ld.shared.v2.b32 	{%r1169, %r1170}, [%r31+12544];
	ld.shared.v2.b32 	{%r1171, %r1172}, [%r31+12800];
	ld.shared.v2.b32 	{%r1173, %r1174}, [%r31+13056];
	ld.shared.v2.b32 	{%r1175, %r1176}, [%r31+13312];
	ld.shared.v2.b32 	{%r1177, %r1178}, [%r31+13568];
	ld.shared.v2.b32 	{%r1179, %r1180}, [%r31+13824];
	ld.shared.v2.b32 	{%r1181, %r1182}, [%r31+14080];
	ld.shared.v2.b32 	{%r1183, %r1184}, [%r31+14336];
	ld.shared.v2.b32 	{%r1185, %r1186}, [%r31+14592];
	ld.shared.v2.b32 	{%r1187, %r1188}, [%r31+14848];
	ld.shared.v2.b32 	{%r1189, %r1190}, [%r31+15104];
	ld.shared.v2.b32 	{%r1191, %r1192}, [%r31+15360];
	ld.shared.v2.b32 	{%r1193, %r1194}, [%r31+15616];
	ld.shared.v2.b32 	{%r1195, %r1196}, [%r31+15872];
	ld.shared.v2.b32 	{%r1197, %r1198}, [%r33];
	bar.sync 	0;
	st.shared.v2.b32 	[%r34], {%r1071, %r1072};
	st.shared.v2.b32 	[%r34+1024], {%r1087, %r1088};
	st.shared.v2.b32 	[%r34+2048], {%r1103, %r1104};
	st.shared.v2.b32 	[%r34+3072], {%r1119, %r1120};
	st.shared.v2.b32 	[%r34+4096], {%r1135, %r1136};
	st.shared.v2.b32 	[%r34+5120], {%r1151, %r1152};
	st.shared.v2.b32 	[%r34+6144], {%r1167, %r1168};
	st.shared.v2.b32 	[%r34+7168], {%r1183, %r1184};
	st.shared.v2.b32 	[%r35+128], {%r1073, %r1074};
	st.shared.v2.b32 	[%r35+1152], {%r1089, %r1090};
	st.shared.v2.b32 	[%r35+2176], {%r1105, %r1106};
	st.shared.v2.b32 	[%r35+3200], {%r1121, %r1122};
	st.shared.v2.b32 	[%r35+4224], {%r1137, %r1138};
	st.shared.v2.b32 	[%r35+5248], {%r1153, %r1154};
	st.shared.v2.b32 	[%r35+6272], {%r1169, %r1170};
	st.shared.v2.b32 	[%r35+7296], {%r1185, %r1186};
	st.shared.v2.b32 	[%r36+256], {%r1075, %r1076};
	st.shared.v2.b32 	[%r36+1280], {%r1091, %r1092};
	st.shared.v2.b32 	[%r36+2304], {%r1107, %r1108};
	st.shared.v2.b32 	[%r36+3328], {%r1123, %r1124};
	st.shared.v2.b32 	[%r36+4352], {%r1139, %r1140};
	st.shared.v2.b32 	[%r36+5376], {%r1155, %r1156};
	st.shared.v2.b32 	[%r36+6400], {%r1171, %r1172};
	st.shared.v2.b32 	[%r36+7424], {%r1187, %r1188};
	st.shared.v2.b32 	[%r37+384], {%r1077, %r1078};
	st.shared.v2.b32 	[%r37+1408], {%r1093, %r1094};
	st.shared.v2.b32 	[%r37+2432], {%r1109, %r1110};
	st.shared.v2.b32 	[%r37+3456], {%r1125, %r1126};
	st.shared.v2.b32 	[%r37+4480], {%r1141, %r1142};
	st.shared.v2.b32 	[%r37+5504], {%r1157, %r1158};
	st.shared.v2.b32 	[%r37+6528], {%r1173, %r1174};
	st.shared.v2.b32 	[%r37+7552], {%r1189, %r1190};
	st.shared.v2.b32 	[%r38+512], {%r1079, %r1080};
	st.shared.v2.b32 	[%r38+1536], {%r1095, %r1096};
	st.shared.v2.b32 	[%r38+2560], {%r1111, %r1112};
	st.shared.v2.b32 	[%r38+3584], {%r1127, %r1128};
	st.shared.v2.b32 	[%r38+4608], {%r1143, %r1144};
	st.shared.v2.b32 	[%r38+5632], {%r1159, %r1160};
	st.shared.v2.b32 	[%r38+6656], {%r1175, %r1176};
	st.shared.v2.b32 	[%r38+7680], {%r1191, %r1192};
	st.shared.v2.b32 	[%r39+640], {%r1081, %r1082};
	st.shared.v2.b32 	[%r39+1664], {%r1097, %r1098};
	st.shared.v2.b32 	[%r39+2688], {%r1113, %r1114};
	st.shared.v2.b32 	[%r39+3712], {%r1129, %r1130};
	st.shared.v2.b32 	[%r39+4736], {%r1145, %r1146};
	st.shared.v2.b32 	[%r39+5760], {%r1161, %r1162};
	st.shared.v2.b32 	[%r39+6784], {%r1177, %r1178};
	st.shared.v2.b32 	[%r39+7808], {%r1193, %r1194};
	st.shared.v2.b32 	[%r40+768], {%r1083, %r1084};
	st.shared.v2.b32 	[%r40+1792], {%r1099, %r1100};
	st.shared.v2.b32 	[%r40+2816], {%r1115, %r1116};
	st.shared.v2.b32 	[%r40+3840], {%r1131, %r1132};
	st.shared.v2.b32 	[%r40+4864], {%r1147, %r1148};
	st.shared.v2.b32 	[%r40+5888], {%r1163, %r1164};
	st.shared.v2.b32 	[%r40+6912], {%r1179, %r1180};
	st.shared.v2.b32 	[%r40+7936], {%r1195, %r1196};
	st.shared.v2.b32 	[%r41+896], {%r1085, %r1086};
	st.shared.v2.b32 	[%r41+1920], {%r1101, %r1102};
	st.shared.v2.b32 	[%r41+2944], {%r1117, %r1118};
	st.shared.v2.b32 	[%r41+3968], {%r1133, %r1134};
	st.shared.v2.b32 	[%r41+4992], {%r1149, %r1150};
	st.shared.v2.b32 	[%r41+6016], {%r1165, %r1166};
	st.shared.v2.b32 	[%r41+7040], {%r1181, %r1182};
	st.shared.v2.b32 	[%r41+8064], {%r1197, %r1198};
	// begin inline asm
	fence.proxy.async.shared::cta;
	// end inline asm
	bar.sync 	0;
	shfl.sync.idx.b32 	%r1199, %r3, 0, 31, -1;
	wgmma.fence.sync.aligned;
	mov.pred 	%p2, -1;
	// begin inline asm
	wgmma.mma_async.sync.aligned.m64n128k16.f32.f16.f16 {%r1313,%r1314,%r1315,%r1316,%r1317,%r1318,%r1319,%r1320,%r1321,%r1322,%r1323,%r1324,%r1325,%r1326,%r1327,%r1328,%r1329,%r1330,%r1331,%r1332,%r1333,%r1334,%r1335,%r1336,%r1337,%r1338,%r1339,%r1340,%r1341,%r1342,%r1343,%r1344,%r1345,%r1346,%r1347,%r1348,%r1349,%r1350,%r1351,%r1352,%r1353,%r1354,%r1355,%r1356,%r1357,%r1358,%r1359,%r1360,%r1361,%r1362,%r1363,%r1364,%r1365,%r1366,%r1367,%r1368,%r1369,%r1370,%r1371,%r1372,%r1373,%r1374,%r1375,%r1376}, {%r373,%r374,%r375,%r376}, %rd47, %p2, 1, 1, 1;
	// end inline asm
	// begin inline asm
	wgmma.mma_async.sync.aligned.m64n128k16.f32.f16.f16 {%r1313,%r1314,%r1315,%r1316,%r1317,%r1318,%r1319,%r1320,%r1321,%r1322,%r1323,%r1324,%r1325,%r1326,%r1327,%r1328,%r1329,%r1330,%r1331,%r1332,%r1333,%r1334,%r1335,%r1336,%r1337,%r1338,%r1339,%r1340,%r1341,%r1342,%r1343,%r1344,%r1345,%r1346,%r1347,%r1348,%r1349,%r1350,%r1351,%r1352,%r1353,%r1354,%r1355,%r1356,%r1357,%r1358,%r1359,%r1360,%r1361,%r1362,%r1363,%r1364,%r1365,%r1366,%r1367,%r1368,%r1369,%r1370,%r1371,%r1372,%r1373,%r1374,%r1375,%r1376}, {%r378,%r379,%r380,%r381}, %rd48, %p2, 1, 1, 1;
	// end inline asm
	// begin inline asm
	wgmma.mma_async.sync.aligned.m64n128k16.f32.f16.f16 {%r1313,%r1314,%r1315,%r1316,%r1317,%r1318,%r1319,%r1320,%r1321,%r1322,%r1323,%r1324,%r1325,%r1326,%r1327,%r1328,%r1329,%r1330,%r1331,%r1332,%r1333,%r1334,%r1335,%r1336,%r1337,%r1338,%r1339,%r1340,%r1341,%r1342,%r1343,%r1344,%r1345,%r1346,%r1347,%r1348,%r1349,%r1350,%r1351,%r1352,%r1353,%r1354,%r1355,%r1356,%r1357,%r1358,%r1359,%r1360,%r1361,%r1362,%r1363,%r1364,%r1365,%r1366,%r1367,%r1368,%r1369,%r1370,%r1371,%r1372,%r1373,%r1374,%r1375,%r1376}, {%r383,%r384,%r385,%r386}, %rd49, %p2, 1, 1, 1;
	// end inline asm
	// begin inline asm
	wgmma.mma_async.sync.aligned.m64n128k16.f32.f16.f16 {%r1313,%r1314,%r1315,%r1316,%r1317,%r1318,%r1319,%r1320,%r1321,%r1322,%r1323,%r1324,%r1325,%r1326,%r1327,%r1328,%r1329,%r1330,%r1331,%r1332,%r1333,%r1334,%r1335,%r1336,%r1337,%r1338,%r1339,%r1340,%r1341,%r1342,%r1343,%r1344,%r1345,%r1346,%r1347,%r1348,%r1349,%r1350,%r1351,%r1352,%r1353,%r1354,%r1355,%r1356,%r1357,%r1358,%r1359,%r1360,%r1361,%r1362,%r1363,%r1364,%r1365,%r1366,%r1367,%r1368,%r1369,%r1370,%r1371,%r1372,%r1373,%r1374,%r1375,%r1376}, {%r388,%r389,%r390,%r391}, %rd50, %p2, 1, 1, 1;
	// end inline asm
	wgmma.commit_group.sync.aligned;
	mov.b32 	%r1002, 0;
	mov.b32 	%r1001, %r1310;
	mov.b32 	%r1003, %r1002;
	// begin inline asm
	// wait for regs: %r1313,%r1314,%r1315,%r1316,%r1317,%r1318,%r1319,%r1320,%r1321,%r1322,%r1323,%r1324,%r1325,%r1326,%r1327,%r1328,%r1329,%r1330,%r1331,%r1332,%r1333,%r1334,%r1335,%r1336,%r1337,%r1338,%r1339,%r1340,%r1341,%r1342,%r1343,%r1344,%r1345,%r1346,%r1347,%r1348,%r1349,%r1350,%r1351,%r1352,%r1353,%r1354,%r1355,%r1356,%r1357,%r1358,%r1359,%r1360,%r1361,%r1362,%r1363,%r1364,%r1365,%r1366,%r1367,%r1368,%r1369,%r1370,%r1371,%r1372,%r1373,%r1374,%r1375,%r1376,%r1001,%r1002,%r1003
	wgmma.wait_group.sync.aligned 0;
	// end inline asm
	add.s64 	%rd72, %rd72, 128;
	add.s64 	%rd71, %rd71, 128;
	add.s64 	%rd70, %rd70, 128;
	add.s64 	%rd69, %rd69, 128;
	add.s64 	%rd68, %rd68, %rd10;
	add.s32 	%r1312, %r1312, -1;
	add.s32 	%r1311, %r1311, -64;
	setp.ne.b32 	%p15, %r1312, 0;
	@%p15 bra 	$L__BB0_2;
// %bb.3:                               // %._crit_edge.loopexit
	cvt.rn.f16x2.f32 	%r1408, %r1376, %r1375;
	cvt.rn.f16x2.f32 	%r1407, %r1372, %r1371;
	cvt.rn.f16x2.f32 	%r1406, %r1368, %r1367;
	cvt.rn.f16x2.f32 	%r1405, %r1364, %r1363;
	cvt.rn.f16x2.f32 	%r1404, %r1360, %r1359;
	cvt.rn.f16x2.f32 	%r1403, %r1356, %r1355;
	cvt.rn.f16x2.f32 	%r1402, %r1352, %r1351;
	cvt.rn.f16x2.f32 	%r1401, %r1348, %r1347;
	cvt.rn.f16x2.f32 	%r1400, %r1344, %r1343;
	cvt.rn.f16x2.f32 	%r1399, %r1340, %r1339;
	cvt.rn.f16x2.f32 	%r1398, %r1336, %r1335;
	cvt.rn.f16x2.f32 	%r1397, %r1332, %r1331;
	cvt.rn.f16x2.f32 	%r1396, %r1328, %r1327;
	cvt.rn.f16x2.f32 	%r1395, %r1324, %r1323;
	cvt.rn.f16x2.f32 	%r1394, %r1320, %r1319;
	cvt.rn.f16x2.f32 	%r1393, %r1316, %r1315;
	cvt.rn.f16x2.f32 	%r1392, %r1374, %r1373;
	cvt.rn.f16x2.f32 	%r1391, %r1370, %r1369;
	cvt.rn.f16x2.f32 	%r1390, %r1366, %r1365;
	cvt.rn.f16x2.f32 	%r1389, %r1362, %r1361;
	cvt.rn.f16x2.f32 	%r1388, %r1358, %r1357;
	cvt.rn.f16x2.f32 	%r1387, %r1354, %r1353;
	cvt.rn.f16x2.f32 	%r1386, %r1350, %r1349;
	cvt.rn.f16x2.f32 	%r1385, %r1346, %r1345;
	cvt.rn.f16x2.f32 	%r1384, %r1342, %r1341;
	cvt.rn.f16x2.f32 	%r1383, %r1338, %r1337;
	cvt.rn.f16x2.f32 	%r1382, %r1334, %r1333;
	cvt.rn.f16x2.f32 	%r1381, %r1330, %r1329;
	cvt.rn.f16x2.f32 	%r1380, %r1326, %r1325;
	cvt.rn.f16x2.f32 	%r1379, %r1322, %r1321;
	cvt.rn.f16x2.f32 	%r1378, %r1318, %r1317;
	cvt.rn.f16x2.f32 	%r1377, %r1314, %r1313;
$L__BB0_4:                              // %._crit_edge
	or.b32 	%r1272, %r1, %r12;
	or.b32 	%r1273, %r1, %r11;
	or.b32 	%r1274, %r1, %r10;
	or.b32 	%r1275, %r1, %r9;
	or.b32 	%r1276, %r1, %r8;
	or.b32 	%r1277, %r1, %r7;
	or.b32 	%r1278, %r1, %r6;
	or.b32 	%r1279, %r1, %r5;
	mul.lo.s32 	%r1280, %r1279, %r243;
	mul.lo.s32 	%r1281, %r1278, %r243;
	mul.lo.s32 	%r1282, %r1277, %r243;
	mul.lo.s32 	%r1283, %r1276, %r243;
	mul.lo.s32 	%r1284, %r1275, %r243;
	mul.lo.s32 	%r1285, %r1274, %r243;
	mul.lo.s32 	%r1286, %r1273, %r243;
	mul.lo.s32 	%r1287, %r1272, %r243;
	mad.wide.s32 	%rd59, %r1280, 2, %rd30;
	mad.wide.s32 	%rd60, %r1281, 2, %rd30;
	mad.wide.s32 	%rd61, %r1282, 2, %rd30;
	mad.wide.s32 	%rd62, %r1283, 2, %rd30;
	mad.wide.s32 	%rd63, %r1284, 2, %rd30;
	mad.wide.s32 	%rd64, %r1285, 2, %rd30;
	mad.wide.s32 	%rd65, %r1286, 2, %rd30;
	mad.wide.s32 	%rd66, %r1287, 2, %rd30;
	mul.wide.s32 	%rd67, %r15, 2;
	add.s64 	%rd51, %rd59, %rd67;
	add.s64 	%rd52, %rd60, %rd67;
	add.s64 	%rd53, %rd61, %rd67;
	add.s64 	%rd54, %rd62, %rd67;
	add.s64 	%rd55, %rd63, %rd67;
	add.s64 	%rd56, %rd64, %rd67;
	add.s64 	%rd57, %rd65, %rd67;
	add.s64 	%rd58, %rd66, %rd67;
	setp.lt.s32 	%p24, %r1279, %r238;
	setp.lt.s32 	%p25, %r1278, %r238;
	setp.lt.s32 	%p26, %r1277, %r238;
	setp.lt.s32 	%p27, %r1276, %r238;
	setp.lt.s32 	%p28, %r1275, %r238;
	setp.lt.s32 	%p29, %r1274, %r238;
	setp.lt.s32 	%p30, %r1273, %r238;
	setp.lt.s32 	%p31, %r1272, %r238;
	setp.lt.s32 	%p32, %r15, %r239;
	and.pred 	%p16, %p24, %p32;
	and.pred 	%p17, %p25, %p32;
	and.pred 	%p18, %p26, %p32;
	and.pred 	%p19, %p27, %p32;
	and.pred 	%p20, %p28, %p32;
	and.pred 	%p21, %p29, %p32;
	and.pred 	%p22, %p30, %p32;
	and.pred 	%p23, %p31, %p32;
	bar.sync 	0;
	and.b32 	%r1288, %r2, 3;
	shl.b32 	%r1289, %r1288, 11;
	shl.b32 	%r1290, %r1288, 5;
	shl.b32 	%r1291, %r4, 4;
	shl.b32 	%r1292, %r2, 2;
	and.b32 	%r1293, %r1292, 16;
	or.b32 	%r1294, %r1291, %r1293;
	or.b32 	%r1295, %r1294, %r1289;
	or.b32 	%r1296, %r1295, %r1290;
	add.s32 	%r1298, %r1310, %r1296;
	st.shared.v4.b32 	[%r1298], {%r1377, %r1378, %r1379, %r1380};
	xor.b32 	%r1299, %r1296, 32;
	add.s32 	%r1300, %r1310, %r1299;
	st.shared.v4.b32 	[%r1300], {%r1381, %r1382, %r1383, %r1384};
	xor.b32 	%r1301, %r1296, 64;
	add.s32 	%r1302, %r1310, %r1301;
	st.shared.v4.b32 	[%r1302], {%r1385, %r1386, %r1387, %r1388};
	xor.b32 	%r1303, %r1296, 96;
	add.s32 	%r1304, %r1310, %r1303;
	st.shared.v4.b32 	[%r1304], {%r1389, %r1390, %r1391, %r1392};
	bar.sync 	0;
	shl.b32 	%r1305, %r2, 8;
	and.b32 	%r1306, %r1305, 6144;
	and.b32 	%r1307, %r1292, 496;
	or.b32 	%r1308, %r1306, %r1290;
	xor.b32 	%r1309, %r1308, %r1307;
	add.s32 	%r1204, %r1310, %r1309;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r1240, %r1241, %r1242, %r1243}, [%r1204];
	// end inline asm
	add.s32 	%r1209, %r1204, 512;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r1248, %r1249, %r1250, %r1251}, [%r1209];
	// end inline asm
	add.s32 	%r1214, %r1204, 1024;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r1256, %r1257, %r1258, %r1259}, [%r1214];
	// end inline asm
	add.s32 	%r1219, %r1204, 1536;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r1264, %r1265, %r1266, %r1267}, [%r1219];
	// end inline asm
	bar.sync 	0;
	st.shared.v4.b32 	[%r1298], {%r1393, %r1394, %r1395, %r1396};
	st.shared.v4.b32 	[%r1300], {%r1397, %r1398, %r1399, %r1400};
	st.shared.v4.b32 	[%r1302], {%r1401, %r1402, %r1403, %r1404};
	st.shared.v4.b32 	[%r1304], {%r1405, %r1406, %r1407, %r1408};
	bar.sync 	0;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r1244, %r1245, %r1246, %r1247}, [%r1204];
	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r1252, %r1253, %r1254, %r1255}, [%r1209];
	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r1260, %r1261, %r1262, %r1263}, [%r1214];
	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r1268, %r1269, %r1270, %r1271}, [%r1219];
	// end inline asm
	// begin inline asm
	@%p16 st.global.v4.b32 [ %rd51 + 0 ], { %r1240, %r1241, %r1242, %r1243 };
	// end inline asm
	// begin inline asm
	@%p17 st.global.v4.b32 [ %rd52 + 0 ], { %r1244, %r1245, %r1246, %r1247 };
	// end inline asm
	// begin inline asm
	@%p18 st.global.v4.b32 [ %rd53 + 0 ], { %r1248, %r1249, %r1250, %r1251 };
	// end inline asm
	// begin inline asm
	@%p19 st.global.v4.b32 [ %rd54 + 0 ], { %r1252, %r1253, %r1254, %r1255 };
	// end inline asm
	// begin inline asm
	@%p20 st.global.v4.b32 [ %rd55 + 0 ], { %r1256, %r1257, %r1258, %r1259 };
	// end inline asm
	// begin inline asm
	@%p21 st.global.v4.b32 [ %rd56 + 0 ], { %r1260, %r1261, %r1262, %r1263 };
	// end inline asm
	// begin inline asm
	@%p22 st.global.v4.b32 [ %rd57 + 0 ], { %r1264, %r1265, %r1266, %r1267 };
	// end inline asm
	// begin inline asm
	@%p23 st.global.v4.b32 [ %rd58 + 0 ], { %r1268, %r1269, %r1270, %r1271 };
	// end inline asm
	ret;
                                        // -- End function
}
