//
// Generated by LLVM NVPTX Back-End
//

.version 8.7
.target sm_90a
.address_size 64

	// .globl	_layer_norm_fwd_fused   // -- Begin function _layer_norm_fwd_fused
.extern .shared .align 16 .b8 global_smem[];
.global .align 1 .b8 _$_str[11] = {95, 95, 67, 85, 68, 65, 95, 70, 84, 90};
.global .align 1 .b8 _$_str_$_2[17] = {95, 95, 67, 85, 68, 65, 95, 80, 82, 69, 67, 95, 83, 81, 82, 84};
                                        // @_layer_norm_fwd_fused
.visible .entry _layer_norm_fwd_fused(
	.param .u64 .ptr .global .align 1 _layer_norm_fwd_fused_param_0,
	.param .u64 .ptr .global .align 1 _layer_norm_fwd_fused_param_1,
	.param .u64 .ptr .global .align 1 _layer_norm_fwd_fused_param_2,
	.param .u64 .ptr .global .align 1 _layer_norm_fwd_fused_param_3,
	.param .u64 .ptr .global .align 1 _layer_norm_fwd_fused_param_4,
	.param .u64 .ptr .global .align 1 _layer_norm_fwd_fused_param_5,
	.param .u32 _layer_norm_fwd_fused_param_6,
	.param .u32 _layer_norm_fwd_fused_param_7,
	.param .f32 _layer_norm_fwd_fused_param_8,
	.param .u64 .ptr .global .align 1 _layer_norm_fwd_fused_param_9,
	.param .u64 .ptr .global .align 1 _layer_norm_fwd_fused_param_10
)
.reqntid 256
{
	.reg .pred 	%p<59>;
	.reg .b16 	%rs<161>;
	.reg .b32 	%r<900>;
	.reg .b64 	%rd<301>;

// %bb.0:
	ld.param.b32 	%r66, [_layer_norm_fwd_fused_param_7];
	ld.param.b64 	%rd137, [_layer_norm_fwd_fused_param_0];
	mov.u32 	%r1, %ctaid.x;
	ld.param.b32 	%r68, [_layer_norm_fwd_fused_param_6];
	mul.lo.s32 	%r69, %r68, %r1;
	mul.wide.s32 	%rd139, %r69, 2;
	add.s64 	%rd2, %rd137, %rd139;
	mov.u32 	%r2, %tid.x;
	and.b32 	%r3, %r2, 31;
	shr.u32 	%r4, %r2, 5;
	and.b32 	%r5, %r2, 255;
	shl.b32 	%r6, %r5, 3;
	mov.b32 	%r70, 0f00000000;
	mov.b64 	%rd285, {%r70, %r70};
	setp.lt.s32 	%p1, %r66, 1;
	shl.b32 	%r895, %r5, 4;
	mov.b32 	%r896, global_smem;
	cvt.u64.u32 	%rd236, %r6;
	mov.b64 	%rd253, %rd285;
	mov.b64 	%rd254, %rd285;
	mov.b64 	%rd255, %rd285;
	mov.b64 	%rd256, %rd285;
	mov.b64 	%rd257, %rd285;
	mov.b64 	%rd258, %rd285;
	mov.b64 	%rd259, %rd285;
	mov.b64 	%rd260, %rd285;
	mov.b64 	%rd261, %rd285;
	mov.b64 	%rd262, %rd285;
	mov.b64 	%rd263, %rd285;
	mov.b64 	%rd264, %rd285;
	mov.b64 	%rd265, %rd285;
	mov.b64 	%rd266, %rd285;
	mov.b64 	%rd267, %rd285;
	mov.b64 	%rd268, %rd285;
	@%p1 bra 	$L__BB0_3;
// %bb.1:                               // %.lr.ph
	add.s32 	%r75, %r896, %r895;
	add.s32 	%r77, %r75, 4096;
	add.s32 	%r79, %r75, 8192;
	add.s32 	%r81, %r75, 12288;
	mad.lo.s32 	%r11, %r5, -14, %r75;
	mov.b64 	%rd253, {%r70, %r70};
	mov.b32 	%r897, 0;
	mov.b64 	%rd254, %rd253;
	mov.b64 	%rd255, %rd253;
	mov.b64 	%rd256, %rd253;
	mov.b64 	%rd257, %rd253;
	mov.b64 	%rd258, %rd253;
	mov.b64 	%rd259, %rd253;
	mov.b64 	%rd260, %rd253;
	mov.b64 	%rd261, %rd253;
	mov.b64 	%rd262, %rd253;
	mov.b64 	%rd263, %rd253;
	mov.b64 	%rd264, %rd253;
	mov.b64 	%rd265, %rd253;
	mov.b64 	%rd266, %rd253;
	mov.b64 	%rd267, %rd253;
	mov.b64 	%rd268, %rd253;
$L__BB0_2:                              // =>This Inner Loop Header: Depth=1
	add.s32 	%r83, %r6, %r897;
	add.s32 	%r84, %r83, 2048;
	add.s32 	%r85, %r83, 4096;
	add.s32 	%r86, %r83, 6144;
	setp.lt.s32 	%p2, %r83, %r66;
	setp.lt.s32 	%p3, %r84, %r66;
	setp.lt.s32 	%p4, %r85, %r66;
	setp.lt.s32 	%p5, %r86, %r66;
	mad.wide.s32 	%rd156, %r83, 2, %rd2;
	cvt.s64.s32 	%rd160, %r897;
	add.s64 	%rd162, %rd160, %rd236;
	shl.b64 	%rd163, %rd162, 1;
	add.s64 	%rd164, %rd2, %rd163;
	add.s64 	%rd157, %rd164, 4096;
	add.s64 	%rd158, %rd164, 8192;
	add.s64 	%rd159, %rd164, 12288;
	bar.sync 	0;
	selp.b32 	%r76, 16, 0, %p2;
	// begin inline asm
	cp.async.cg.shared.global [ %r75 + 0 ], [ %rd156 + 0 ], 0x10, %r76;
	// end inline asm
	selp.b32 	%r78, 16, 0, %p3;
	// begin inline asm
	cp.async.cg.shared.global [ %r77 + 0 ], [ %rd157 + 0 ], 0x10, %r78;
	// end inline asm
	selp.b32 	%r80, 16, 0, %p4;
	// begin inline asm
	cp.async.cg.shared.global [ %r79 + 0 ], [ %rd158 + 0 ], 0x10, %r80;
	// end inline asm
	selp.b32 	%r82, 16, 0, %p5;
	// begin inline asm
	cp.async.cg.shared.global [ %r81 + 0 ], [ %rd159 + 0 ], 0x10, %r82;
	// end inline asm
	cp.async.commit_group;
	cp.async.wait_group 	0;
	bar.sync 	0;
	ld.shared.b16 	%rs1, [%r11];
	ld.shared.b16 	%rs2, [%r11+512];
	ld.shared.b16 	%rs3, [%r11+1024];
	ld.shared.b16 	%rs4, [%r11+1536];
	ld.shared.b16 	%rs5, [%r11+2048];
	ld.shared.b16 	%rs6, [%r11+2560];
	ld.shared.b16 	%rs7, [%r11+3072];
	ld.shared.b16 	%rs8, [%r11+3584];
	ld.shared.b16 	%rs9, [%r11+4096];
	ld.shared.b16 	%rs10, [%r11+4608];
	ld.shared.b16 	%rs11, [%r11+5120];
	ld.shared.b16 	%rs12, [%r11+5632];
	ld.shared.b16 	%rs13, [%r11+6144];
	ld.shared.b16 	%rs14, [%r11+6656];
	ld.shared.b16 	%rs15, [%r11+7168];
	ld.shared.b16 	%rs16, [%r11+7680];
	ld.shared.b16 	%rs17, [%r11+8192];
	ld.shared.b16 	%rs18, [%r11+8704];
	ld.shared.b16 	%rs19, [%r11+9216];
	ld.shared.b16 	%rs20, [%r11+9728];
	ld.shared.b16 	%rs21, [%r11+10240];
	ld.shared.b16 	%rs22, [%r11+10752];
	ld.shared.b16 	%rs23, [%r11+11264];
	ld.shared.b16 	%rs24, [%r11+11776];
	ld.shared.b16 	%rs25, [%r11+12288];
	ld.shared.b16 	%rs26, [%r11+12800];
	ld.shared.b16 	%rs27, [%r11+13312];
	ld.shared.b16 	%rs28, [%r11+13824];
	ld.shared.b16 	%rs29, [%r11+14336];
	ld.shared.b16 	%rs30, [%r11+14848];
	ld.shared.b16 	%rs31, [%r11+15360];
	ld.shared.b16 	%rs32, [%r11+15872];
	cvt.f32.f16 	%r87, %rs31;
	cvt.f32.f16 	%r88, %rs32;
	cvt.f32.f16 	%r89, %rs29;
	cvt.f32.f16 	%r90, %rs30;
	cvt.f32.f16 	%r91, %rs27;
	cvt.f32.f16 	%r92, %rs28;
	cvt.f32.f16 	%r93, %rs1;
	cvt.f32.f16 	%r94, %rs2;
	cvt.f32.f16 	%r95, %rs3;
	cvt.f32.f16 	%r96, %rs4;
	cvt.f32.f16 	%r97, %rs5;
	cvt.f32.f16 	%r98, %rs6;
	cvt.f32.f16 	%r99, %rs7;
	cvt.f32.f16 	%r100, %rs8;
	cvt.f32.f16 	%r101, %rs9;
	cvt.f32.f16 	%r102, %rs10;
	cvt.f32.f16 	%r103, %rs11;
	cvt.f32.f16 	%r104, %rs12;
	cvt.f32.f16 	%r105, %rs13;
	cvt.f32.f16 	%r106, %rs14;
	cvt.f32.f16 	%r107, %rs15;
	cvt.f32.f16 	%r108, %rs16;
	cvt.f32.f16 	%r109, %rs17;
	cvt.f32.f16 	%r110, %rs18;
	cvt.f32.f16 	%r111, %rs19;
	cvt.f32.f16 	%r112, %rs20;
	cvt.f32.f16 	%r113, %rs21;
	cvt.f32.f16 	%r114, %rs22;
	cvt.f32.f16 	%r115, %rs23;
	cvt.f32.f16 	%r116, %rs24;
	cvt.f32.f16 	%r117, %rs25;
	cvt.f32.f16 	%r118, %rs26;
	mov.b64 	{%r119, %r120}, %rd265;
	add.f32 	%r121, %r120, %r118;
	add.f32 	%r122, %r119, %r117;
	mov.b64 	%rd265, {%r122, %r121};
	mov.b64 	{%r123, %r124}, %rd264;
	add.f32 	%r125, %r124, %r116;
	add.f32 	%r126, %r123, %r115;
	mov.b64 	%rd264, {%r126, %r125};
	mov.b64 	{%r127, %r128}, %rd263;
	add.f32 	%r129, %r128, %r114;
	add.f32 	%r130, %r127, %r113;
	mov.b64 	%rd263, {%r130, %r129};
	mov.b64 	{%r131, %r132}, %rd262;
	add.f32 	%r133, %r132, %r112;
	add.f32 	%r134, %r131, %r111;
	mov.b64 	%rd262, {%r134, %r133};
	mov.b64 	{%r135, %r136}, %rd261;
	add.f32 	%r137, %r136, %r110;
	add.f32 	%r138, %r135, %r109;
	mov.b64 	%rd261, {%r138, %r137};
	mov.b64 	{%r139, %r140}, %rd260;
	add.f32 	%r141, %r140, %r108;
	add.f32 	%r142, %r139, %r107;
	mov.b64 	%rd260, {%r142, %r141};
	mov.b64 	{%r143, %r144}, %rd259;
	add.f32 	%r145, %r144, %r106;
	add.f32 	%r146, %r143, %r105;
	mov.b64 	%rd259, {%r146, %r145};
	mov.b64 	{%r147, %r148}, %rd258;
	add.f32 	%r149, %r148, %r104;
	add.f32 	%r150, %r147, %r103;
	mov.b64 	%rd258, {%r150, %r149};
	mov.b64 	{%r151, %r152}, %rd257;
	add.f32 	%r153, %r152, %r102;
	add.f32 	%r154, %r151, %r101;
	mov.b64 	%rd257, {%r154, %r153};
	mov.b64 	{%r155, %r156}, %rd256;
	add.f32 	%r157, %r156, %r100;
	add.f32 	%r158, %r155, %r99;
	mov.b64 	%rd256, {%r158, %r157};
	mov.b64 	{%r159, %r160}, %rd255;
	add.f32 	%r161, %r160, %r98;
	add.f32 	%r162, %r159, %r97;
	mov.b64 	%rd255, {%r162, %r161};
	mov.b64 	{%r163, %r164}, %rd254;
	add.f32 	%r165, %r164, %r96;
	add.f32 	%r166, %r163, %r95;
	mov.b64 	%rd254, {%r166, %r165};
	mov.b64 	{%r167, %r168}, %rd253;
	add.f32 	%r169, %r168, %r94;
	add.f32 	%r170, %r167, %r93;
	mov.b64 	%rd253, {%r170, %r169};
	mov.b64 	{%r171, %r172}, %rd266;
	add.f32 	%r173, %r172, %r92;
	add.f32 	%r174, %r171, %r91;
	mov.b64 	%rd266, {%r174, %r173};
	mov.b64 	{%r175, %r176}, %rd267;
	add.f32 	%r177, %r176, %r90;
	add.f32 	%r178, %r175, %r89;
	mov.b64 	%rd267, {%r178, %r177};
	mov.b64 	{%r179, %r180}, %rd268;
	add.f32 	%r181, %r180, %r88;
	add.f32 	%r182, %r179, %r87;
	mov.b64 	%rd268, {%r182, %r181};
	add.s32 	%r897, %r897, 8192;
	setp.lt.s32 	%p6, %r897, %r66;
	@%p6 bra 	$L__BB0_2;
$L__BB0_3:                              // %._crit_edge
	ld.param.b32 	%r67, [_layer_norm_fwd_fused_param_8];
	ld.param.b64 	%rd120, [_layer_norm_fwd_fused_param_5];
	ld.param.b64 	%rd119, [_layer_norm_fwd_fused_param_4];
	bar.sync 	0;
	mov.b64 	{%r189, %r190}, %rd253;
	add.f32 	%r191, %r189, %r190;
	mov.b64 	{%r192, %r193}, %rd254;
	add.f32 	%r194, %r192, %r191;
	add.f32 	%r195, %r193, %r194;
	mov.b64 	{%r196, %r197}, %rd255;
	add.f32 	%r198, %r196, %r195;
	add.f32 	%r199, %r197, %r198;
	mov.b64 	{%r200, %r201}, %rd256;
	add.f32 	%r202, %r200, %r199;
	add.f32 	%r203, %r201, %r202;
	mov.b64 	{%r204, %r205}, %rd257;
	add.f32 	%r206, %r204, %r203;
	add.f32 	%r207, %r205, %r206;
	mov.b64 	{%r208, %r209}, %rd258;
	add.f32 	%r210, %r208, %r207;
	add.f32 	%r211, %r209, %r210;
	mov.b64 	{%r212, %r213}, %rd259;
	add.f32 	%r214, %r212, %r211;
	add.f32 	%r215, %r213, %r214;
	mov.b64 	{%r216, %r217}, %rd260;
	add.f32 	%r218, %r216, %r215;
	add.f32 	%r219, %r217, %r218;
	mov.b64 	{%r220, %r221}, %rd261;
	add.f32 	%r222, %r220, %r219;
	add.f32 	%r223, %r221, %r222;
	mov.b64 	{%r224, %r225}, %rd262;
	add.f32 	%r226, %r224, %r223;
	add.f32 	%r227, %r225, %r226;
	mov.b64 	{%r228, %r229}, %rd263;
	add.f32 	%r230, %r228, %r227;
	add.f32 	%r231, %r229, %r230;
	mov.b64 	{%r232, %r233}, %rd264;
	add.f32 	%r234, %r232, %r231;
	add.f32 	%r235, %r233, %r234;
	mov.b64 	{%r236, %r237}, %rd265;
	add.f32 	%r238, %r236, %r235;
	add.f32 	%r239, %r237, %r238;
	mov.b64 	{%r240, %r241}, %rd266;
	add.f32 	%r242, %r240, %r239;
	add.f32 	%r243, %r241, %r242;
	mov.b64 	{%r244, %r245}, %rd267;
	add.f32 	%r246, %r244, %r243;
	add.f32 	%r247, %r245, %r246;
	mov.b64 	{%r248, %r249}, %rd268;
	add.f32 	%r250, %r248, %r247;
	add.f32 	%r251, %r249, %r250;
	shfl.sync.bfly.b32 	%r252, %r251, 16, 31, -1;
	add.f32 	%r253, %r251, %r252;
	shfl.sync.bfly.b32 	%r254, %r253, 8, 31, -1;
	add.f32 	%r255, %r253, %r254;
	shfl.sync.bfly.b32 	%r256, %r255, 4, 31, -1;
	add.f32 	%r257, %r255, %r256;
	shfl.sync.bfly.b32 	%r258, %r257, 2, 31, -1;
	add.f32 	%r259, %r257, %r258;
	shfl.sync.bfly.b32 	%r260, %r259, 1, 31, -1;
	add.f32 	%r184, %r259, %r260;
	and.b32 	%r261, %r4, 7;
	setp.eq.b32 	%p7, %r3, 0;
	shl.b32 	%r262, %r261, 2;
	add.s32 	%r513, %r896, %r262;
	// begin inline asm
	@%p7 st.shared.b32 [ %r513 + 0 ], %r184;
	// end inline asm
	bar.sync 	0;
	setp.lt.u32 	%p8, %r2, 8;
	shl.b32 	%r264, %r2, 2;
	add.s32 	%r517, %r896, %r264;
	// begin inline asm
	@%p8 ld.shared.b32 %r185, [ %r517 + 0 ];
	// end inline asm
	shfl.sync.bfly.b32 	%r265, %r185, 4, 31, -1;
	add.f32 	%r266, %r185, %r265;
	shfl.sync.bfly.b32 	%r267, %r266, 2, 31, -1;
	add.f32 	%r268, %r266, %r267;
	shfl.sync.bfly.b32 	%r269, %r268, 1, 31, -1;
	add.f32 	%r188, %r268, %r269;
	setp.eq.b32 	%p50, %r2, 0;
	// begin inline asm
	@%p50 st.shared.b32 [ %r517 + 0 ], %r188;
	// end inline asm
	bar.sync 	0;
	ld.shared.b32 	%r270, [global_smem];
	cvt.rn.f32.s32 	%r16, %r66;
	div.full.f32 	%r519, %r270, %r16;
	mov.b64 	%rd234, {%r519, %r519};
	mov.b64 	%rd286, %rd285;
	mov.b64 	%rd287, %rd285;
	mov.b64 	%rd288, %rd285;
	mov.b64 	%rd289, %rd285;
	mov.b64 	%rd290, %rd285;
	mov.b64 	%rd291, %rd285;
	mov.b64 	%rd292, %rd285;
	mov.b64 	%rd293, %rd285;
	mov.b64 	%rd294, %rd285;
	mov.b64 	%rd295, %rd285;
	mov.b64 	%rd296, %rd285;
	mov.b64 	%rd297, %rd285;
	mov.b64 	%rd298, %rd285;
	mov.b64 	%rd299, %rd285;
	mov.b64 	%rd300, %rd285;
	@%p1 bra 	$L__BB0_6;
// %bb.4:                               // %.lr.ph148
	add.s32 	%r276, %r896, %r895;
	add.s32 	%r278, %r276, 4096;
	add.s32 	%r280, %r276, 8192;
	add.s32 	%r282, %r276, 12288;
	mad.lo.s32 	%r22, %r5, -14, %r276;
	mov.b32 	%r275, 0f00000000;
	mov.b64 	%rd285, {%r275, %r275};
	mov.b32 	%r898, 0;
	mov.b64 	%rd286, %rd285;
	mov.b64 	%rd287, %rd285;
	mov.b64 	%rd288, %rd285;
	mov.b64 	%rd289, %rd285;
	mov.b64 	%rd290, %rd285;
	mov.b64 	%rd291, %rd285;
	mov.b64 	%rd292, %rd285;
	mov.b64 	%rd293, %rd285;
	mov.b64 	%rd294, %rd285;
	mov.b64 	%rd295, %rd285;
	mov.b64 	%rd296, %rd285;
	mov.b64 	%rd297, %rd285;
	mov.b64 	%rd298, %rd285;
	mov.b64 	%rd299, %rd285;
	mov.b64 	%rd300, %rd285;
$L__BB0_5:                              // =>This Inner Loop Header: Depth=1
	add.s32 	%r284, %r6, %r898;
	add.s32 	%r285, %r284, 2048;
	add.s32 	%r286, %r284, 4096;
	add.s32 	%r287, %r284, 6144;
	or.b32 	%r288, %r898, %r2;
	or.b32 	%r289, %r898, %r5;
	setp.lt.s32 	%p11, %r284, %r66;
	setp.lt.s32 	%p12, %r285, %r66;
	setp.lt.s32 	%p13, %r286, %r66;
	setp.lt.s32 	%p14, %r287, %r66;
	mad.wide.s32 	%rd197, %r284, 2, %rd2;
	cvt.s64.s32 	%rd201, %r898;
	add.s64 	%rd203, %rd201, %rd236;
	shl.b64 	%rd204, %rd203, 1;
	add.s64 	%rd205, %rd2, %rd204;
	add.s64 	%rd198, %rd205, 4096;
	add.s64 	%rd199, %rd205, 8192;
	add.s64 	%rd200, %rd205, 12288;
	bar.sync 	0;
	selp.b32 	%r277, 16, 0, %p11;
	// begin inline asm
	cp.async.cg.shared.global [ %r276 + 0 ], [ %rd197 + 0 ], 0x10, %r277;
	// end inline asm
	selp.b32 	%r279, 16, 0, %p12;
	// begin inline asm
	cp.async.cg.shared.global [ %r278 + 0 ], [ %rd198 + 0 ], 0x10, %r279;
	// end inline asm
	selp.b32 	%r281, 16, 0, %p13;
	// begin inline asm
	cp.async.cg.shared.global [ %r280 + 0 ], [ %rd199 + 0 ], 0x10, %r281;
	// end inline asm
	selp.b32 	%r283, 16, 0, %p14;
	// begin inline asm
	cp.async.cg.shared.global [ %r282 + 0 ], [ %rd200 + 0 ], 0x10, %r283;
	// end inline asm
	cp.async.commit_group;
	cp.async.wait_group 	0;
	bar.sync 	0;
	ld.shared.b16 	%rs33, [%r22];
	ld.shared.b16 	%rs34, [%r22+512];
	ld.shared.b16 	%rs35, [%r22+1024];
	ld.shared.b16 	%rs36, [%r22+1536];
	ld.shared.b16 	%rs37, [%r22+2048];
	ld.shared.b16 	%rs38, [%r22+2560];
	ld.shared.b16 	%rs39, [%r22+3072];
	ld.shared.b16 	%rs40, [%r22+3584];
	ld.shared.b16 	%rs41, [%r22+4096];
	ld.shared.b16 	%rs42, [%r22+4608];
	ld.shared.b16 	%rs43, [%r22+5120];
	ld.shared.b16 	%rs44, [%r22+5632];
	ld.shared.b16 	%rs45, [%r22+6144];
	ld.shared.b16 	%rs46, [%r22+6656];
	ld.shared.b16 	%rs47, [%r22+7168];
	ld.shared.b16 	%rs48, [%r22+7680];
	ld.shared.b16 	%rs49, [%r22+8192];
	ld.shared.b16 	%rs50, [%r22+8704];
	ld.shared.b16 	%rs51, [%r22+9216];
	ld.shared.b16 	%rs52, [%r22+9728];
	ld.shared.b16 	%rs53, [%r22+10240];
	ld.shared.b16 	%rs54, [%r22+10752];
	ld.shared.b16 	%rs55, [%r22+11264];
	ld.shared.b16 	%rs56, [%r22+11776];
	ld.shared.b16 	%rs57, [%r22+12288];
	ld.shared.b16 	%rs58, [%r22+12800];
	ld.shared.b16 	%rs59, [%r22+13312];
	ld.shared.b16 	%rs60, [%r22+13824];
	ld.shared.b16 	%rs61, [%r22+14336];
	ld.shared.b16 	%rs62, [%r22+14848];
	ld.shared.b16 	%rs63, [%r22+15360];
	ld.shared.b16 	%rs64, [%r22+15872];
	or.b32 	%r290, %r289, 4096;
	or.b32 	%r291, %r288, 3840;
	or.b32 	%r292, %r289, 3584;
	or.b32 	%r293, %r289, 256;
	or.b32 	%r294, %r289, 512;
	or.b32 	%r295, %r288, 768;
	or.b32 	%r296, %r289, 1024;
	or.b32 	%r297, %r289, 1280;
	or.b32 	%r298, %r289, 1536;
	or.b32 	%r299, %r288, 1792;
	or.b32 	%r300, %r289, 2048;
	or.b32 	%r301, %r289, 2304;
	or.b32 	%r302, %r289, 2560;
	or.b32 	%r303, %r288, 2816;
	or.b32 	%r304, %r289, 3072;
	or.b32 	%r305, %r289, 3328;
	or.b32 	%r306, %r289, 6400;
	or.b32 	%r307, %r289, 6656;
	or.b32 	%r308, %r288, 6912;
	or.b32 	%r309, %r289, 7168;
	or.b32 	%r310, %r289, 7424;
	or.b32 	%r311, %r289, 7680;
	or.b32 	%r312, %r288, 7936;
	or.b32 	%r313, %r289, 6144;
	or.b32 	%r314, %r288, 5888;
	or.b32 	%r315, %r289, 5632;
	or.b32 	%r316, %r289, 5376;
	or.b32 	%r317, %r289, 5120;
	or.b32 	%r318, %r288, 4864;
	or.b32 	%r319, %r289, 4608;
	or.b32 	%r320, %r289, 4352;
	setp.lt.s32 	%p15, %r305, %r66;
	setp.lt.s32 	%p16, %r304, %r66;
	setp.lt.s32 	%p17, %r303, %r66;
	setp.lt.s32 	%p18, %r302, %r66;
	setp.lt.s32 	%p19, %r301, %r66;
	setp.lt.s32 	%p20, %r300, %r66;
	setp.lt.s32 	%p21, %r299, %r66;
	setp.lt.s32 	%p22, %r298, %r66;
	setp.lt.s32 	%p23, %r297, %r66;
	setp.lt.s32 	%p24, %r296, %r66;
	setp.lt.s32 	%p25, %r295, %r66;
	setp.lt.s32 	%p26, %r294, %r66;
	setp.lt.s32 	%p27, %r293, %r66;
	setp.lt.s32 	%p28, %r289, %r66;
	setp.lt.s32 	%p29, %r292, %r66;
	setp.lt.s32 	%p30, %r291, %r66;
	setp.lt.s32 	%p31, %r290, %r66;
	setp.lt.s32 	%p32, %r320, %r66;
	setp.lt.s32 	%p33, %r319, %r66;
	setp.lt.s32 	%p34, %r318, %r66;
	setp.lt.s32 	%p35, %r317, %r66;
	setp.lt.s32 	%p36, %r316, %r66;
	setp.lt.s32 	%p37, %r315, %r66;
	setp.lt.s32 	%p38, %r314, %r66;
	setp.lt.s32 	%p39, %r313, %r66;
	setp.lt.s32 	%p40, %r306, %r66;
	setp.lt.s32 	%p41, %r307, %r66;
	setp.lt.s32 	%p42, %r308, %r66;
	setp.lt.s32 	%p43, %r309, %r66;
	setp.lt.s32 	%p44, %r310, %r66;
	setp.lt.s32 	%p45, %r311, %r66;
	setp.lt.s32 	%p46, %r312, %r66;
	cvt.f32.f16 	%r321, %rs64;
	cvt.f32.f16 	%r322, %rs63;
	cvt.f32.f16 	%r323, %rs62;
	cvt.f32.f16 	%r324, %rs61;
	cvt.f32.f16 	%r325, %rs60;
	cvt.f32.f16 	%r326, %rs59;
	cvt.f32.f16 	%r327, %rs58;
	cvt.f32.f16 	%r328, %rs57;
	cvt.f32.f16 	%r329, %rs56;
	cvt.f32.f16 	%r330, %rs55;
	cvt.f32.f16 	%r331, %rs54;
	cvt.f32.f16 	%r332, %rs53;
	cvt.f32.f16 	%r333, %rs52;
	cvt.f32.f16 	%r334, %rs51;
	cvt.f32.f16 	%r335, %rs50;
	cvt.f32.f16 	%r336, %rs49;
	cvt.f32.f16 	%r337, %rs48;
	cvt.f32.f16 	%r338, %rs47;
	cvt.f32.f16 	%r339, %rs33;
	cvt.f32.f16 	%r340, %rs34;
	cvt.f32.f16 	%r341, %rs35;
	cvt.f32.f16 	%r342, %rs36;
	cvt.f32.f16 	%r343, %rs37;
	cvt.f32.f16 	%r344, %rs38;
	cvt.f32.f16 	%r345, %rs39;
	cvt.f32.f16 	%r346, %rs40;
	cvt.f32.f16 	%r347, %rs41;
	cvt.f32.f16 	%r348, %rs42;
	cvt.f32.f16 	%r349, %rs43;
	cvt.f32.f16 	%r350, %rs44;
	cvt.f32.f16 	%r351, %rs45;
	cvt.f32.f16 	%r352, %rs46;
	mov.b64 	{%r353, %r354}, %rd234;
	sub.f32 	%r355, %r352, %r354;
	sub.f32 	%r356, %r351, %r353;
	sub.f32 	%r359, %r350, %r354;
	sub.f32 	%r360, %r349, %r353;
	sub.f32 	%r363, %r348, %r354;
	sub.f32 	%r364, %r347, %r353;
	sub.f32 	%r367, %r346, %r354;
	sub.f32 	%r368, %r345, %r353;
	sub.f32 	%r371, %r344, %r354;
	sub.f32 	%r372, %r343, %r353;
	sub.f32 	%r375, %r342, %r354;
	sub.f32 	%r376, %r341, %r353;
	sub.f32 	%r379, %r340, %r354;
	sub.f32 	%r380, %r339, %r353;
	sub.f32 	%r383, %r338, %r353;
	sub.f32 	%r384, %r337, %r354;
	sub.f32 	%r387, %r336, %r353;
	sub.f32 	%r388, %r335, %r354;
	sub.f32 	%r391, %r334, %r353;
	sub.f32 	%r392, %r333, %r354;
	sub.f32 	%r395, %r332, %r353;
	sub.f32 	%r396, %r331, %r354;
	sub.f32 	%r399, %r330, %r353;
	sub.f32 	%r400, %r329, %r354;
	sub.f32 	%r403, %r328, %r353;
	sub.f32 	%r404, %r327, %r354;
	sub.f32 	%r407, %r326, %r353;
	sub.f32 	%r408, %r325, %r354;
	sub.f32 	%r411, %r324, %r353;
	sub.f32 	%r412, %r323, %r354;
	sub.f32 	%r415, %r322, %r353;
	sub.f32 	%r416, %r321, %r354;
	selp.f32 	%r417, %r416, 0f00000000, %p46;
	selp.f32 	%r418, %r415, 0f00000000, %p45;
	selp.f32 	%r419, %r412, 0f00000000, %p44;
	selp.f32 	%r420, %r411, 0f00000000, %p43;
	selp.f32 	%r421, %r408, 0f00000000, %p42;
	selp.f32 	%r422, %r407, 0f00000000, %p41;
	selp.f32 	%r423, %r404, 0f00000000, %p40;
	selp.f32 	%r424, %r403, 0f00000000, %p39;
	selp.f32 	%r425, %r400, 0f00000000, %p38;
	selp.f32 	%r426, %r399, 0f00000000, %p37;
	selp.f32 	%r427, %r396, 0f00000000, %p36;
	selp.f32 	%r428, %r395, 0f00000000, %p35;
	selp.f32 	%r429, %r392, 0f00000000, %p34;
	selp.f32 	%r430, %r391, 0f00000000, %p33;
	selp.f32 	%r431, %r388, 0f00000000, %p32;
	selp.f32 	%r432, %r387, 0f00000000, %p31;
	selp.f32 	%r433, %r384, 0f00000000, %p30;
	selp.f32 	%r434, %r383, 0f00000000, %p29;
	selp.f32 	%r435, %r380, 0f00000000, %p28;
	selp.f32 	%r436, %r379, 0f00000000, %p27;
	selp.f32 	%r437, %r376, 0f00000000, %p26;
	selp.f32 	%r438, %r375, 0f00000000, %p25;
	selp.f32 	%r439, %r372, 0f00000000, %p24;
	selp.f32 	%r440, %r371, 0f00000000, %p23;
	selp.f32 	%r441, %r368, 0f00000000, %p22;
	selp.f32 	%r442, %r367, 0f00000000, %p21;
	selp.f32 	%r443, %r364, 0f00000000, %p20;
	selp.f32 	%r444, %r363, 0f00000000, %p19;
	selp.f32 	%r445, %r360, 0f00000000, %p18;
	selp.f32 	%r446, %r359, 0f00000000, %p17;
	selp.f32 	%r447, %r356, 0f00000000, %p16;
	selp.f32 	%r448, %r355, 0f00000000, %p15;
	mov.b64 	{%r449, %r450}, %rd291;
	fma.rn.f32 	%r451, %r448, %r448, %r450;
	fma.rn.f32 	%r452, %r447, %r447, %r449;
	mov.b64 	%rd291, {%r452, %r451};
	mov.b64 	{%r453, %r454}, %rd290;
	fma.rn.f32 	%r455, %r446, %r446, %r454;
	fma.rn.f32 	%r456, %r445, %r445, %r453;
	mov.b64 	%rd290, {%r456, %r455};
	mov.b64 	{%r457, %r458}, %rd289;
	fma.rn.f32 	%r459, %r444, %r444, %r458;
	fma.rn.f32 	%r460, %r443, %r443, %r457;
	mov.b64 	%rd289, {%r460, %r459};
	mov.b64 	{%r461, %r462}, %rd288;
	fma.rn.f32 	%r463, %r442, %r442, %r462;
	fma.rn.f32 	%r464, %r441, %r441, %r461;
	mov.b64 	%rd288, {%r464, %r463};
	mov.b64 	{%r465, %r466}, %rd287;
	fma.rn.f32 	%r467, %r440, %r440, %r466;
	fma.rn.f32 	%r468, %r439, %r439, %r465;
	mov.b64 	%rd287, {%r468, %r467};
	mov.b64 	{%r469, %r470}, %rd286;
	fma.rn.f32 	%r471, %r438, %r438, %r470;
	fma.rn.f32 	%r472, %r437, %r437, %r469;
	mov.b64 	%rd286, {%r472, %r471};
	mov.b64 	{%r473, %r474}, %rd285;
	fma.rn.f32 	%r475, %r436, %r436, %r474;
	fma.rn.f32 	%r476, %r435, %r435, %r473;
	mov.b64 	%rd285, {%r476, %r475};
	mov.b64 	{%r477, %r478}, %rd292;
	fma.rn.f32 	%r479, %r434, %r434, %r477;
	fma.rn.f32 	%r480, %r433, %r433, %r478;
	mov.b64 	%rd292, {%r479, %r480};
	mov.b64 	{%r481, %r482}, %rd293;
	fma.rn.f32 	%r483, %r432, %r432, %r481;
	fma.rn.f32 	%r484, %r431, %r431, %r482;
	mov.b64 	%rd293, {%r483, %r484};
	mov.b64 	{%r485, %r486}, %rd294;
	fma.rn.f32 	%r487, %r430, %r430, %r485;
	fma.rn.f32 	%r488, %r429, %r429, %r486;
	mov.b64 	%rd294, {%r487, %r488};
	mov.b64 	{%r489, %r490}, %rd295;
	fma.rn.f32 	%r491, %r428, %r428, %r489;
	fma.rn.f32 	%r492, %r427, %r427, %r490;
	mov.b64 	%rd295, {%r491, %r492};
	mov.b64 	{%r493, %r494}, %rd296;
	fma.rn.f32 	%r495, %r426, %r426, %r493;
	fma.rn.f32 	%r496, %r425, %r425, %r494;
	mov.b64 	%rd296, {%r495, %r496};
	mov.b64 	{%r497, %r498}, %rd297;
	fma.rn.f32 	%r499, %r424, %r424, %r497;
	fma.rn.f32 	%r500, %r423, %r423, %r498;
	mov.b64 	%rd297, {%r499, %r500};
	mov.b64 	{%r501, %r502}, %rd298;
	fma.rn.f32 	%r503, %r422, %r422, %r501;
	fma.rn.f32 	%r504, %r421, %r421, %r502;
	mov.b64 	%rd298, {%r503, %r504};
	mov.b64 	{%r505, %r506}, %rd299;
	fma.rn.f32 	%r507, %r420, %r420, %r505;
	fma.rn.f32 	%r508, %r419, %r419, %r506;
	mov.b64 	%rd299, {%r507, %r508};
	mov.b64 	{%r509, %r510}, %rd300;
	fma.rn.f32 	%r511, %r418, %r418, %r509;
	fma.rn.f32 	%r512, %r417, %r417, %r510;
	mov.b64 	%rd300, {%r511, %r512};
	add.s32 	%r898, %r898, 8192;
	setp.lt.s32 	%p47, %r898, %r66;
	@%p47 bra 	$L__BB0_5;
$L__BB0_6:                              // %._crit_edge149
	bar.sync 	0;
	mov.b64 	{%r521, %r522}, %rd285;
	add.f32 	%r523, %r521, %r522;
	mov.b64 	{%r524, %r525}, %rd286;
	add.f32 	%r526, %r524, %r523;
	add.f32 	%r527, %r525, %r526;
	mov.b64 	{%r528, %r529}, %rd287;
	add.f32 	%r530, %r528, %r527;
	add.f32 	%r531, %r529, %r530;
	mov.b64 	{%r532, %r533}, %rd288;
	add.f32 	%r534, %r532, %r531;
	add.f32 	%r535, %r533, %r534;
	mov.b64 	{%r536, %r537}, %rd289;
	add.f32 	%r538, %r536, %r535;
	add.f32 	%r539, %r537, %r538;
	mov.b64 	{%r540, %r541}, %rd290;
	add.f32 	%r542, %r540, %r539;
	add.f32 	%r543, %r541, %r542;
	mov.b64 	{%r544, %r545}, %rd291;
	add.f32 	%r546, %r544, %r543;
	add.f32 	%r547, %r545, %r546;
	mov.b64 	{%r548, %r549}, %rd292;
	add.f32 	%r550, %r548, %r547;
	add.f32 	%r551, %r549, %r550;
	mov.b64 	{%r552, %r553}, %rd293;
	add.f32 	%r554, %r552, %r551;
	add.f32 	%r555, %r553, %r554;
	mov.b64 	{%r556, %r557}, %rd294;
	add.f32 	%r558, %r556, %r555;
	add.f32 	%r559, %r557, %r558;
	mov.b64 	{%r560, %r561}, %rd295;
	add.f32 	%r562, %r560, %r559;
	add.f32 	%r563, %r561, %r562;
	mov.b64 	{%r564, %r565}, %rd296;
	add.f32 	%r566, %r564, %r563;
	add.f32 	%r567, %r565, %r566;
	mov.b64 	{%r568, %r569}, %rd297;
	add.f32 	%r570, %r568, %r567;
	add.f32 	%r571, %r569, %r570;
	mov.b64 	{%r572, %r573}, %rd298;
	add.f32 	%r574, %r572, %r571;
	add.f32 	%r575, %r573, %r574;
	mov.b64 	{%r576, %r577}, %rd299;
	add.f32 	%r578, %r576, %r575;
	add.f32 	%r579, %r577, %r578;
	mov.b64 	{%r580, %r581}, %rd300;
	add.f32 	%r582, %r580, %r579;
	add.f32 	%r583, %r581, %r582;
	shfl.sync.bfly.b32 	%r584, %r583, 16, 31, -1;
	add.f32 	%r585, %r583, %r584;
	shfl.sync.bfly.b32 	%r586, %r585, 8, 31, -1;
	add.f32 	%r587, %r585, %r586;
	shfl.sync.bfly.b32 	%r588, %r587, 4, 31, -1;
	add.f32 	%r589, %r587, %r588;
	shfl.sync.bfly.b32 	%r590, %r589, 2, 31, -1;
	add.f32 	%r591, %r589, %r590;
	shfl.sync.bfly.b32 	%r592, %r591, 1, 31, -1;
	add.f32 	%r514, %r591, %r592;
	// begin inline asm
	@%p7 st.shared.b32 [ %r513 + 0 ], %r514;
	// end inline asm
	bar.sync 	0;
	// begin inline asm
	@%p8 ld.shared.b32 %r515, [ %r517 + 0 ];
	// end inline asm
	shfl.sync.bfly.b32 	%r593, %r515, 4, 31, -1;
	add.f32 	%r594, %r515, %r593;
	shfl.sync.bfly.b32 	%r595, %r594, 2, 31, -1;
	add.f32 	%r596, %r594, %r595;
	shfl.sync.bfly.b32 	%r597, %r596, 1, 31, -1;
	add.f32 	%r518, %r596, %r597;
	// begin inline asm
	@%p50 st.shared.b32 [ %r517 + 0 ], %r518;
	// end inline asm
	bar.sync 	0;
	ld.shared.b32 	%r598, [global_smem];
	div.full.f32 	%r599, %r598, %r16;
	add.f32 	%r600, %r67, %r599;
	sqrt.approx.ftz.f32 	%r601, %r600;
	mov.b32 	%r602, 0f3F800000;
	div.full.f32 	%r520, %r602, %r601;
	mul.wide.u32 	%rd208, %r1, 4;
	add.s64 	%rd206, %rd119, %rd208;
	// begin inline asm
	@%p50 st.global.b32 [ %rd206 + 0 ], { %r519 };
	// end inline asm
	add.s64 	%rd207, %rd120, %rd208;
	// begin inline asm
	@%p50 st.global.b32 [ %rd207 + 0 ], { %r520 };
	// end inline asm
	@%p1 bra 	$L__BB0_9;
// %bb.7:                               // %.lr.ph151
	ld.param.b64 	%rd138, [_layer_norm_fwd_fused_param_1];
	ld.param.b64 	%rd118, [_layer_norm_fwd_fused_param_3];
	ld.param.b64 	%rd117, [_layer_norm_fwd_fused_param_2];
	add.s64 	%rd1, %rd138, %rd139;
	add.s32 	%r606, %r896, %r895;
	add.s32 	%r608, %r606, 4096;
	add.s32 	%r610, %r606, 8192;
	add.s32 	%r612, %r606, 12288;
	mov.b64 	%rd115, {%r520, %r520};
	mov.b32 	%r899, 0;
$L__BB0_8:                              // =>This Inner Loop Header: Depth=1
	add.s32 	%r646, %r6, %r899;
	add.s32 	%r647, %r646, 2048;
	add.s32 	%r648, %r646, 4096;
	add.s32 	%r649, %r646, 6144;
	setp.lt.s32 	%p54, %r646, %r66;
	setp.lt.s32 	%p55, %r647, %r66;
	setp.lt.s32 	%p56, %r648, %r66;
	setp.lt.s32 	%p57, %r649, %r66;
	mul.wide.s32 	%rd225, %r646, 2;
	add.s64 	%rd209, %rd117, %rd225;
	cvt.s64.s32 	%rd226, %r899;
	add.s64 	%rd228, %rd226, %rd236;
	shl.b64 	%rd229, %rd228, 1;
	add.s64 	%rd230, %rd117, %rd229;
	add.s64 	%rd210, %rd230, 4096;
	add.s64 	%rd211, %rd230, 8192;
	add.s64 	%rd212, %rd230, 12288;
	bar.sync 	0;
	selp.b32 	%r607, 16, 0, %p54;
	// begin inline asm
	cp.async.cg.shared.global [ %r606 + 0 ], [ %rd209 + 0 ], 0x10, %r607;
	// end inline asm
	selp.b32 	%r609, 16, 0, %p55;
	// begin inline asm
	cp.async.cg.shared.global [ %r608 + 0 ], [ %rd210 + 0 ], 0x10, %r609;
	// end inline asm
	selp.b32 	%r611, 16, 0, %p56;
	// begin inline asm
	cp.async.cg.shared.global [ %r610 + 0 ], [ %rd211 + 0 ], 0x10, %r611;
	// end inline asm
	selp.b32 	%r613, 16, 0, %p57;
	// begin inline asm
	cp.async.cg.shared.global [ %r612 + 0 ], [ %rd212 + 0 ], 0x10, %r613;
	// end inline asm
	cp.async.commit_group;
	cp.async.wait_group 	0;
	bar.sync 	0;
	add.s64 	%rd213, %rd118, %rd225;
	add.s64 	%rd231, %rd118, %rd229;
	add.s64 	%rd214, %rd231, 4096;
	add.s64 	%rd215, %rd231, 8192;
	add.s64 	%rd216, %rd231, 12288;
	add.s64 	%rd217, %rd2, %rd225;
	add.s64 	%rd232, %rd2, %rd229;
	add.s64 	%rd218, %rd232, 4096;
	add.s64 	%rd219, %rd232, 8192;
	add.s64 	%rd220, %rd232, 12288;
	add.s64 	%rd221, %rd1, %rd225;
	add.s64 	%rd233, %rd1, %rd229;
	add.s64 	%rd222, %rd233, 4096;
	add.s64 	%rd223, %rd233, 8192;
	add.s64 	%rd224, %rd233, 12288;
	ld.shared.v4.b32 	{%r650, %r651, %r652, %r653}, [%r606];
	mov.b32 	{%rs65, %rs66}, %r653;
	cvt.f32.f16 	%r654, %rs66;
	cvt.f32.f16 	%r655, %rs65;
	mov.b32 	{%rs67, %rs68}, %r652;
	cvt.f32.f16 	%r656, %rs68;
	cvt.f32.f16 	%r657, %rs67;
	mov.b32 	{%rs69, %rs70}, %r651;
	cvt.f32.f16 	%r658, %rs70;
	cvt.f32.f16 	%r659, %rs69;
	mov.b32 	{%rs71, %rs72}, %r650;
	cvt.f32.f16 	%r660, %rs72;
	cvt.f32.f16 	%r661, %rs71;
	ld.shared.v4.b32 	{%r662, %r663, %r664, %r665}, [%r606+4096];
	mov.b32 	{%rs73, %rs74}, %r665;
	cvt.f32.f16 	%r666, %rs74;
	cvt.f32.f16 	%r667, %rs73;
	mov.b32 	{%rs75, %rs76}, %r664;
	cvt.f32.f16 	%r668, %rs76;
	cvt.f32.f16 	%r669, %rs75;
	mov.b32 	{%rs77, %rs78}, %r663;
	cvt.f32.f16 	%r670, %rs78;
	cvt.f32.f16 	%r671, %rs77;
	mov.b32 	{%rs79, %rs80}, %r662;
	cvt.f32.f16 	%r672, %rs80;
	cvt.f32.f16 	%r673, %rs79;
	ld.shared.v4.b32 	{%r674, %r675, %r676, %r677}, [%r606+8192];
	mov.b32 	{%rs81, %rs82}, %r677;
	cvt.f32.f16 	%r678, %rs82;
	cvt.f32.f16 	%r679, %rs81;
	mov.b32 	{%rs83, %rs84}, %r676;
	cvt.f32.f16 	%r680, %rs84;
	cvt.f32.f16 	%r681, %rs83;
	mov.b32 	{%rs85, %rs86}, %r675;
	cvt.f32.f16 	%r682, %rs86;
	cvt.f32.f16 	%r683, %rs85;
	mov.b32 	{%rs87, %rs88}, %r674;
	cvt.f32.f16 	%r684, %rs88;
	cvt.f32.f16 	%r685, %rs87;
	ld.shared.v4.b32 	{%r686, %r687, %r688, %r689}, [%r606+12288];
	mov.b32 	{%rs89, %rs90}, %r689;
	cvt.f32.f16 	%r690, %rs90;
	cvt.f32.f16 	%r691, %rs89;
	mov.b32 	{%rs91, %rs92}, %r688;
	cvt.f32.f16 	%r692, %rs92;
	cvt.f32.f16 	%r693, %rs91;
	mov.b32 	{%rs93, %rs94}, %r687;
	cvt.f32.f16 	%r694, %rs94;
	cvt.f32.f16 	%r695, %rs93;
	mov.b32 	{%rs95, %rs96}, %r686;
	cvt.f32.f16 	%r696, %rs96;
	cvt.f32.f16 	%r697, %rs95;
	bar.sync 	0;
	// begin inline asm
	cp.async.cg.shared.global [ %r606 + 0 ], [ %rd213 + 0 ], 0x10, %r607;
	// end inline asm
	// begin inline asm
	cp.async.cg.shared.global [ %r608 + 0 ], [ %rd214 + 0 ], 0x10, %r609;
	// end inline asm
	// begin inline asm
	cp.async.cg.shared.global [ %r610 + 0 ], [ %rd215 + 0 ], 0x10, %r611;
	// end inline asm
	// begin inline asm
	cp.async.cg.shared.global [ %r612 + 0 ], [ %rd216 + 0 ], 0x10, %r613;
	// end inline asm
	cp.async.commit_group;
	cp.async.wait_group 	0;
	bar.sync 	0;
	ld.shared.v4.b32 	{%r698, %r699, %r700, %r701}, [%r606];
	ld.shared.v4.b32 	{%r702, %r703, %r704, %r705}, [%r606+4096];
	ld.shared.v4.b32 	{%r706, %r707, %r708, %r709}, [%r606+8192];
	ld.shared.v4.b32 	{%r710, %r711, %r712, %r713}, [%r606+12288];
	bar.sync 	0;
	// begin inline asm
	cp.async.cg.shared.global [ %r606 + 0 ], [ %rd217 + 0 ], 0x10, %r607;
	// end inline asm
	// begin inline asm
	cp.async.cg.shared.global [ %r608 + 0 ], [ %rd218 + 0 ], 0x10, %r609;
	// end inline asm
	// begin inline asm
	cp.async.cg.shared.global [ %r610 + 0 ], [ %rd219 + 0 ], 0x10, %r611;
	// end inline asm
	// begin inline asm
	cp.async.cg.shared.global [ %r612 + 0 ], [ %rd220 + 0 ], 0x10, %r613;
	// end inline asm
	cp.async.commit_group;
	cp.async.wait_group 	0;
	bar.sync 	0;
	ld.shared.v4.b32 	{%r714, %r715, %r716, %r717}, [%r606];
	ld.shared.v4.b32 	{%r718, %r719, %r720, %r721}, [%r606+4096];
	ld.shared.v4.b32 	{%r722, %r723, %r724, %r725}, [%r606+8192];
	ld.shared.v4.b32 	{%r726, %r727, %r728, %r729}, [%r606+12288];
	mov.b32 	{%rs97, %rs98}, %r714;
	cvt.f32.f16 	%r730, %rs98;
	cvt.f32.f16 	%r731, %rs97;
	mov.b32 	{%rs99, %rs100}, %r715;
	cvt.f32.f16 	%r732, %rs100;
	cvt.f32.f16 	%r733, %rs99;
	mov.b32 	{%rs101, %rs102}, %r716;
	cvt.f32.f16 	%r734, %rs102;
	cvt.f32.f16 	%r735, %rs101;
	mov.b32 	{%rs103, %rs104}, %r717;
	cvt.f32.f16 	%r736, %rs104;
	cvt.f32.f16 	%r737, %rs103;
	mov.b32 	{%rs105, %rs106}, %r718;
	cvt.f32.f16 	%r738, %rs106;
	cvt.f32.f16 	%r739, %rs105;
	mov.b32 	{%rs107, %rs108}, %r719;
	cvt.f32.f16 	%r740, %rs108;
	cvt.f32.f16 	%r741, %rs107;
	mov.b32 	{%rs109, %rs110}, %r720;
	cvt.f32.f16 	%r742, %rs110;
	cvt.f32.f16 	%r743, %rs109;
	mov.b32 	{%rs111, %rs112}, %r721;
	cvt.f32.f16 	%r744, %rs112;
	cvt.f32.f16 	%r745, %rs111;
	mov.b32 	{%rs113, %rs114}, %r722;
	cvt.f32.f16 	%r746, %rs114;
	cvt.f32.f16 	%r747, %rs113;
	mov.b32 	{%rs115, %rs116}, %r723;
	cvt.f32.f16 	%r748, %rs116;
	cvt.f32.f16 	%r749, %rs115;
	mov.b32 	{%rs117, %rs118}, %r724;
	cvt.f32.f16 	%r750, %rs118;
	cvt.f32.f16 	%r751, %rs117;
	mov.b32 	{%rs119, %rs120}, %r725;
	cvt.f32.f16 	%r752, %rs120;
	cvt.f32.f16 	%r753, %rs119;
	mov.b32 	{%rs121, %rs122}, %r726;
	cvt.f32.f16 	%r754, %rs122;
	cvt.f32.f16 	%r755, %rs121;
	mov.b32 	{%rs123, %rs124}, %r727;
	cvt.f32.f16 	%r756, %rs124;
	cvt.f32.f16 	%r757, %rs123;
	mov.b32 	{%rs125, %rs126}, %r728;
	cvt.f32.f16 	%r758, %rs126;
	cvt.f32.f16 	%r759, %rs125;
	mov.b32 	{%rs127, %rs128}, %r729;
	cvt.f32.f16 	%r760, %rs128;
	cvt.f32.f16 	%r761, %rs127;
	mov.b64 	{%r762, %r763}, %rd234;
	sub.f32 	%r764, %r731, %r762;
	sub.f32 	%r765, %r730, %r763;
	sub.f32 	%r766, %r733, %r762;
	sub.f32 	%r767, %r732, %r763;
	sub.f32 	%r768, %r735, %r762;
	sub.f32 	%r769, %r734, %r763;
	sub.f32 	%r770, %r737, %r762;
	sub.f32 	%r771, %r736, %r763;
	sub.f32 	%r772, %r739, %r762;
	sub.f32 	%r773, %r738, %r763;
	sub.f32 	%r774, %r741, %r762;
	sub.f32 	%r775, %r740, %r763;
	sub.f32 	%r776, %r743, %r762;
	sub.f32 	%r777, %r742, %r763;
	sub.f32 	%r778, %r745, %r762;
	sub.f32 	%r779, %r744, %r763;
	sub.f32 	%r780, %r747, %r762;
	sub.f32 	%r781, %r746, %r763;
	sub.f32 	%r782, %r749, %r762;
	sub.f32 	%r783, %r748, %r763;
	sub.f32 	%r784, %r751, %r762;
	sub.f32 	%r785, %r750, %r763;
	sub.f32 	%r786, %r753, %r762;
	sub.f32 	%r787, %r752, %r763;
	sub.f32 	%r788, %r755, %r762;
	sub.f32 	%r789, %r754, %r763;
	sub.f32 	%r790, %r757, %r762;
	sub.f32 	%r791, %r756, %r763;
	sub.f32 	%r792, %r759, %r762;
	sub.f32 	%r793, %r758, %r763;
	sub.f32 	%r794, %r761, %r762;
	sub.f32 	%r795, %r760, %r763;
	mov.b64 	{%r796, %r797}, %rd115;
	mul.f32 	%r798, %r797, %r765;
	mul.f32 	%r799, %r796, %r764;
	mul.f32 	%r800, %r797, %r767;
	mul.f32 	%r801, %r796, %r766;
	mul.f32 	%r802, %r797, %r769;
	mul.f32 	%r803, %r796, %r768;
	mul.f32 	%r804, %r797, %r771;
	mul.f32 	%r805, %r796, %r770;
	mul.f32 	%r806, %r797, %r773;
	mul.f32 	%r807, %r796, %r772;
	mul.f32 	%r808, %r797, %r775;
	mul.f32 	%r809, %r796, %r774;
	mul.f32 	%r810, %r797, %r777;
	mul.f32 	%r811, %r796, %r776;
	mul.f32 	%r812, %r797, %r779;
	mul.f32 	%r813, %r796, %r778;
	mul.f32 	%r814, %r797, %r781;
	mul.f32 	%r815, %r796, %r780;
	mul.f32 	%r816, %r797, %r783;
	mul.f32 	%r817, %r796, %r782;
	mul.f32 	%r818, %r797, %r785;
	mul.f32 	%r819, %r796, %r784;
	mul.f32 	%r820, %r797, %r787;
	mul.f32 	%r821, %r796, %r786;
	mul.f32 	%r822, %r797, %r789;
	mul.f32 	%r823, %r796, %r788;
	mul.f32 	%r824, %r797, %r791;
	mul.f32 	%r825, %r796, %r790;
	mul.f32 	%r826, %r797, %r793;
	mul.f32 	%r827, %r796, %r792;
	mul.f32 	%r828, %r797, %r795;
	mul.f32 	%r829, %r796, %r794;
	mov.b32 	{%rs129, %rs130}, %r698;
	cvt.f32.f16 	%r830, %rs130;
	cvt.f32.f16 	%r831, %rs129;
	mov.b32 	{%rs131, %rs132}, %r699;
	cvt.f32.f16 	%r832, %rs132;
	cvt.f32.f16 	%r833, %rs131;
	mov.b32 	{%rs133, %rs134}, %r700;
	cvt.f32.f16 	%r834, %rs134;
	cvt.f32.f16 	%r835, %rs133;
	mov.b32 	{%rs135, %rs136}, %r701;
	cvt.f32.f16 	%r836, %rs136;
	cvt.f32.f16 	%r837, %rs135;
	mov.b32 	{%rs137, %rs138}, %r702;
	cvt.f32.f16 	%r838, %rs138;
	cvt.f32.f16 	%r839, %rs137;
	mov.b32 	{%rs139, %rs140}, %r703;
	cvt.f32.f16 	%r840, %rs140;
	cvt.f32.f16 	%r841, %rs139;
	mov.b32 	{%rs141, %rs142}, %r704;
	cvt.f32.f16 	%r842, %rs142;
	cvt.f32.f16 	%r843, %rs141;
	mov.b32 	{%rs143, %rs144}, %r705;
	cvt.f32.f16 	%r844, %rs144;
	cvt.f32.f16 	%r845, %rs143;
	mov.b32 	{%rs145, %rs146}, %r706;
	cvt.f32.f16 	%r846, %rs146;
	cvt.f32.f16 	%r847, %rs145;
	mov.b32 	{%rs147, %rs148}, %r707;
	cvt.f32.f16 	%r848, %rs148;
	cvt.f32.f16 	%r849, %rs147;
	mov.b32 	{%rs149, %rs150}, %r708;
	cvt.f32.f16 	%r850, %rs150;
	cvt.f32.f16 	%r851, %rs149;
	mov.b32 	{%rs151, %rs152}, %r709;
	cvt.f32.f16 	%r852, %rs152;
	cvt.f32.f16 	%r853, %rs151;
	mov.b32 	{%rs153, %rs154}, %r710;
	cvt.f32.f16 	%r854, %rs154;
	cvt.f32.f16 	%r855, %rs153;
	mov.b32 	{%rs155, %rs156}, %r711;
	cvt.f32.f16 	%r856, %rs156;
	cvt.f32.f16 	%r857, %rs155;
	mov.b32 	{%rs157, %rs158}, %r712;
	cvt.f32.f16 	%r858, %rs158;
	cvt.f32.f16 	%r859, %rs157;
	mov.b32 	{%rs159, %rs160}, %r713;
	cvt.f32.f16 	%r860, %rs160;
	cvt.f32.f16 	%r861, %rs159;
	fma.rn.f32 	%r862, %r799, %r661, %r831;
	fma.rn.f32 	%r863, %r798, %r660, %r830;
	fma.rn.f32 	%r864, %r801, %r659, %r833;
	fma.rn.f32 	%r865, %r800, %r658, %r832;
	fma.rn.f32 	%r866, %r803, %r657, %r835;
	fma.rn.f32 	%r867, %r802, %r656, %r834;
	fma.rn.f32 	%r868, %r805, %r655, %r837;
	fma.rn.f32 	%r869, %r804, %r654, %r836;
	fma.rn.f32 	%r870, %r807, %r673, %r839;
	fma.rn.f32 	%r871, %r806, %r672, %r838;
	fma.rn.f32 	%r872, %r809, %r671, %r841;
	fma.rn.f32 	%r873, %r808, %r670, %r840;
	fma.rn.f32 	%r874, %r811, %r669, %r843;
	fma.rn.f32 	%r875, %r810, %r668, %r842;
	fma.rn.f32 	%r876, %r813, %r667, %r845;
	fma.rn.f32 	%r877, %r812, %r666, %r844;
	fma.rn.f32 	%r878, %r815, %r685, %r847;
	fma.rn.f32 	%r879, %r814, %r684, %r846;
	fma.rn.f32 	%r880, %r817, %r683, %r849;
	fma.rn.f32 	%r881, %r816, %r682, %r848;
	fma.rn.f32 	%r882, %r819, %r681, %r851;
	fma.rn.f32 	%r883, %r818, %r680, %r850;
	fma.rn.f32 	%r884, %r821, %r679, %r853;
	fma.rn.f32 	%r885, %r820, %r678, %r852;
	fma.rn.f32 	%r886, %r823, %r697, %r855;
	fma.rn.f32 	%r887, %r822, %r696, %r854;
	fma.rn.f32 	%r888, %r825, %r695, %r857;
	fma.rn.f32 	%r889, %r824, %r694, %r856;
	fma.rn.f32 	%r890, %r827, %r693, %r859;
	fma.rn.f32 	%r891, %r826, %r692, %r858;
	fma.rn.f32 	%r892, %r829, %r691, %r861;
	fma.rn.f32 	%r893, %r828, %r690, %r860;
	cvt.rn.f16x2.f32 	%r630, %r863, %r862;
	cvt.rn.f16x2.f32 	%r631, %r865, %r864;
	cvt.rn.f16x2.f32 	%r632, %r867, %r866;
	cvt.rn.f16x2.f32 	%r633, %r869, %r868;
	cvt.rn.f16x2.f32 	%r634, %r871, %r870;
	cvt.rn.f16x2.f32 	%r635, %r873, %r872;
	cvt.rn.f16x2.f32 	%r636, %r875, %r874;
	cvt.rn.f16x2.f32 	%r637, %r877, %r876;
	cvt.rn.f16x2.f32 	%r638, %r879, %r878;
	cvt.rn.f16x2.f32 	%r639, %r881, %r880;
	cvt.rn.f16x2.f32 	%r640, %r883, %r882;
	cvt.rn.f16x2.f32 	%r641, %r885, %r884;
	cvt.rn.f16x2.f32 	%r642, %r887, %r886;
	cvt.rn.f16x2.f32 	%r643, %r889, %r888;
	cvt.rn.f16x2.f32 	%r644, %r891, %r890;
	cvt.rn.f16x2.f32 	%r645, %r893, %r892;
	// begin inline asm
	@%p54 st.global.v4.b32 [ %rd221 + 0 ], { %r630, %r631, %r632, %r633 };
	// end inline asm
	// begin inline asm
	@%p55 st.global.v4.b32 [ %rd222 + 0 ], { %r634, %r635, %r636, %r637 };
	// end inline asm
	// begin inline asm
	@%p56 st.global.v4.b32 [ %rd223 + 0 ], { %r638, %r639, %r640, %r641 };
	// end inline asm
	// begin inline asm
	@%p57 st.global.v4.b32 [ %rd224 + 0 ], { %r642, %r643, %r644, %r645 };
	// end inline asm
	add.s32 	%r899, %r899, 8192;
	setp.lt.s32 	%p58, %r899, %r66;
	@%p58 bra 	$L__BB0_8;
$L__BB0_9:                              // %._crit_edge152
	ret;
                                        // -- End function
}
