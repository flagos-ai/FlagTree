//
// Generated by LLVM NVPTX Back-End
//

.version 8.7
.target sm_90a
.address_size 64

	// .globl	_layer_norm_fwd_fused   // -- Begin function _layer_norm_fwd_fused
.extern .shared .align 16 .b8 global_smem[];
.global .align 1 .b8 _$_str[11] = {95, 95, 67, 85, 68, 65, 95, 70, 84, 90};
.global .align 1 .b8 _$_str_$_2[17] = {95, 95, 67, 85, 68, 65, 95, 80, 82, 69, 67, 95, 83, 81, 82, 84};
                                        // @_layer_norm_fwd_fused
.visible .entry _layer_norm_fwd_fused(
	.param .u64 .ptr .global .align 1 _layer_norm_fwd_fused_param_0,
	.param .u64 .ptr .global .align 1 _layer_norm_fwd_fused_param_1,
	.param .u64 .ptr .global .align 1 _layer_norm_fwd_fused_param_2,
	.param .u64 .ptr .global .align 1 _layer_norm_fwd_fused_param_3,
	.param .u64 .ptr .global .align 1 _layer_norm_fwd_fused_param_4,
	.param .u64 .ptr .global .align 1 _layer_norm_fwd_fused_param_5,
	.param .u32 _layer_norm_fwd_fused_param_6,
	.param .u32 _layer_norm_fwd_fused_param_7,
	.param .f32 _layer_norm_fwd_fused_param_8,
	.param .u64 .ptr .global .align 1 _layer_norm_fwd_fused_param_9,
	.param .u64 .ptr .global .align 1 _layer_norm_fwd_fused_param_10
)
.reqntid 256
{
	.reg .pred 	%p<39>;
	.reg .b16 	%rs<161>;
	.reg .b32 	%r<863>;
	.reg .b64 	%rd<248>;

// %bb.0:
	ld.param.b32 	%r30, [_layer_norm_fwd_fused_param_7];
	ld.param.b64 	%rd105, [_layer_norm_fwd_fused_param_0];
	mov.u32 	%r1, %ctaid.x;
	ld.param.b32 	%r33, [_layer_norm_fwd_fused_param_6];
	mul.lo.s32 	%r34, %r33, %r1;
	mul.wide.s32 	%rd107, %r34, 2;
	add.s64 	%rd2, %rd105, %rd107;
	mov.u32 	%r2, %tid.x;
	and.b32 	%r3, %r2, 31;
	shr.u32 	%r4, %r2, 5;
	shl.b32 	%r35, %r2, 3;
	and.b32 	%r5, %r35, 2040;
	or.b32 	%r8, %r35, 6144;
	setp.lt.s32 	%p1, %r30, 1;
	mov.b32 	%r32, 0f00000000;
	cvt.u64.u32 	%rd199, %r5;
	mov.b32 	%r860, %r32;
	@%p1 bra 	$L__BB0_4;
// %bb.1:                               // %.lr.ph.preheader
	mov.b32 	%r37, 0f00000000;
	mov.b64 	%rd200, {%r37, %r37};
	mov.b32 	%r42, 0;
	mov.b32 	%r859, %r42;
	mov.b64 	%rd201, %rd200;
	mov.b64 	%rd202, %rd200;
	mov.b64 	%rd203, %rd200;
	mov.b64 	%rd204, %rd200;
	mov.b64 	%rd205, %rd200;
	mov.b64 	%rd206, %rd200;
	mov.b64 	%rd207, %rd200;
	mov.b64 	%rd208, %rd200;
	mov.b64 	%rd209, %rd200;
	mov.b64 	%rd210, %rd200;
	mov.b64 	%rd211, %rd200;
	mov.b64 	%rd212, %rd200;
	mov.b64 	%rd213, %rd200;
	mov.b64 	%rd214, %rd200;
	mov.b64 	%rd215, %rd200;
$L__BB0_2:                              // %.lr.ph
                                        // =>This Inner Loop Header: Depth=1
	add.s32 	%r70, %r5, %r859;
	add.s32 	%r71, %r70, 2048;
	add.s32 	%r72, %r70, 4096;
	add.s32 	%r73, %r8, %r859;
	setp.lt.s32 	%p2, %r70, %r30;
	setp.lt.s32 	%p3, %r71, %r30;
	setp.lt.s32 	%p4, %r72, %r30;
	setp.lt.s32 	%p5, %r73, %r30;
	mad.wide.s32 	%rd124, %r70, 2, %rd2;
	cvt.s64.s32 	%rd128, %r859;
	add.s64 	%rd130, %rd128, %rd199;
	shl.b64 	%rd131, %rd130, 1;
	add.s64 	%rd132, %rd2, %rd131;
	add.s64 	%rd125, %rd132, 4096;
	add.s64 	%rd126, %rd132, 8192;
	mad.wide.s32 	%rd127, %r73, 2, %rd2;
	// begin inline asm
	mov.u32 %r38, %r42;
	mov.u32 %r39, %r42;
	mov.u32 %r40, %r42;
	mov.u32 %r41, %r42;
	@%p2 ld.global.v4.b32 { %r38, %r39, %r40, %r41 }, [ %rd124 + 0 ];
	// end inline asm
	// begin inline asm
	mov.u32 %r46, %r42;
	mov.u32 %r47, %r42;
	mov.u32 %r48, %r42;
	mov.u32 %r49, %r42;
	@%p3 ld.global.v4.b32 { %r46, %r47, %r48, %r49 }, [ %rd125 + 0 ];
	// end inline asm
	// begin inline asm
	mov.u32 %r54, %r42;
	mov.u32 %r55, %r42;
	mov.u32 %r56, %r42;
	mov.u32 %r57, %r42;
	@%p4 ld.global.v4.b32 { %r54, %r55, %r56, %r57 }, [ %rd126 + 0 ];
	// end inline asm
	// begin inline asm
	mov.u32 %r62, %r42;
	mov.u32 %r63, %r42;
	mov.u32 %r64, %r42;
	mov.u32 %r65, %r42;
	@%p5 ld.global.v4.b32 { %r62, %r63, %r64, %r65 }, [ %rd127 + 0 ];
	// end inline asm
	mov.b32 	{%rs1, %rs2}, %r65;
	cvt.f32.f16 	%r74, %rs1;
	cvt.f32.f16 	%r75, %rs2;
	mov.b32 	{%rs3, %rs4}, %r64;
	cvt.f32.f16 	%r76, %rs3;
	cvt.f32.f16 	%r77, %rs4;
	mov.b32 	{%rs5, %rs6}, %r63;
	cvt.f32.f16 	%r78, %rs5;
	cvt.f32.f16 	%r79, %rs6;
	mov.b32 	{%rs7, %rs8}, %r62;
	cvt.f32.f16 	%r80, %rs7;
	cvt.f32.f16 	%r81, %rs8;
	mov.b32 	{%rs9, %rs10}, %r57;
	cvt.f32.f16 	%r82, %rs9;
	cvt.f32.f16 	%r83, %rs10;
	mov.b32 	{%rs11, %rs12}, %r56;
	cvt.f32.f16 	%r84, %rs11;
	cvt.f32.f16 	%r85, %rs12;
	mov.b32 	{%rs13, %rs14}, %r55;
	cvt.f32.f16 	%r86, %rs13;
	cvt.f32.f16 	%r87, %rs14;
	mov.b32 	{%rs15, %rs16}, %r54;
	cvt.f32.f16 	%r88, %rs15;
	cvt.f32.f16 	%r89, %rs16;
	mov.b32 	{%rs17, %rs18}, %r46;
	cvt.f32.f16 	%r90, %rs17;
	cvt.f32.f16 	%r91, %rs18;
	mov.b32 	{%rs19, %rs20}, %r47;
	cvt.f32.f16 	%r92, %rs19;
	cvt.f32.f16 	%r93, %rs20;
	mov.b32 	{%rs21, %rs22}, %r48;
	cvt.f32.f16 	%r94, %rs21;
	cvt.f32.f16 	%r95, %rs22;
	mov.b32 	{%rs23, %rs24}, %r49;
	cvt.f32.f16 	%r96, %rs23;
	cvt.f32.f16 	%r97, %rs24;
	mov.b32 	{%rs25, %rs26}, %r38;
	cvt.f32.f16 	%r98, %rs25;
	cvt.f32.f16 	%r99, %rs26;
	mov.b32 	{%rs27, %rs28}, %r39;
	cvt.f32.f16 	%r100, %rs27;
	cvt.f32.f16 	%r101, %rs28;
	mov.b32 	{%rs29, %rs30}, %r40;
	cvt.f32.f16 	%r102, %rs29;
	cvt.f32.f16 	%r103, %rs30;
	mov.b32 	{%rs31, %rs32}, %r41;
	cvt.f32.f16 	%r104, %rs31;
	cvt.f32.f16 	%r105, %rs32;
	mov.b64 	{%r106, %r107}, %rd203;
	add.f32 	%r108, %r107, %r105;
	add.f32 	%r109, %r106, %r104;
	mov.b64 	%rd203, {%r109, %r108};
	mov.b64 	{%r110, %r111}, %rd202;
	add.f32 	%r112, %r111, %r103;
	add.f32 	%r113, %r110, %r102;
	mov.b64 	%rd202, {%r113, %r112};
	mov.b64 	{%r114, %r115}, %rd201;
	add.f32 	%r116, %r115, %r101;
	add.f32 	%r117, %r114, %r100;
	mov.b64 	%rd201, {%r117, %r116};
	mov.b64 	{%r118, %r119}, %rd200;
	add.f32 	%r120, %r119, %r99;
	add.f32 	%r121, %r118, %r98;
	mov.b64 	%rd200, {%r121, %r120};
	mov.b64 	{%r122, %r123}, %rd207;
	add.f32 	%r124, %r123, %r97;
	add.f32 	%r125, %r122, %r96;
	mov.b64 	%rd207, {%r125, %r124};
	mov.b64 	{%r126, %r127}, %rd206;
	add.f32 	%r128, %r127, %r95;
	add.f32 	%r129, %r126, %r94;
	mov.b64 	%rd206, {%r129, %r128};
	mov.b64 	{%r130, %r131}, %rd205;
	add.f32 	%r132, %r131, %r93;
	add.f32 	%r133, %r130, %r92;
	mov.b64 	%rd205, {%r133, %r132};
	mov.b64 	{%r134, %r135}, %rd204;
	add.f32 	%r136, %r135, %r91;
	add.f32 	%r137, %r134, %r90;
	mov.b64 	%rd204, {%r137, %r136};
	mov.b64 	{%r138, %r139}, %rd208;
	add.f32 	%r140, %r139, %r89;
	add.f32 	%r141, %r138, %r88;
	mov.b64 	%rd208, {%r141, %r140};
	mov.b64 	{%r142, %r143}, %rd209;
	add.f32 	%r144, %r143, %r87;
	add.f32 	%r145, %r142, %r86;
	mov.b64 	%rd209, {%r145, %r144};
	mov.b64 	{%r146, %r147}, %rd210;
	add.f32 	%r148, %r147, %r85;
	add.f32 	%r149, %r146, %r84;
	mov.b64 	%rd210, {%r149, %r148};
	mov.b64 	{%r150, %r151}, %rd211;
	add.f32 	%r152, %r151, %r83;
	add.f32 	%r153, %r150, %r82;
	mov.b64 	%rd211, {%r153, %r152};
	mov.b64 	{%r154, %r155}, %rd212;
	add.f32 	%r156, %r155, %r81;
	add.f32 	%r157, %r154, %r80;
	mov.b64 	%rd212, {%r157, %r156};
	mov.b64 	{%r158, %r159}, %rd213;
	add.f32 	%r160, %r159, %r79;
	add.f32 	%r161, %r158, %r78;
	mov.b64 	%rd213, {%r161, %r160};
	mov.b64 	{%r162, %r163}, %rd214;
	add.f32 	%r164, %r163, %r77;
	add.f32 	%r165, %r162, %r76;
	mov.b64 	%rd214, {%r165, %r164};
	mov.b64 	{%r166, %r167}, %rd215;
	add.f32 	%r168, %r167, %r75;
	add.f32 	%r169, %r166, %r74;
	mov.b64 	%rd215, {%r169, %r168};
	add.s32 	%r859, %r859, 8192;
	setp.lt.s32 	%p6, %r859, %r30;
	@%p6 bra 	$L__BB0_2;
// %bb.3:                               // %._crit_edge.loopexit
	mov.b64 	{%r170, %r171}, %rd200;
	add.f32 	%r172, %r170, %r171;
	mov.b64 	{%r173, %r174}, %rd201;
	add.f32 	%r175, %r173, %r172;
	add.f32 	%r176, %r174, %r175;
	mov.b64 	{%r177, %r178}, %rd202;
	add.f32 	%r179, %r177, %r176;
	add.f32 	%r180, %r178, %r179;
	mov.b64 	{%r181, %r182}, %rd203;
	add.f32 	%r183, %r181, %r180;
	add.f32 	%r184, %r182, %r183;
	mov.b64 	{%r185, %r186}, %rd204;
	add.f32 	%r187, %r185, %r184;
	add.f32 	%r188, %r186, %r187;
	mov.b64 	{%r189, %r190}, %rd205;
	add.f32 	%r191, %r189, %r188;
	add.f32 	%r192, %r190, %r191;
	mov.b64 	{%r193, %r194}, %rd206;
	add.f32 	%r195, %r193, %r192;
	add.f32 	%r196, %r194, %r195;
	mov.b64 	{%r197, %r198}, %rd207;
	add.f32 	%r199, %r197, %r196;
	add.f32 	%r200, %r198, %r199;
	mov.b64 	{%r201, %r202}, %rd208;
	add.f32 	%r203, %r201, %r200;
	add.f32 	%r204, %r202, %r203;
	mov.b64 	{%r205, %r206}, %rd209;
	add.f32 	%r207, %r205, %r204;
	add.f32 	%r208, %r206, %r207;
	mov.b64 	{%r209, %r210}, %rd210;
	add.f32 	%r211, %r209, %r208;
	add.f32 	%r212, %r210, %r211;
	mov.b64 	{%r213, %r214}, %rd211;
	add.f32 	%r215, %r213, %r212;
	add.f32 	%r216, %r214, %r215;
	mov.b64 	{%r217, %r218}, %rd212;
	add.f32 	%r219, %r217, %r216;
	add.f32 	%r220, %r218, %r219;
	mov.b64 	{%r221, %r222}, %rd213;
	add.f32 	%r223, %r221, %r220;
	add.f32 	%r224, %r222, %r223;
	mov.b64 	{%r225, %r226}, %rd214;
	add.f32 	%r227, %r225, %r224;
	add.f32 	%r228, %r226, %r227;
	mov.b64 	{%r229, %r230}, %rd215;
	add.f32 	%r231, %r229, %r228;
	add.f32 	%r860, %r230, %r231;
$L__BB0_4:                              // %._crit_edge
	ld.param.b32 	%r31, [_layer_norm_fwd_fused_param_8];
	ld.param.b64 	%rd104, [_layer_norm_fwd_fused_param_5];
	ld.param.b64 	%rd103, [_layer_norm_fwd_fused_param_4];
	shfl.sync.bfly.b32 	%r238, %r860, 16, 31, -1;
	add.f32 	%r239, %r860, %r238;
	shfl.sync.bfly.b32 	%r240, %r239, 8, 31, -1;
	add.f32 	%r241, %r239, %r240;
	shfl.sync.bfly.b32 	%r242, %r241, 4, 31, -1;
	add.f32 	%r243, %r241, %r242;
	shfl.sync.bfly.b32 	%r244, %r243, 2, 31, -1;
	add.f32 	%r245, %r243, %r244;
	shfl.sync.bfly.b32 	%r246, %r245, 1, 31, -1;
	add.f32 	%r233, %r245, %r246;
	and.b32 	%r247, %r4, 7;
	setp.eq.b32 	%p7, %r3, 0;
	shl.b32 	%r248, %r247, 2;
	mov.b32 	%r249, global_smem;
	add.s32 	%r488, %r249, %r248;
	// begin inline asm
	@%p7 st.shared.b32 [ %r488 + 0 ], %r233;
	// end inline asm
	bar.sync 	0;
	setp.lt.u32 	%p8, %r2, 8;
	shl.b32 	%r250, %r2, 2;
	add.s32 	%r492, %r249, %r250;
	// begin inline asm
	@%p8 ld.shared.b32 %r234, [ %r492 + 0 ];
	// end inline asm
	shfl.sync.bfly.b32 	%r251, %r234, 4, 31, -1;
	add.f32 	%r252, %r234, %r251;
	shfl.sync.bfly.b32 	%r253, %r252, 2, 31, -1;
	add.f32 	%r254, %r252, %r253;
	shfl.sync.bfly.b32 	%r255, %r254, 1, 31, -1;
	add.f32 	%r237, %r254, %r255;
	setp.eq.b32 	%p18, %r2, 0;
	// begin inline asm
	@%p18 st.shared.b32 [ %r492 + 0 ], %r237;
	// end inline asm
	bar.sync 	0;
	ld.shared.b32 	%r256, [global_smem];
	cvt.rn.f32.s32 	%r15, %r30;
	div.full.f32 	%r494, %r256, %r15;
	mov.b64 	%rd232, {%r32, %r32};
	mov.b64 	%rd198, {%r494, %r494};
	mov.b64 	%rd233, %rd232;
	mov.b64 	%rd234, %rd232;
	mov.b64 	%rd235, %rd232;
	mov.b64 	%rd236, %rd232;
	mov.b64 	%rd237, %rd232;
	mov.b64 	%rd238, %rd232;
	mov.b64 	%rd239, %rd232;
	mov.b64 	%rd240, %rd232;
	mov.b64 	%rd241, %rd232;
	mov.b64 	%rd242, %rd232;
	mov.b64 	%rd243, %rd232;
	mov.b64 	%rd244, %rd232;
	mov.b64 	%rd245, %rd232;
	mov.b64 	%rd246, %rd232;
	mov.b64 	%rd247, %rd232;
	@%p1 bra 	$L__BB0_7;
// %bb.5:                               // %.lr.ph6.preheader
	or.b32 	%r6, %r5, 2048;
	or.b32 	%r7, %r5, 4096;
	mov.b32 	%r259, 0f00000000;
	mov.b64 	%rd232, {%r259, %r259};
	mov.b32 	%r264, 0;
	mov.b32 	%r861, %r264;
	mov.b64 	%rd233, %rd232;
	mov.b64 	%rd234, %rd232;
	mov.b64 	%rd235, %rd232;
	mov.b64 	%rd236, %rd232;
	mov.b64 	%rd237, %rd232;
	mov.b64 	%rd238, %rd232;
	mov.b64 	%rd239, %rd232;
	mov.b64 	%rd240, %rd232;
	mov.b64 	%rd241, %rd232;
	mov.b64 	%rd242, %rd232;
	mov.b64 	%rd243, %rd232;
	mov.b64 	%rd244, %rd232;
	mov.b64 	%rd245, %rd232;
	mov.b64 	%rd246, %rd232;
	mov.b64 	%rd247, %rd232;
$L__BB0_6:                              // %.lr.ph6
                                        // =>This Inner Loop Header: Depth=1
	or.b32 	%r292, %r861, %r8;
	or.b32 	%r293, %r861, %r7;
	or.b32 	%r294, %r861, %r6;
	or.b32 	%r295, %r861, %r5;
	mad.wide.s32 	%rd165, %r295, 2, %rd2;
	mad.wide.s32 	%rd166, %r294, 2, %rd2;
	mad.wide.s32 	%rd167, %r293, 2, %rd2;
	mad.wide.s32 	%rd168, %r292, 2, %rd2;
	setp.lt.s32 	%p14, %r292, %r30;
	setp.lt.s32 	%p13, %r293, %r30;
	setp.lt.s32 	%p12, %r294, %r30;
	setp.lt.s32 	%p11, %r295, %r30;
	// begin inline asm
	mov.u32 %r260, %r264;
	mov.u32 %r261, %r264;
	mov.u32 %r262, %r264;
	mov.u32 %r263, %r264;
	@%p11 ld.global.v4.b32 { %r260, %r261, %r262, %r263 }, [ %rd165 + 0 ];
	// end inline asm
	// begin inline asm
	mov.u32 %r268, %r264;
	mov.u32 %r269, %r264;
	mov.u32 %r270, %r264;
	mov.u32 %r271, %r264;
	@%p12 ld.global.v4.b32 { %r268, %r269, %r270, %r271 }, [ %rd166 + 0 ];
	// end inline asm
	// begin inline asm
	mov.u32 %r276, %r264;
	mov.u32 %r277, %r264;
	mov.u32 %r278, %r264;
	mov.u32 %r279, %r264;
	@%p13 ld.global.v4.b32 { %r276, %r277, %r278, %r279 }, [ %rd167 + 0 ];
	// end inline asm
	// begin inline asm
	mov.u32 %r284, %r264;
	mov.u32 %r285, %r264;
	mov.u32 %r286, %r264;
	mov.u32 %r287, %r264;
	@%p14 ld.global.v4.b32 { %r284, %r285, %r286, %r287 }, [ %rd168 + 0 ];
	// end inline asm
	mov.b32 	{%rs33, %rs34}, %r287;
	cvt.f32.f16 	%r296, %rs33;
	cvt.f32.f16 	%r297, %rs34;
	mov.b32 	{%rs35, %rs36}, %r286;
	cvt.f32.f16 	%r298, %rs35;
	cvt.f32.f16 	%r299, %rs36;
	mov.b32 	{%rs37, %rs38}, %r285;
	cvt.f32.f16 	%r300, %rs37;
	cvt.f32.f16 	%r301, %rs38;
	mov.b32 	{%rs39, %rs40}, %r284;
	cvt.f32.f16 	%r302, %rs39;
	cvt.f32.f16 	%r303, %rs40;
	mov.b32 	{%rs41, %rs42}, %r279;
	cvt.f32.f16 	%r304, %rs41;
	cvt.f32.f16 	%r305, %rs42;
	mov.b32 	{%rs43, %rs44}, %r278;
	cvt.f32.f16 	%r306, %rs43;
	cvt.f32.f16 	%r307, %rs44;
	mov.b32 	{%rs45, %rs46}, %r277;
	cvt.f32.f16 	%r308, %rs45;
	cvt.f32.f16 	%r309, %rs46;
	mov.b32 	{%rs47, %rs48}, %r276;
	cvt.f32.f16 	%r310, %rs47;
	cvt.f32.f16 	%r311, %rs48;
	mov.b32 	{%rs49, %rs50}, %r271;
	cvt.f32.f16 	%r312, %rs49;
	cvt.f32.f16 	%r313, %rs50;
	mov.b32 	{%rs51, %rs52}, %r270;
	cvt.f32.f16 	%r314, %rs51;
	cvt.f32.f16 	%r315, %rs52;
	mov.b32 	{%rs53, %rs54}, %r268;
	cvt.f32.f16 	%r316, %rs53;
	cvt.f32.f16 	%r317, %rs54;
	mov.b32 	{%rs55, %rs56}, %r269;
	cvt.f32.f16 	%r318, %rs55;
	cvt.f32.f16 	%r319, %rs56;
	mov.b32 	{%rs57, %rs58}, %r260;
	cvt.f32.f16 	%r320, %rs57;
	cvt.f32.f16 	%r321, %rs58;
	mov.b32 	{%rs59, %rs60}, %r261;
	cvt.f32.f16 	%r322, %rs59;
	cvt.f32.f16 	%r323, %rs60;
	mov.b32 	{%rs61, %rs62}, %r262;
	cvt.f32.f16 	%r324, %rs61;
	cvt.f32.f16 	%r325, %rs62;
	mov.b32 	{%rs63, %rs64}, %r263;
	cvt.f32.f16 	%r326, %rs63;
	cvt.f32.f16 	%r327, %rs64;
	mov.b64 	{%r328, %r329}, %rd198;
	sub.f32 	%r330, %r327, %r329;
	sub.f32 	%r331, %r326, %r328;
	sub.f32 	%r334, %r325, %r329;
	sub.f32 	%r335, %r324, %r328;
	sub.f32 	%r338, %r323, %r329;
	sub.f32 	%r339, %r322, %r328;
	sub.f32 	%r342, %r321, %r329;
	sub.f32 	%r343, %r320, %r328;
	sub.f32 	%r346, %r319, %r329;
	sub.f32 	%r347, %r318, %r328;
	sub.f32 	%r350, %r317, %r329;
	sub.f32 	%r351, %r316, %r328;
	sub.f32 	%r354, %r315, %r329;
	sub.f32 	%r355, %r314, %r328;
	sub.f32 	%r358, %r313, %r329;
	sub.f32 	%r359, %r312, %r328;
	sub.f32 	%r362, %r311, %r329;
	sub.f32 	%r363, %r310, %r328;
	sub.f32 	%r366, %r309, %r329;
	sub.f32 	%r367, %r308, %r328;
	sub.f32 	%r370, %r307, %r329;
	sub.f32 	%r371, %r306, %r328;
	sub.f32 	%r374, %r305, %r329;
	sub.f32 	%r375, %r304, %r328;
	sub.f32 	%r378, %r303, %r329;
	sub.f32 	%r379, %r302, %r328;
	sub.f32 	%r382, %r301, %r329;
	sub.f32 	%r383, %r300, %r328;
	sub.f32 	%r386, %r299, %r329;
	sub.f32 	%r387, %r298, %r328;
	sub.f32 	%r390, %r297, %r329;
	sub.f32 	%r391, %r296, %r328;
	selp.f32 	%r392, %r391, 0f00000000, %p14;
	selp.f32 	%r393, %r390, 0f00000000, %p14;
	selp.f32 	%r394, %r387, 0f00000000, %p14;
	selp.f32 	%r395, %r386, 0f00000000, %p14;
	selp.f32 	%r396, %r383, 0f00000000, %p14;
	selp.f32 	%r397, %r382, 0f00000000, %p14;
	selp.f32 	%r398, %r379, 0f00000000, %p14;
	selp.f32 	%r399, %r378, 0f00000000, %p14;
	selp.f32 	%r400, %r375, 0f00000000, %p13;
	selp.f32 	%r401, %r374, 0f00000000, %p13;
	selp.f32 	%r402, %r371, 0f00000000, %p13;
	selp.f32 	%r403, %r370, 0f00000000, %p13;
	selp.f32 	%r404, %r367, 0f00000000, %p13;
	selp.f32 	%r405, %r366, 0f00000000, %p13;
	selp.f32 	%r406, %r363, 0f00000000, %p13;
	selp.f32 	%r407, %r362, 0f00000000, %p13;
	selp.f32 	%r408, %r359, 0f00000000, %p12;
	selp.f32 	%r409, %r358, 0f00000000, %p12;
	selp.f32 	%r410, %r355, 0f00000000, %p12;
	selp.f32 	%r411, %r354, 0f00000000, %p12;
	selp.f32 	%r412, %r351, 0f00000000, %p12;
	selp.f32 	%r413, %r350, 0f00000000, %p12;
	selp.f32 	%r414, %r347, 0f00000000, %p12;
	selp.f32 	%r415, %r346, 0f00000000, %p12;
	selp.f32 	%r416, %r343, 0f00000000, %p11;
	selp.f32 	%r417, %r342, 0f00000000, %p11;
	selp.f32 	%r418, %r339, 0f00000000, %p11;
	selp.f32 	%r419, %r338, 0f00000000, %p11;
	selp.f32 	%r420, %r335, 0f00000000, %p11;
	selp.f32 	%r421, %r334, 0f00000000, %p11;
	selp.f32 	%r422, %r331, 0f00000000, %p11;
	selp.f32 	%r423, %r330, 0f00000000, %p11;
	mov.b64 	{%r424, %r425}, %rd235;
	fma.rn.f32 	%r426, %r423, %r423, %r425;
	fma.rn.f32 	%r427, %r422, %r422, %r424;
	mov.b64 	%rd235, {%r427, %r426};
	mov.b64 	{%r428, %r429}, %rd234;
	fma.rn.f32 	%r430, %r421, %r421, %r429;
	fma.rn.f32 	%r431, %r420, %r420, %r428;
	mov.b64 	%rd234, {%r431, %r430};
	mov.b64 	{%r432, %r433}, %rd233;
	fma.rn.f32 	%r434, %r419, %r419, %r433;
	fma.rn.f32 	%r435, %r418, %r418, %r432;
	mov.b64 	%rd233, {%r435, %r434};
	mov.b64 	{%r436, %r437}, %rd232;
	fma.rn.f32 	%r438, %r417, %r417, %r437;
	fma.rn.f32 	%r439, %r416, %r416, %r436;
	mov.b64 	%rd232, {%r439, %r438};
	mov.b64 	{%r440, %r441}, %rd237;
	fma.rn.f32 	%r442, %r415, %r415, %r441;
	fma.rn.f32 	%r443, %r414, %r414, %r440;
	mov.b64 	%rd237, {%r443, %r442};
	mov.b64 	{%r444, %r445}, %rd236;
	fma.rn.f32 	%r446, %r413, %r413, %r445;
	fma.rn.f32 	%r447, %r412, %r412, %r444;
	mov.b64 	%rd236, {%r447, %r446};
	mov.b64 	{%r448, %r449}, %rd238;
	fma.rn.f32 	%r450, %r411, %r411, %r449;
	fma.rn.f32 	%r451, %r410, %r410, %r448;
	mov.b64 	%rd238, {%r451, %r450};
	mov.b64 	{%r452, %r453}, %rd239;
	fma.rn.f32 	%r454, %r409, %r409, %r453;
	fma.rn.f32 	%r455, %r408, %r408, %r452;
	mov.b64 	%rd239, {%r455, %r454};
	mov.b64 	{%r456, %r457}, %rd240;
	fma.rn.f32 	%r458, %r407, %r407, %r457;
	fma.rn.f32 	%r459, %r406, %r406, %r456;
	mov.b64 	%rd240, {%r459, %r458};
	mov.b64 	{%r460, %r461}, %rd241;
	fma.rn.f32 	%r462, %r405, %r405, %r461;
	fma.rn.f32 	%r463, %r404, %r404, %r460;
	mov.b64 	%rd241, {%r463, %r462};
	mov.b64 	{%r464, %r465}, %rd242;
	fma.rn.f32 	%r466, %r403, %r403, %r465;
	fma.rn.f32 	%r467, %r402, %r402, %r464;
	mov.b64 	%rd242, {%r467, %r466};
	mov.b64 	{%r468, %r469}, %rd243;
	fma.rn.f32 	%r470, %r401, %r401, %r469;
	fma.rn.f32 	%r471, %r400, %r400, %r468;
	mov.b64 	%rd243, {%r471, %r470};
	mov.b64 	{%r472, %r473}, %rd244;
	fma.rn.f32 	%r474, %r399, %r399, %r473;
	fma.rn.f32 	%r475, %r398, %r398, %r472;
	mov.b64 	%rd244, {%r475, %r474};
	mov.b64 	{%r476, %r477}, %rd245;
	fma.rn.f32 	%r478, %r397, %r397, %r477;
	fma.rn.f32 	%r479, %r396, %r396, %r476;
	mov.b64 	%rd245, {%r479, %r478};
	mov.b64 	{%r480, %r481}, %rd246;
	fma.rn.f32 	%r482, %r395, %r395, %r481;
	fma.rn.f32 	%r483, %r394, %r394, %r480;
	mov.b64 	%rd246, {%r483, %r482};
	mov.b64 	{%r484, %r485}, %rd247;
	fma.rn.f32 	%r486, %r393, %r393, %r485;
	fma.rn.f32 	%r487, %r392, %r392, %r484;
	mov.b64 	%rd247, {%r487, %r486};
	add.s32 	%r861, %r861, 8192;
	setp.lt.s32 	%p15, %r861, %r30;
	@%p15 bra 	$L__BB0_6;
$L__BB0_7:                              // %._crit_edge7
	bar.sync 	0;
	mov.b64 	{%r496, %r497}, %rd232;
	add.f32 	%r498, %r496, %r497;
	mov.b64 	{%r499, %r500}, %rd233;
	add.f32 	%r501, %r499, %r498;
	add.f32 	%r502, %r500, %r501;
	mov.b64 	{%r503, %r504}, %rd234;
	add.f32 	%r505, %r503, %r502;
	add.f32 	%r506, %r504, %r505;
	mov.b64 	{%r507, %r508}, %rd235;
	add.f32 	%r509, %r507, %r506;
	add.f32 	%r510, %r508, %r509;
	mov.b64 	{%r511, %r512}, %rd236;
	add.f32 	%r513, %r511, %r510;
	add.f32 	%r514, %r512, %r513;
	mov.b64 	{%r515, %r516}, %rd237;
	add.f32 	%r517, %r515, %r514;
	add.f32 	%r518, %r516, %r517;
	mov.b64 	{%r519, %r520}, %rd238;
	add.f32 	%r521, %r519, %r518;
	add.f32 	%r522, %r520, %r521;
	mov.b64 	{%r523, %r524}, %rd239;
	add.f32 	%r525, %r523, %r522;
	add.f32 	%r526, %r524, %r525;
	mov.b64 	{%r527, %r528}, %rd240;
	add.f32 	%r529, %r527, %r526;
	add.f32 	%r530, %r528, %r529;
	mov.b64 	{%r531, %r532}, %rd241;
	add.f32 	%r533, %r531, %r530;
	add.f32 	%r534, %r532, %r533;
	mov.b64 	{%r535, %r536}, %rd242;
	add.f32 	%r537, %r535, %r534;
	add.f32 	%r538, %r536, %r537;
	mov.b64 	{%r539, %r540}, %rd243;
	add.f32 	%r541, %r539, %r538;
	add.f32 	%r542, %r540, %r541;
	mov.b64 	{%r543, %r544}, %rd244;
	add.f32 	%r545, %r543, %r542;
	add.f32 	%r546, %r544, %r545;
	mov.b64 	{%r547, %r548}, %rd245;
	add.f32 	%r549, %r547, %r546;
	add.f32 	%r550, %r548, %r549;
	mov.b64 	{%r551, %r552}, %rd246;
	add.f32 	%r553, %r551, %r550;
	add.f32 	%r554, %r552, %r553;
	mov.b64 	{%r555, %r556}, %rd247;
	add.f32 	%r557, %r555, %r554;
	add.f32 	%r558, %r556, %r557;
	shfl.sync.bfly.b32 	%r559, %r558, 16, 31, -1;
	add.f32 	%r560, %r558, %r559;
	shfl.sync.bfly.b32 	%r561, %r560, 8, 31, -1;
	add.f32 	%r562, %r560, %r561;
	shfl.sync.bfly.b32 	%r563, %r562, 4, 31, -1;
	add.f32 	%r564, %r562, %r563;
	shfl.sync.bfly.b32 	%r565, %r564, 2, 31, -1;
	add.f32 	%r566, %r564, %r565;
	shfl.sync.bfly.b32 	%r567, %r566, 1, 31, -1;
	add.f32 	%r489, %r566, %r567;
	// begin inline asm
	@%p7 st.shared.b32 [ %r488 + 0 ], %r489;
	// end inline asm
	bar.sync 	0;
	// begin inline asm
	@%p8 ld.shared.b32 %r490, [ %r492 + 0 ];
	// end inline asm
	shfl.sync.bfly.b32 	%r568, %r490, 4, 31, -1;
	add.f32 	%r569, %r490, %r568;
	shfl.sync.bfly.b32 	%r570, %r569, 2, 31, -1;
	add.f32 	%r571, %r569, %r570;
	shfl.sync.bfly.b32 	%r572, %r571, 1, 31, -1;
	add.f32 	%r493, %r571, %r572;
	// begin inline asm
	@%p18 st.shared.b32 [ %r492 + 0 ], %r493;
	// end inline asm
	bar.sync 	0;
	ld.shared.b32 	%r573, [global_smem];
	div.full.f32 	%r574, %r573, %r15;
	add.f32 	%r575, %r31, %r574;
	sqrt.approx.ftz.f32 	%r576, %r575;
	mov.b32 	%r577, 0f3F800000;
	div.full.f32 	%r495, %r577, %r576;
	mul.wide.u32 	%rd171, %r1, 4;
	add.s64 	%rd169, %rd103, %rd171;
	// begin inline asm
	@%p18 st.global.b32 [ %rd169 + 0 ], { %r494 };
	// end inline asm
	add.s64 	%rd170, %rd104, %rd171;
	// begin inline asm
	@%p18 st.global.b32 [ %rd170 + 0 ], { %r495 };
	// end inline asm
	@%p1 bra 	$L__BB0_10;
// %bb.8:                               // %.lr.ph9.preheader
	ld.param.b64 	%rd106, [_layer_norm_fwd_fused_param_1];
	ld.param.b64 	%rd102, [_layer_norm_fwd_fused_param_3];
	ld.param.b64 	%rd101, [_layer_norm_fwd_fused_param_2];
	add.s64 	%rd1, %rd106, %rd107;
	mov.b64 	%rd100, {%r495, %r495};
	mov.b32 	%r615, 0;
	mov.b32 	%r862, %r615;
$L__BB0_9:                              // %.lr.ph9
                                        // =>This Inner Loop Header: Depth=1
	add.s32 	%r659, %r5, %r862;
	add.s32 	%r660, %r659, 2048;
	add.s32 	%r661, %r659, 4096;
	add.s32 	%r662, %r8, %r862;
	setp.lt.s32 	%p22, %r659, %r30;
	setp.lt.s32 	%p23, %r660, %r30;
	setp.lt.s32 	%p24, %r661, %r30;
	setp.lt.s32 	%p25, %r662, %r30;
	mul.wide.s32 	%rd188, %r659, 2;
	add.s64 	%rd172, %rd101, %rd188;
	cvt.s64.s32 	%rd189, %r862;
	add.s64 	%rd191, %rd189, %rd199;
	shl.b64 	%rd192, %rd191, 1;
	add.s64 	%rd193, %rd101, %rd192;
	add.s64 	%rd173, %rd193, 4096;
	add.s64 	%rd174, %rd193, 8192;
	mul.wide.s32 	%rd194, %r662, 2;
	add.s64 	%rd175, %rd101, %rd194;
	// begin inline asm
	mov.u32 %r579, 0x0;
	mov.u32 %r580, 0x0;
	mov.u32 %r581, 0x0;
	mov.u32 %r582, 0x0;
	@%p22 ld.global.v4.b32 { %r579, %r580, %r581, %r582 }, [ %rd172 + 0 ];
	// end inline asm
	// begin inline asm
	mov.u32 %r583, 0x0;
	mov.u32 %r584, 0x0;
	mov.u32 %r585, 0x0;
	mov.u32 %r586, 0x0;
	@%p23 ld.global.v4.b32 { %r583, %r584, %r585, %r586 }, [ %rd173 + 0 ];
	// end inline asm
	// begin inline asm
	mov.u32 %r587, 0x0;
	mov.u32 %r588, 0x0;
	mov.u32 %r589, 0x0;
	mov.u32 %r590, 0x0;
	@%p24 ld.global.v4.b32 { %r587, %r588, %r589, %r590 }, [ %rd174 + 0 ];
	// end inline asm
	// begin inline asm
	mov.u32 %r591, 0x0;
	mov.u32 %r592, 0x0;
	mov.u32 %r593, 0x0;
	mov.u32 %r594, 0x0;
	@%p25 ld.global.v4.b32 { %r591, %r592, %r593, %r594 }, [ %rd175 + 0 ];
	// end inline asm
	add.s64 	%rd176, %rd102, %rd188;
	add.s64 	%rd195, %rd102, %rd192;
	add.s64 	%rd177, %rd195, 4096;
	add.s64 	%rd178, %rd195, 8192;
	add.s64 	%rd179, %rd102, %rd194;
	// begin inline asm
	mov.u32 %r595, 0x0;
	mov.u32 %r596, 0x0;
	mov.u32 %r597, 0x0;
	mov.u32 %r598, 0x0;
	@%p22 ld.global.v4.b32 { %r595, %r596, %r597, %r598 }, [ %rd176 + 0 ];
	// end inline asm
	// begin inline asm
	mov.u32 %r599, 0x0;
	mov.u32 %r600, 0x0;
	mov.u32 %r601, 0x0;
	mov.u32 %r602, 0x0;
	@%p23 ld.global.v4.b32 { %r599, %r600, %r601, %r602 }, [ %rd177 + 0 ];
	// end inline asm
	// begin inline asm
	mov.u32 %r603, 0x0;
	mov.u32 %r604, 0x0;
	mov.u32 %r605, 0x0;
	mov.u32 %r606, 0x0;
	@%p24 ld.global.v4.b32 { %r603, %r604, %r605, %r606 }, [ %rd178 + 0 ];
	// end inline asm
	// begin inline asm
	mov.u32 %r607, 0x0;
	mov.u32 %r608, 0x0;
	mov.u32 %r609, 0x0;
	mov.u32 %r610, 0x0;
	@%p25 ld.global.v4.b32 { %r607, %r608, %r609, %r610 }, [ %rd179 + 0 ];
	// end inline asm
	add.s64 	%rd180, %rd2, %rd188;
	add.s64 	%rd196, %rd2, %rd192;
	add.s64 	%rd181, %rd196, 4096;
	add.s64 	%rd182, %rd196, 8192;
	add.s64 	%rd183, %rd2, %rd194;
	// begin inline asm
	mov.u32 %r611, %r615;
	mov.u32 %r612, %r615;
	mov.u32 %r613, %r615;
	mov.u32 %r614, %r615;
	@%p22 ld.global.v4.b32 { %r611, %r612, %r613, %r614 }, [ %rd180 + 0 ];
	// end inline asm
	// begin inline asm
	mov.u32 %r619, %r615;
	mov.u32 %r620, %r615;
	mov.u32 %r621, %r615;
	mov.u32 %r622, %r615;
	@%p23 ld.global.v4.b32 { %r619, %r620, %r621, %r622 }, [ %rd181 + 0 ];
	// end inline asm
	// begin inline asm
	mov.u32 %r627, %r615;
	mov.u32 %r628, %r615;
	mov.u32 %r629, %r615;
	mov.u32 %r630, %r615;
	@%p24 ld.global.v4.b32 { %r627, %r628, %r629, %r630 }, [ %rd182 + 0 ];
	// end inline asm
	// begin inline asm
	mov.u32 %r635, %r615;
	mov.u32 %r636, %r615;
	mov.u32 %r637, %r615;
	mov.u32 %r638, %r615;
	@%p25 ld.global.v4.b32 { %r635, %r636, %r637, %r638 }, [ %rd183 + 0 ];
	// end inline asm
	add.s64 	%rd184, %rd1, %rd188;
	add.s64 	%rd197, %rd1, %rd192;
	add.s64 	%rd185, %rd197, 4096;
	add.s64 	%rd186, %rd197, 8192;
	add.s64 	%rd187, %rd1, %rd194;
	mov.b32 	{%rs65, %rs66}, %r611;
	cvt.f32.f16 	%r663, %rs66;
	cvt.f32.f16 	%r664, %rs65;
	mov.b64 	{%r665, %r666}, %rd198;
	sub.f32 	%r667, %r664, %r665;
	sub.f32 	%r668, %r663, %r666;
	mov.b64 	{%r669, %r670}, %rd100;
	mul.f32 	%r671, %r670, %r668;
	mul.f32 	%r672, %r669, %r667;
	mov.b32 	{%rs67, %rs68}, %r579;
	cvt.f32.f16 	%r673, %rs68;
	cvt.f32.f16 	%r674, %rs67;
	mov.b32 	{%rs69, %rs70}, %r595;
	cvt.f32.f16 	%r675, %rs70;
	cvt.f32.f16 	%r676, %rs69;
	fma.rn.f32 	%r677, %r672, %r674, %r676;
	fma.rn.f32 	%r678, %r671, %r673, %r675;
	cvt.rn.f16x2.f32 	%r643, %r678, %r677;
	mov.b32 	{%rs71, %rs72}, %r612;
	cvt.f32.f16 	%r679, %rs72;
	cvt.f32.f16 	%r680, %rs71;
	sub.f32 	%r681, %r680, %r665;
	sub.f32 	%r682, %r679, %r666;
	mul.f32 	%r683, %r670, %r682;
	mul.f32 	%r684, %r669, %r681;
	mov.b32 	{%rs73, %rs74}, %r580;
	cvt.f32.f16 	%r685, %rs74;
	cvt.f32.f16 	%r686, %rs73;
	mov.b32 	{%rs75, %rs76}, %r596;
	cvt.f32.f16 	%r687, %rs76;
	cvt.f32.f16 	%r688, %rs75;
	fma.rn.f32 	%r689, %r684, %r686, %r688;
	fma.rn.f32 	%r690, %r683, %r685, %r687;
	cvt.rn.f16x2.f32 	%r644, %r690, %r689;
	mov.b32 	{%rs77, %rs78}, %r613;
	cvt.f32.f16 	%r691, %rs78;
	cvt.f32.f16 	%r692, %rs77;
	sub.f32 	%r693, %r692, %r665;
	sub.f32 	%r694, %r691, %r666;
	mul.f32 	%r695, %r670, %r694;
	mul.f32 	%r696, %r669, %r693;
	mov.b32 	{%rs79, %rs80}, %r581;
	cvt.f32.f16 	%r697, %rs80;
	cvt.f32.f16 	%r698, %rs79;
	mov.b32 	{%rs81, %rs82}, %r597;
	cvt.f32.f16 	%r699, %rs82;
	cvt.f32.f16 	%r700, %rs81;
	fma.rn.f32 	%r701, %r696, %r698, %r700;
	fma.rn.f32 	%r702, %r695, %r697, %r699;
	cvt.rn.f16x2.f32 	%r645, %r702, %r701;
	mov.b32 	{%rs83, %rs84}, %r614;
	cvt.f32.f16 	%r703, %rs84;
	cvt.f32.f16 	%r704, %rs83;
	sub.f32 	%r705, %r704, %r665;
	sub.f32 	%r706, %r703, %r666;
	mul.f32 	%r707, %r670, %r706;
	mul.f32 	%r708, %r669, %r705;
	mov.b32 	{%rs85, %rs86}, %r582;
	cvt.f32.f16 	%r709, %rs86;
	cvt.f32.f16 	%r710, %rs85;
	mov.b32 	{%rs87, %rs88}, %r598;
	cvt.f32.f16 	%r711, %rs88;
	cvt.f32.f16 	%r712, %rs87;
	fma.rn.f32 	%r713, %r708, %r710, %r712;
	fma.rn.f32 	%r714, %r707, %r709, %r711;
	cvt.rn.f16x2.f32 	%r646, %r714, %r713;
	mov.b32 	{%rs89, %rs90}, %r619;
	cvt.f32.f16 	%r715, %rs90;
	cvt.f32.f16 	%r716, %rs89;
	sub.f32 	%r717, %r716, %r665;
	sub.f32 	%r718, %r715, %r666;
	mul.f32 	%r719, %r670, %r718;
	mul.f32 	%r720, %r669, %r717;
	mov.b32 	{%rs91, %rs92}, %r583;
	cvt.f32.f16 	%r721, %rs92;
	cvt.f32.f16 	%r722, %rs91;
	mov.b32 	{%rs93, %rs94}, %r599;
	cvt.f32.f16 	%r723, %rs94;
	cvt.f32.f16 	%r724, %rs93;
	fma.rn.f32 	%r725, %r720, %r722, %r724;
	fma.rn.f32 	%r726, %r719, %r721, %r723;
	cvt.rn.f16x2.f32 	%r647, %r726, %r725;
	mov.b32 	{%rs95, %rs96}, %r620;
	cvt.f32.f16 	%r727, %rs96;
	cvt.f32.f16 	%r728, %rs95;
	sub.f32 	%r729, %r728, %r665;
	sub.f32 	%r730, %r727, %r666;
	mul.f32 	%r731, %r670, %r730;
	mul.f32 	%r732, %r669, %r729;
	mov.b32 	{%rs97, %rs98}, %r584;
	cvt.f32.f16 	%r733, %rs98;
	cvt.f32.f16 	%r734, %rs97;
	mov.b32 	{%rs99, %rs100}, %r600;
	cvt.f32.f16 	%r735, %rs100;
	cvt.f32.f16 	%r736, %rs99;
	fma.rn.f32 	%r737, %r732, %r734, %r736;
	fma.rn.f32 	%r738, %r731, %r733, %r735;
	cvt.rn.f16x2.f32 	%r648, %r738, %r737;
	mov.b32 	{%rs101, %rs102}, %r621;
	cvt.f32.f16 	%r739, %rs102;
	cvt.f32.f16 	%r740, %rs101;
	sub.f32 	%r741, %r740, %r665;
	sub.f32 	%r742, %r739, %r666;
	mul.f32 	%r743, %r670, %r742;
	mul.f32 	%r744, %r669, %r741;
	mov.b32 	{%rs103, %rs104}, %r585;
	cvt.f32.f16 	%r745, %rs104;
	cvt.f32.f16 	%r746, %rs103;
	mov.b32 	{%rs105, %rs106}, %r601;
	cvt.f32.f16 	%r747, %rs106;
	cvt.f32.f16 	%r748, %rs105;
	fma.rn.f32 	%r749, %r744, %r746, %r748;
	fma.rn.f32 	%r750, %r743, %r745, %r747;
	cvt.rn.f16x2.f32 	%r649, %r750, %r749;
	mov.b32 	{%rs107, %rs108}, %r622;
	cvt.f32.f16 	%r751, %rs108;
	cvt.f32.f16 	%r752, %rs107;
	sub.f32 	%r753, %r752, %r665;
	sub.f32 	%r754, %r751, %r666;
	mul.f32 	%r755, %r670, %r754;
	mul.f32 	%r756, %r669, %r753;
	mov.b32 	{%rs109, %rs110}, %r586;
	cvt.f32.f16 	%r757, %rs110;
	cvt.f32.f16 	%r758, %rs109;
	mov.b32 	{%rs111, %rs112}, %r602;
	cvt.f32.f16 	%r759, %rs112;
	cvt.f32.f16 	%r760, %rs111;
	fma.rn.f32 	%r761, %r756, %r758, %r760;
	fma.rn.f32 	%r762, %r755, %r757, %r759;
	cvt.rn.f16x2.f32 	%r650, %r762, %r761;
	mov.b32 	{%rs113, %rs114}, %r627;
	cvt.f32.f16 	%r763, %rs114;
	cvt.f32.f16 	%r764, %rs113;
	sub.f32 	%r765, %r764, %r665;
	sub.f32 	%r766, %r763, %r666;
	mul.f32 	%r767, %r670, %r766;
	mul.f32 	%r768, %r669, %r765;
	mov.b32 	{%rs115, %rs116}, %r587;
	cvt.f32.f16 	%r769, %rs116;
	cvt.f32.f16 	%r770, %rs115;
	mov.b32 	{%rs117, %rs118}, %r603;
	cvt.f32.f16 	%r771, %rs118;
	cvt.f32.f16 	%r772, %rs117;
	fma.rn.f32 	%r773, %r768, %r770, %r772;
	fma.rn.f32 	%r774, %r767, %r769, %r771;
	cvt.rn.f16x2.f32 	%r651, %r774, %r773;
	mov.b32 	{%rs119, %rs120}, %r628;
	cvt.f32.f16 	%r775, %rs120;
	cvt.f32.f16 	%r776, %rs119;
	sub.f32 	%r777, %r776, %r665;
	sub.f32 	%r778, %r775, %r666;
	mul.f32 	%r779, %r670, %r778;
	mul.f32 	%r780, %r669, %r777;
	mov.b32 	{%rs121, %rs122}, %r588;
	cvt.f32.f16 	%r781, %rs122;
	cvt.f32.f16 	%r782, %rs121;
	mov.b32 	{%rs123, %rs124}, %r604;
	cvt.f32.f16 	%r783, %rs124;
	cvt.f32.f16 	%r784, %rs123;
	fma.rn.f32 	%r785, %r780, %r782, %r784;
	fma.rn.f32 	%r786, %r779, %r781, %r783;
	cvt.rn.f16x2.f32 	%r652, %r786, %r785;
	mov.b32 	{%rs125, %rs126}, %r629;
	cvt.f32.f16 	%r787, %rs126;
	cvt.f32.f16 	%r788, %rs125;
	sub.f32 	%r789, %r788, %r665;
	sub.f32 	%r790, %r787, %r666;
	mul.f32 	%r791, %r670, %r790;
	mul.f32 	%r792, %r669, %r789;
	mov.b32 	{%rs127, %rs128}, %r589;
	cvt.f32.f16 	%r793, %rs128;
	cvt.f32.f16 	%r794, %rs127;
	mov.b32 	{%rs129, %rs130}, %r605;
	cvt.f32.f16 	%r795, %rs130;
	cvt.f32.f16 	%r796, %rs129;
	fma.rn.f32 	%r797, %r792, %r794, %r796;
	fma.rn.f32 	%r798, %r791, %r793, %r795;
	cvt.rn.f16x2.f32 	%r653, %r798, %r797;
	mov.b32 	{%rs131, %rs132}, %r630;
	cvt.f32.f16 	%r799, %rs132;
	cvt.f32.f16 	%r800, %rs131;
	sub.f32 	%r801, %r800, %r665;
	sub.f32 	%r802, %r799, %r666;
	mul.f32 	%r803, %r670, %r802;
	mul.f32 	%r804, %r669, %r801;
	mov.b32 	{%rs133, %rs134}, %r590;
	cvt.f32.f16 	%r805, %rs134;
	cvt.f32.f16 	%r806, %rs133;
	mov.b32 	{%rs135, %rs136}, %r606;
	cvt.f32.f16 	%r807, %rs136;
	cvt.f32.f16 	%r808, %rs135;
	fma.rn.f32 	%r809, %r804, %r806, %r808;
	fma.rn.f32 	%r810, %r803, %r805, %r807;
	cvt.rn.f16x2.f32 	%r654, %r810, %r809;
	mov.b32 	{%rs137, %rs138}, %r635;
	cvt.f32.f16 	%r811, %rs138;
	cvt.f32.f16 	%r812, %rs137;
	sub.f32 	%r813, %r812, %r665;
	sub.f32 	%r814, %r811, %r666;
	mul.f32 	%r815, %r670, %r814;
	mul.f32 	%r816, %r669, %r813;
	mov.b32 	{%rs139, %rs140}, %r591;
	cvt.f32.f16 	%r817, %rs140;
	cvt.f32.f16 	%r818, %rs139;
	mov.b32 	{%rs141, %rs142}, %r607;
	cvt.f32.f16 	%r819, %rs142;
	cvt.f32.f16 	%r820, %rs141;
	fma.rn.f32 	%r821, %r816, %r818, %r820;
	fma.rn.f32 	%r822, %r815, %r817, %r819;
	cvt.rn.f16x2.f32 	%r655, %r822, %r821;
	mov.b32 	{%rs143, %rs144}, %r636;
	cvt.f32.f16 	%r823, %rs144;
	cvt.f32.f16 	%r824, %rs143;
	sub.f32 	%r825, %r824, %r665;
	sub.f32 	%r826, %r823, %r666;
	mul.f32 	%r827, %r670, %r826;
	mul.f32 	%r828, %r669, %r825;
	mov.b32 	{%rs145, %rs146}, %r592;
	cvt.f32.f16 	%r829, %rs146;
	cvt.f32.f16 	%r830, %rs145;
	mov.b32 	{%rs147, %rs148}, %r608;
	cvt.f32.f16 	%r831, %rs148;
	cvt.f32.f16 	%r832, %rs147;
	fma.rn.f32 	%r833, %r828, %r830, %r832;
	fma.rn.f32 	%r834, %r827, %r829, %r831;
	cvt.rn.f16x2.f32 	%r656, %r834, %r833;
	mov.b32 	{%rs149, %rs150}, %r637;
	cvt.f32.f16 	%r835, %rs150;
	cvt.f32.f16 	%r836, %rs149;
	sub.f32 	%r837, %r836, %r665;
	sub.f32 	%r838, %r835, %r666;
	mul.f32 	%r839, %r670, %r838;
	mul.f32 	%r840, %r669, %r837;
	mov.b32 	{%rs151, %rs152}, %r593;
	cvt.f32.f16 	%r841, %rs152;
	cvt.f32.f16 	%r842, %rs151;
	mov.b32 	{%rs153, %rs154}, %r609;
	cvt.f32.f16 	%r843, %rs154;
	cvt.f32.f16 	%r844, %rs153;
	fma.rn.f32 	%r845, %r840, %r842, %r844;
	fma.rn.f32 	%r846, %r839, %r841, %r843;
	cvt.rn.f16x2.f32 	%r657, %r846, %r845;
	mov.b32 	{%rs155, %rs156}, %r638;
	cvt.f32.f16 	%r847, %rs156;
	cvt.f32.f16 	%r848, %rs155;
	sub.f32 	%r849, %r848, %r665;
	sub.f32 	%r850, %r847, %r666;
	mul.f32 	%r851, %r670, %r850;
	mul.f32 	%r852, %r669, %r849;
	mov.b32 	{%rs157, %rs158}, %r594;
	cvt.f32.f16 	%r853, %rs158;
	cvt.f32.f16 	%r854, %rs157;
	mov.b32 	{%rs159, %rs160}, %r610;
	cvt.f32.f16 	%r855, %rs160;
	cvt.f32.f16 	%r856, %rs159;
	fma.rn.f32 	%r857, %r852, %r854, %r856;
	fma.rn.f32 	%r858, %r851, %r853, %r855;
	cvt.rn.f16x2.f32 	%r658, %r858, %r857;
	// begin inline asm
	@%p22 st.global.v4.b32 [ %rd184 + 0 ], { %r643, %r644, %r645, %r646 };
	// end inline asm
	// begin inline asm
	@%p23 st.global.v4.b32 [ %rd185 + 0 ], { %r647, %r648, %r649, %r650 };
	// end inline asm
	// begin inline asm
	@%p24 st.global.v4.b32 [ %rd186 + 0 ], { %r651, %r652, %r653, %r654 };
	// end inline asm
	// begin inline asm
	@%p25 st.global.v4.b32 [ %rd187 + 0 ], { %r655, %r656, %r657, %r658 };
	// end inline asm
	add.s32 	%r862, %r862, 8192;
	setp.lt.s32 	%p38, %r862, %r30;
	@%p38 bra 	$L__BB0_9;
$L__BB0_10:                             // %._crit_edge10
	ret;
                                        // -- End function
}
