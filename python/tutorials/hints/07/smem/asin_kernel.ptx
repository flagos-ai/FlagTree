//
// Generated by LLVM NVPTX Back-End
//

.version 8.7
.target sm_90a
.address_size 64

	// .globl	asin_kernel             // -- Begin function asin_kernel
.extern .shared .align 16 .b8 global_smem[];
.global .align 1 .b8 _$_str[11] = {95, 95, 67, 85, 68, 65, 95, 70, 84, 90};
.global .align 1 .b8 _$_str_$_2[17] = {95, 95, 67, 85, 68, 65, 95, 80, 82, 69, 67, 95, 83, 81, 82, 84};
                                        // @asin_kernel
.visible .entry asin_kernel(
	.param .u64 .ptr .global .align 1 asin_kernel_param_0,
	.param .u64 .ptr .global .align 1 asin_kernel_param_1,
	.param .u32 asin_kernel_param_2,
	.param .u64 .ptr .global .align 1 asin_kernel_param_3,
	.param .u64 .ptr .global .align 1 asin_kernel_param_4
)
.reqntid 128
{
	.reg .pred 	%p<21>;
	.reg .b32 	%r<240>;
	.reg .b64 	%rd<9>;

// %bb.0:                               // %__nv_sqrtf.exit.i
	ld.param.b32 	%r33, [asin_kernel_param_2];
	ld.param.b64 	%rd5, [asin_kernel_param_0];
	mov.u32 	%r38, %ctaid.x;
	shl.b32 	%r39, %r38, 10;
	mov.u32 	%r40, %tid.x;
	and.b32 	%r41, %r40, 127;
	shl.b32 	%r42, %r41, 2;
	or.b32 	%r43, %r42, %r39;
	or.b32 	%r1, %r43, 512;
	setp.lt.s32 	%p1, %r43, %r33;
	setp.lt.s32 	%p2, %r1, %r33;
	mad.wide.s32 	%rd3, %r43, 4, %rd5;
	add.s64 	%rd4, %rd3, 2048;
	shl.b32 	%r44, %r41, 4;
	mov.b32 	%r45, global_smem;
	add.s32 	%r34, %r45, %r44;
	selp.b32 	%r35, 16, 0, %p1;
	// begin inline asm
	cp.async.cg.shared.global [ %r34 + 0 ], [ %rd3 + 0 ], 0x10, %r35;
	// end inline asm
	add.s32 	%r36, %r34, 2048;
	selp.b32 	%r37, 16, 0, %p2;
	// begin inline asm
	cp.async.cg.shared.global [ %r36 + 0 ], [ %rd4 + 0 ], 0x10, %r37;
	// end inline asm
	cp.async.commit_group;
	cp.async.wait_group 	0;
	bar.sync 	0;
	ld.shared.v4.b32 	{%r46, %r2, %r3, %r4}, [%r34];
	abs.ftz.f32 	%r47, %r46;
	mov.b32 	%r48, 0f3F800000;
	sub.f32 	%r49, %r48, %r47;
	mul.f32 	%r50, %r49, 0f3F000000;
	sqrt.approx.ftz.f32 	%r51, %r50;
	setp.gt.f32 	%p3, %r47, 0f3F11EB85;
	selp.f32 	%r52, %r51, %r47, %p3;
	mul.f32 	%r53, %r52, %r52;
	mov.b32 	%r54, 0f3C94D2E9;
	mov.b32 	%r55, 0f3D53F941;
	fma.rn.ftz.f32 	%r56, %r55, %r53, %r54;
	mov.b32 	%r57, 0f3D3F841F;
	fma.rn.ftz.f32 	%r58, %r56, %r53, %r57;
	mov.b32 	%r59, 0f3D994929;
	fma.rn.ftz.f32 	%r60, %r58, %r53, %r59;
	mov.b32 	%r61, 0f3E2AAB94;
	fma.rn.ftz.f32 	%r62, %r60, %r53, %r61;
	mul.f32 	%r63, %r53, %r62;
	fma.rn.ftz.f32 	%r64, %r63, %r52, %r52;
	mov.b32 	%r65, 0f3FC90FDB;
	mov.b32 	%r66, 0fC0000000;
	fma.rn.ftz.f32 	%r67, %r66, %r64, %r65;
	selp.f32 	%r232, %r67, %r64, %p3;
	setp.nan.f32 	%p4, %r232, %r232;
	@%p4 bra 	$L__BB0_2;
// %bb.1:
	and.b32 	%r68, %r46, -2147483648;
	or.b32 	%r232, %r68, %r232;
$L__BB0_2:                              // %__nv_asinf.exit
	abs.ftz.f32 	%r69, %r2;
	sub.f32 	%r71, %r48, %r69;
	mul.f32 	%r72, %r71, 0f3F000000;
	sqrt.approx.ftz.f32 	%r73, %r72;
	setp.gt.f32 	%p5, %r69, 0f3F11EB85;
	selp.f32 	%r74, %r73, %r69, %p5;
	mul.f32 	%r75, %r74, %r74;
	fma.rn.ftz.f32 	%r78, %r55, %r75, %r54;
	fma.rn.ftz.f32 	%r80, %r78, %r75, %r57;
	fma.rn.ftz.f32 	%r82, %r80, %r75, %r59;
	fma.rn.ftz.f32 	%r84, %r82, %r75, %r61;
	mul.f32 	%r85, %r75, %r84;
	fma.rn.ftz.f32 	%r86, %r85, %r74, %r74;
	fma.rn.ftz.f32 	%r89, %r66, %r86, %r65;
	selp.f32 	%r233, %r89, %r86, %p5;
	setp.nan.f32 	%p6, %r233, %r233;
	@%p6 bra 	$L__BB0_4;
// %bb.3:
	and.b32 	%r90, %r2, -2147483648;
	or.b32 	%r233, %r90, %r233;
$L__BB0_4:                              // %__nv_asinf.exit30
	abs.ftz.f32 	%r91, %r3;
	mov.b32 	%r92, 0f3F800000;
	sub.f32 	%r93, %r92, %r91;
	mul.f32 	%r94, %r93, 0f3F000000;
	sqrt.approx.ftz.f32 	%r95, %r94;
	setp.gt.f32 	%p7, %r91, 0f3F11EB85;
	selp.f32 	%r96, %r95, %r91, %p7;
	mul.f32 	%r97, %r96, %r96;
	mov.b32 	%r98, 0f3C94D2E9;
	mov.b32 	%r99, 0f3D53F941;
	fma.rn.ftz.f32 	%r100, %r99, %r97, %r98;
	mov.b32 	%r101, 0f3D3F841F;
	fma.rn.ftz.f32 	%r102, %r100, %r97, %r101;
	mov.b32 	%r103, 0f3D994929;
	fma.rn.ftz.f32 	%r104, %r102, %r97, %r103;
	mov.b32 	%r105, 0f3E2AAB94;
	fma.rn.ftz.f32 	%r106, %r104, %r97, %r105;
	mul.f32 	%r107, %r97, %r106;
	fma.rn.ftz.f32 	%r108, %r107, %r96, %r96;
	mov.b32 	%r109, 0f3FC90FDB;
	mov.b32 	%r110, 0fC0000000;
	fma.rn.ftz.f32 	%r111, %r110, %r108, %r109;
	selp.f32 	%r234, %r111, %r108, %p7;
	setp.nan.f32 	%p8, %r234, %r234;
	@%p8 bra 	$L__BB0_6;
// %bb.5:
	and.b32 	%r112, %r3, -2147483648;
	or.b32 	%r234, %r112, %r234;
$L__BB0_6:                              // %__nv_asinf.exit53
	ld.shared.v4.b32 	{%r5, %r6, %r7, %r8}, [%r34+2048];
	abs.ftz.f32 	%r113, %r4;
	sub.f32 	%r115, %r92, %r113;
	mul.f32 	%r116, %r115, 0f3F000000;
	sqrt.approx.ftz.f32 	%r117, %r116;
	setp.gt.f32 	%p9, %r113, 0f3F11EB85;
	selp.f32 	%r118, %r117, %r113, %p9;
	mul.f32 	%r119, %r118, %r118;
	fma.rn.ftz.f32 	%r122, %r99, %r119, %r98;
	fma.rn.ftz.f32 	%r124, %r122, %r119, %r101;
	fma.rn.ftz.f32 	%r126, %r124, %r119, %r103;
	fma.rn.ftz.f32 	%r128, %r126, %r119, %r105;
	mul.f32 	%r129, %r119, %r128;
	fma.rn.ftz.f32 	%r130, %r129, %r118, %r118;
	fma.rn.ftz.f32 	%r133, %r110, %r130, %r109;
	selp.f32 	%r235, %r133, %r130, %p9;
	setp.nan.f32 	%p10, %r235, %r235;
	@%p10 bra 	$L__BB0_8;
// %bb.7:
	and.b32 	%r134, %r4, -2147483648;
	or.b32 	%r235, %r134, %r235;
$L__BB0_8:                              // %__nv_asinf.exit76
	abs.ftz.f32 	%r135, %r5;
	mov.b32 	%r136, 0f3F800000;
	sub.f32 	%r137, %r136, %r135;
	mul.f32 	%r138, %r137, 0f3F000000;
	sqrt.approx.ftz.f32 	%r139, %r138;
	setp.gt.f32 	%p11, %r135, 0f3F11EB85;
	selp.f32 	%r140, %r139, %r135, %p11;
	mul.f32 	%r141, %r140, %r140;
	mov.b32 	%r142, 0f3C94D2E9;
	mov.b32 	%r143, 0f3D53F941;
	fma.rn.ftz.f32 	%r144, %r143, %r141, %r142;
	mov.b32 	%r145, 0f3D3F841F;
	fma.rn.ftz.f32 	%r146, %r144, %r141, %r145;
	mov.b32 	%r147, 0f3D994929;
	fma.rn.ftz.f32 	%r148, %r146, %r141, %r147;
	mov.b32 	%r149, 0f3E2AAB94;
	fma.rn.ftz.f32 	%r150, %r148, %r141, %r149;
	mul.f32 	%r151, %r141, %r150;
	fma.rn.ftz.f32 	%r152, %r151, %r140, %r140;
	mov.b32 	%r153, 0f3FC90FDB;
	mov.b32 	%r154, 0fC0000000;
	fma.rn.ftz.f32 	%r155, %r154, %r152, %r153;
	selp.f32 	%r236, %r155, %r152, %p11;
	setp.nan.f32 	%p12, %r236, %r236;
	@%p12 bra 	$L__BB0_10;
// %bb.9:
	and.b32 	%r156, %r5, -2147483648;
	or.b32 	%r236, %r156, %r236;
$L__BB0_10:                             // %__nv_asinf.exit99
	abs.ftz.f32 	%r157, %r6;
	sub.f32 	%r159, %r136, %r157;
	mul.f32 	%r160, %r159, 0f3F000000;
	sqrt.approx.ftz.f32 	%r161, %r160;
	setp.gt.f32 	%p13, %r157, 0f3F11EB85;
	selp.f32 	%r162, %r161, %r157, %p13;
	mul.f32 	%r163, %r162, %r162;
	fma.rn.ftz.f32 	%r166, %r143, %r163, %r142;
	fma.rn.ftz.f32 	%r168, %r166, %r163, %r145;
	fma.rn.ftz.f32 	%r170, %r168, %r163, %r147;
	fma.rn.ftz.f32 	%r172, %r170, %r163, %r149;
	mul.f32 	%r173, %r163, %r172;
	fma.rn.ftz.f32 	%r174, %r173, %r162, %r162;
	fma.rn.ftz.f32 	%r177, %r154, %r174, %r153;
	selp.f32 	%r237, %r177, %r174, %p13;
	setp.nan.f32 	%p14, %r237, %r237;
	@%p14 bra 	$L__BB0_12;
// %bb.11:
	and.b32 	%r178, %r6, -2147483648;
	or.b32 	%r237, %r178, %r237;
$L__BB0_12:                             // %__nv_asinf.exit122
	abs.ftz.f32 	%r179, %r7;
	mov.b32 	%r180, 0f3F800000;
	sub.f32 	%r181, %r180, %r179;
	mul.f32 	%r182, %r181, 0f3F000000;
	sqrt.approx.ftz.f32 	%r183, %r182;
	setp.gt.f32 	%p15, %r179, 0f3F11EB85;
	selp.f32 	%r184, %r183, %r179, %p15;
	mul.f32 	%r185, %r184, %r184;
	mov.b32 	%r186, 0f3C94D2E9;
	mov.b32 	%r187, 0f3D53F941;
	fma.rn.ftz.f32 	%r188, %r187, %r185, %r186;
	mov.b32 	%r189, 0f3D3F841F;
	fma.rn.ftz.f32 	%r190, %r188, %r185, %r189;
	mov.b32 	%r191, 0f3D994929;
	fma.rn.ftz.f32 	%r192, %r190, %r185, %r191;
	mov.b32 	%r193, 0f3E2AAB94;
	fma.rn.ftz.f32 	%r194, %r192, %r185, %r193;
	mul.f32 	%r195, %r185, %r194;
	fma.rn.ftz.f32 	%r196, %r195, %r184, %r184;
	mov.b32 	%r197, 0f3FC90FDB;
	mov.b32 	%r198, 0fC0000000;
	fma.rn.ftz.f32 	%r199, %r198, %r196, %r197;
	selp.f32 	%r238, %r199, %r196, %p15;
	setp.nan.f32 	%p16, %r238, %r238;
	@%p16 bra 	$L__BB0_14;
// %bb.13:
	and.b32 	%r200, %r7, -2147483648;
	or.b32 	%r238, %r200, %r238;
$L__BB0_14:                             // %__nv_asinf.exit145
	ld.param.b64 	%rd2, [asin_kernel_param_1];
	cvt.s64.s32 	%rd1, %r43;
	abs.ftz.f32 	%r201, %r8;
	sub.f32 	%r203, %r180, %r201;
	mul.f32 	%r204, %r203, 0f3F000000;
	sqrt.approx.ftz.f32 	%r205, %r204;
	setp.gt.f32 	%p17, %r201, 0f3F11EB85;
	selp.f32 	%r206, %r205, %r201, %p17;
	mul.f32 	%r207, %r206, %r206;
	fma.rn.ftz.f32 	%r210, %r187, %r207, %r186;
	fma.rn.ftz.f32 	%r212, %r210, %r207, %r189;
	fma.rn.ftz.f32 	%r214, %r212, %r207, %r191;
	fma.rn.ftz.f32 	%r216, %r214, %r207, %r193;
	mul.f32 	%r217, %r207, %r216;
	fma.rn.ftz.f32 	%r218, %r217, %r206, %r206;
	fma.rn.ftz.f32 	%r221, %r198, %r218, %r197;
	selp.f32 	%r239, %r221, %r218, %p17;
	setp.nan.f32 	%p18, %r239, %r239;
	@%p18 bra 	$L__BB0_16;
// %bb.15:
	and.b32 	%r222, %r8, -2147483648;
	or.b32 	%r239, %r222, %r239;
$L__BB0_16:                             // %__nv_asinf.exit168
	cvt.u32.u64 	%r231, %rd1;
	setp.lt.s32 	%p19, %r231, %r33;
	shl.b64 	%rd8, %rd1, 2;
	add.s64 	%rd6, %rd2, %rd8;
	add.s64 	%rd7, %rd6, 2048;
	// begin inline asm
	@%p19 st.global.v4.b32 [ %rd6 + 0 ], { %r232, %r233, %r234, %r235 };
	// end inline asm
	// begin inline asm
	@%p2 st.global.v4.b32 [ %rd7 + 0 ], { %r236, %r237, %r238, %r239 };
	// end inline asm
	ret;
                                        // -- End function
}
