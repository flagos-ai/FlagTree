//
// Generated by LLVM NVPTX Back-End
//

.version 8.7
.target sm_90a
.address_size 64

	// .globl	asin_kernel             // -- Begin function asin_kernel
.global .align 1 .b8 _$_str[11] = {95, 95, 67, 85, 68, 65, 95, 70, 84, 90};
.global .align 1 .b8 _$_str_$_2[17] = {95, 95, 67, 85, 68, 65, 95, 80, 82, 69, 67, 95, 83, 81, 82, 84};
                                        // @asin_kernel
.visible .entry asin_kernel(
	.param .u64 .ptr .global .align 1 asin_kernel_param_0,
	.param .u64 .ptr .global .align 1 asin_kernel_param_1,
	.param .u32 asin_kernel_param_2,
	.param .u64 .ptr .global .align 1 asin_kernel_param_3,
	.param .u64 .ptr .global .align 1 asin_kernel_param_4
)
.reqntid 128
{
	.reg .pred 	%p<21>;
	.reg .b32 	%r<169>;
	.reg .b64 	%rd<12>;

// %bb.0:                               // %__nv_sqrtf.exit.i
	ld.param.b64 	%rd5, [asin_kernel_param_0];
	ld.param.b64 	%rd6, [asin_kernel_param_1];
	mov.u32 	%r17, %ctaid.x;
	shl.b32 	%r18, %r17, 10;
	ld.param.b32 	%r19, [asin_kernel_param_2];
	mov.u32 	%r20, %tid.x;
	shl.b32 	%r21, %r20, 2;
	and.b32 	%r22, %r21, 508;
	or.b32 	%r23, %r22, %r18;
	or.b32 	%r24, %r23, 512;
	setp.lt.s32 	%p1, %r23, %r19;
	setp.lt.s32 	%p2, %r24, %r19;
	mul.wide.s32 	%rd7, %r23, 4;
	add.s64 	%rd1, %rd5, %rd7;
	add.s64 	%rd2, %rd1, 2048;
	// begin inline asm
	mov.u32 %r1, 0x0;
	mov.u32 %r2, 0x0;
	mov.u32 %r3, 0x0;
	mov.u32 %r4, 0x0;
	@%p1 ld.global.v4.b32 { %r1, %r2, %r3, %r4 }, [ %rd1 + 0 ];
	// end inline asm
	// begin inline asm
	mov.u32 %r5, 0x0;
	mov.u32 %r6, 0x0;
	mov.u32 %r7, 0x0;
	mov.u32 %r8, 0x0;
	@%p2 ld.global.v4.b32 { %r5, %r6, %r7, %r8 }, [ %rd2 + 0 ];
	// end inline asm
	abs.ftz.f32 	%r25, %r1;
	mov.b32 	%r26, 0f3F800000;
	sub.f32 	%r27, %r26, %r25;
	mul.f32 	%r28, %r27, 0f3F000000;
	sqrt.approx.ftz.f32 	%r29, %r28;
	abs.ftz.f32 	%r30, %r2;
	sub.f32 	%r31, %r26, %r30;
	mul.f32 	%r32, %r31, 0f3F000000;
	sqrt.approx.ftz.f32 	%r33, %r32;
	abs.ftz.f32 	%r34, %r3;
	sub.f32 	%r35, %r26, %r34;
	mul.f32 	%r36, %r35, 0f3F000000;
	sqrt.approx.ftz.f32 	%r37, %r36;
	abs.ftz.f32 	%r38, %r4;
	sub.f32 	%r39, %r26, %r38;
	mul.f32 	%r40, %r39, 0f3F000000;
	sqrt.approx.ftz.f32 	%r41, %r40;
	abs.ftz.f32 	%r42, %r5;
	sub.f32 	%r43, %r26, %r42;
	mul.f32 	%r44, %r43, 0f3F000000;
	sqrt.approx.ftz.f32 	%r45, %r44;
	abs.ftz.f32 	%r46, %r6;
	sub.f32 	%r47, %r26, %r46;
	mul.f32 	%r48, %r47, 0f3F000000;
	sqrt.approx.ftz.f32 	%r49, %r48;
	abs.ftz.f32 	%r50, %r7;
	sub.f32 	%r51, %r26, %r50;
	mul.f32 	%r52, %r51, 0f3F000000;
	sqrt.approx.ftz.f32 	%r53, %r52;
	abs.ftz.f32 	%r54, %r8;
	sub.f32 	%r55, %r26, %r54;
	mul.f32 	%r56, %r55, 0f3F000000;
	sqrt.approx.ftz.f32 	%r57, %r56;
	setp.gt.f32 	%p5, %r54, 0f3F11EB85;
	setp.gt.f32 	%p6, %r25, 0f3F11EB85;
	setp.gt.f32 	%p7, %r30, 0f3F11EB85;
	setp.gt.f32 	%p8, %r34, 0f3F11EB85;
	setp.gt.f32 	%p9, %r38, 0f3F11EB85;
	setp.gt.f32 	%p10, %r42, 0f3F11EB85;
	setp.gt.f32 	%p11, %r46, 0f3F11EB85;
	setp.gt.f32 	%p12, %r50, 0f3F11EB85;
	selp.f32 	%r58, %r53, %r50, %p12;
	mul.f32 	%r59, %r58, %r58;
	mov.b32 	%r60, 0f3C94D2E9;
	mov.b32 	%r61, 0f3D53F941;
	fma.rn.ftz.f32 	%r62, %r61, %r59, %r60;
	mov.b32 	%r63, 0f3D3F841F;
	fma.rn.ftz.f32 	%r64, %r62, %r59, %r63;
	mov.b32 	%r65, 0f3D994929;
	fma.rn.ftz.f32 	%r66, %r64, %r59, %r65;
	mov.b32 	%r67, 0f3E2AAB94;
	fma.rn.ftz.f32 	%r68, %r66, %r59, %r67;
	mul.f32 	%r69, %r59, %r68;
	fma.rn.ftz.f32 	%r70, %r69, %r58, %r58;
	and.b32 	%r71, %r7, -2147483648;
	selp.f32 	%r72, %r49, %r46, %p11;
	mul.f32 	%r73, %r72, %r72;
	fma.rn.ftz.f32 	%r74, %r61, %r73, %r60;
	fma.rn.ftz.f32 	%r75, %r74, %r73, %r63;
	fma.rn.ftz.f32 	%r76, %r75, %r73, %r65;
	fma.rn.ftz.f32 	%r77, %r76, %r73, %r67;
	mul.f32 	%r78, %r73, %r77;
	fma.rn.ftz.f32 	%r79, %r78, %r72, %r72;
	and.b32 	%r80, %r6, -2147483648;
	selp.f32 	%r81, %r45, %r42, %p10;
	mul.f32 	%r82, %r81, %r81;
	fma.rn.ftz.f32 	%r83, %r61, %r82, %r60;
	fma.rn.ftz.f32 	%r84, %r83, %r82, %r63;
	fma.rn.ftz.f32 	%r85, %r84, %r82, %r65;
	fma.rn.ftz.f32 	%r86, %r85, %r82, %r67;
	mul.f32 	%r87, %r82, %r86;
	fma.rn.ftz.f32 	%r88, %r87, %r81, %r81;
	and.b32 	%r89, %r5, -2147483648;
	selp.f32 	%r90, %r41, %r38, %p9;
	mul.f32 	%r91, %r90, %r90;
	fma.rn.ftz.f32 	%r92, %r61, %r91, %r60;
	fma.rn.ftz.f32 	%r93, %r92, %r91, %r63;
	fma.rn.ftz.f32 	%r94, %r93, %r91, %r65;
	fma.rn.ftz.f32 	%r95, %r94, %r91, %r67;
	mul.f32 	%r96, %r91, %r95;
	fma.rn.ftz.f32 	%r97, %r96, %r90, %r90;
	and.b32 	%r98, %r4, -2147483648;
	selp.f32 	%r99, %r37, %r34, %p8;
	mul.f32 	%r100, %r99, %r99;
	fma.rn.ftz.f32 	%r101, %r61, %r100, %r60;
	fma.rn.ftz.f32 	%r102, %r101, %r100, %r63;
	fma.rn.ftz.f32 	%r103, %r102, %r100, %r65;
	fma.rn.ftz.f32 	%r104, %r103, %r100, %r67;
	mul.f32 	%r105, %r100, %r104;
	fma.rn.ftz.f32 	%r106, %r105, %r99, %r99;
	and.b32 	%r107, %r3, -2147483648;
	selp.f32 	%r108, %r33, %r30, %p7;
	mul.f32 	%r109, %r108, %r108;
	fma.rn.ftz.f32 	%r110, %r61, %r109, %r60;
	fma.rn.ftz.f32 	%r111, %r110, %r109, %r63;
	fma.rn.ftz.f32 	%r112, %r111, %r109, %r65;
	fma.rn.ftz.f32 	%r113, %r112, %r109, %r67;
	mul.f32 	%r114, %r109, %r113;
	fma.rn.ftz.f32 	%r115, %r114, %r108, %r108;
	and.b32 	%r116, %r2, -2147483648;
	selp.f32 	%r117, %r29, %r25, %p6;
	mul.f32 	%r118, %r117, %r117;
	fma.rn.ftz.f32 	%r119, %r61, %r118, %r60;
	fma.rn.ftz.f32 	%r120, %r119, %r118, %r63;
	fma.rn.ftz.f32 	%r121, %r120, %r118, %r65;
	fma.rn.ftz.f32 	%r122, %r121, %r118, %r67;
	mul.f32 	%r123, %r118, %r122;
	fma.rn.ftz.f32 	%r124, %r123, %r117, %r117;
	and.b32 	%r125, %r1, -2147483648;
	selp.f32 	%r126, %r57, %r54, %p5;
	mul.f32 	%r127, %r126, %r126;
	fma.rn.ftz.f32 	%r128, %r61, %r127, %r60;
	fma.rn.ftz.f32 	%r129, %r128, %r127, %r63;
	fma.rn.ftz.f32 	%r130, %r129, %r127, %r65;
	fma.rn.ftz.f32 	%r131, %r130, %r127, %r67;
	mul.f32 	%r132, %r127, %r131;
	fma.rn.ftz.f32 	%r133, %r132, %r126, %r126;
	mov.b32 	%r134, 0f3FC90FDB;
	mov.b32 	%r135, 0fC0000000;
	fma.rn.ftz.f32 	%r136, %r135, %r70, %r134;
	fma.rn.ftz.f32 	%r137, %r135, %r79, %r134;
	fma.rn.ftz.f32 	%r138, %r135, %r88, %r134;
	fma.rn.ftz.f32 	%r139, %r135, %r97, %r134;
	fma.rn.ftz.f32 	%r140, %r135, %r106, %r134;
	fma.rn.ftz.f32 	%r141, %r135, %r115, %r134;
	fma.rn.ftz.f32 	%r142, %r135, %r124, %r134;
	fma.rn.ftz.f32 	%r143, %r135, %r133, %r134;
	selp.f32 	%r144, %r142, %r124, %p6;
	selp.f32 	%r145, %r143, %r133, %p5;
	mov.b64 	%rd8, {%r145, %r144};
	cvt.u32.u64 	%r146, %rd8;
	selp.f32 	%r147, %r136, %r70, %p12;
	selp.f32 	%r148, %r137, %r79, %p11;
	mov.b64 	%rd9, {%r148, %r147};
	mov.b64 	{_, %r149}, %rd9;
	cvt.u32.u64 	%r150, %rd9;
	selp.f32 	%r151, %r139, %r97, %p9;
	selp.f32 	%r152, %r138, %r88, %p10;
	mov.b64 	%rd10, {%r151, %r152};
	mov.b64 	{_, %r153}, %rd10;
	cvt.u32.u64 	%r154, %rd10;
	selp.f32 	%r155, %r140, %r106, %p8;
	selp.f32 	%r156, %r141, %r115, %p7;
	mov.b64 	%rd11, {%r156, %r155};
	mov.b64 	{_, %r157}, %rd11;
	cvt.u32.u64 	%r158, %rd11;
	mov.b64 	{_, %r159}, %rd8;
	setp.num.f32 	%p13, %r145, %r145;
	setp.num.f32 	%p14, %r147, %r147;
	setp.num.f32 	%p15, %r148, %r148;
	setp.num.f32 	%p16, %r152, %r152;
	setp.num.f32 	%p17, %r151, %r151;
	setp.num.f32 	%p18, %r155, %r155;
	setp.num.f32 	%p19, %r156, %r156;
	setp.num.f32 	%p20, %r144, %r144;
	and.b32 	%r160, %r8, -2147483648;
	add.s64 	%rd3, %rd6, %rd7;
	add.s64 	%rd4, %rd3, 2048;
	selp.b32 	%r161, %r125, 0, %p20;
	or.b32 	%r9, %r161, %r159;
	selp.b32 	%r162, %r116, 0, %p19;
	or.b32 	%r10, %r162, %r158;
	selp.b32 	%r163, %r107, 0, %p18;
	or.b32 	%r11, %r163, %r157;
	selp.b32 	%r164, %r98, 0, %p17;
	or.b32 	%r12, %r164, %r154;
	// begin inline asm
	@%p1 st.global.v4.b32 [ %rd3 + 0 ], { %r9, %r10, %r11, %r12 };
	// end inline asm
	selp.b32 	%r165, %r89, 0, %p16;
	or.b32 	%r13, %r165, %r153;
	selp.b32 	%r166, %r80, 0, %p15;
	or.b32 	%r14, %r166, %r150;
	selp.b32 	%r167, %r71, 0, %p14;
	or.b32 	%r15, %r167, %r149;
	selp.b32 	%r168, %r160, 0, %p13;
	or.b32 	%r16, %r168, %r146;
	// begin inline asm
	@%p2 st.global.v4.b32 [ %rd4 + 0 ], { %r13, %r14, %r15, %r16 };
	// end inline asm
	ret;
                                        // -- End function
}
