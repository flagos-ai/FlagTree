#blocked = #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [8], order = [0]}>
#loc = loc(unknown)
#loc1 = loc("output_ptr")
#loc2 = loc("input_ptr")
#loc3 = loc("input_row_stride")
#loc4 = loc("output_row_stride")
#loc5 = loc("n_rows")
#loc6 = loc("n_cols")
#shared = #ttg.swizzled_shared<{vec = 1, perPhase = 1, maxPhase = 1, order = [0]}>
#smem = #ttg.shared_memory
module attributes {"ttg.num-ctas" = 1 : i32, "ttg.num-warps" = 8 : i32, ttg.target = "cuda:90", "ttg.threads-per-warp" = 32 : i32} {
  tt.func public @softmax_kernel(%output_ptr: !tt.ptr<f32> {tt.divisibility = 16 : i32} loc("output_ptr"), %input_ptr: !tt.ptr<f32> {tt.divisibility = 16 : i32} loc("input_ptr"), %input_row_stride: i32 loc("input_row_stride"), %output_row_stride: i32 loc("output_row_stride"), %n_rows: i32 loc("n_rows"), %n_cols: i32 loc("n_cols")) attributes {noinline = false} {
    %cst = arith.constant dense<0xFF800000> : tensor<1024xf32, #blocked> loc(#loc)
    %c-1_i32 = arith.constant -1 : i32 loc(#loc)
    %c0_i32 = arith.constant 0 : i32 loc(#loc)
    %c1_i32 = arith.constant 1 : i32 loc(#loc)
    %c3_i32 = arith.constant 3 : i32 loc(#loc)
    %c2_i32 = arith.constant 2 : i32 loc(#loc)
    %0 = tt.get_program_id x : i32 loc(#loc)
    %1 = tt.get_num_programs x : i32 loc(#loc)
    %2 = tt.make_range {end = 1024 : i32, start = 0 : i32} : tensor<1024xi32, #blocked> loc(#loc)
    %3 = tt.splat %n_cols : i32 -> tensor<1024xi32, #blocked> loc(#loc)
    %4 = arith.cmpi slt, %2, %3 : tensor<1024xi32, #blocked> loc(#loc)
    %5 = ttg.local_alloc : () -> !ttg.memdesc<3x1024xf32, #shared, #smem, mutable> loc(#loc)
    %6 = arith.cmpi slt, %0, %n_rows : i32 loc(#loc)
    %7 = arith.muli %0, %input_row_stride : i32 loc(#loc)
    %8 = tt.addptr %input_ptr, %7 : !tt.ptr<f32>, i32 loc(#loc)
    %9 = tt.splat %8 : !tt.ptr<f32> -> tensor<1024x!tt.ptr<f32>, #blocked> loc(#loc)
    %10 = tt.addptr %9, %2 : tensor<1024x!tt.ptr<f32>, #blocked>, tensor<1024xi32, #blocked> loc(#loc)
    %11 = ttg.memdesc_index %5[%c0_i32] : !ttg.memdesc<3x1024xf32, #shared, #smem, mutable> -> !ttg.memdesc<1024xf32, #shared, #smem, mutable, 3x1024> loc(#loc)
    %12 = tt.splat %6 : i1 -> tensor<1024xi1, #blocked> loc(#loc)
    %13 = arith.andi %12, %4 : tensor<1024xi1, #blocked> loc(#loc)
    %14 = ttg.async_copy_global_to_local %10, %11 mask %13 other %cst : tensor<1024x!tt.ptr<f32>, #blocked> -> <1024xf32, #shared, #smem, mutable, 3x1024> loc(#loc)
    %15 = ttg.async_commit_group tokens %14 loc(#loc)
    %16 = arith.addi %0, %1 : i32 loc(#loc)
    %17 = arith.cmpi slt, %16, %n_rows : i32 loc(#loc)
    %18 = arith.muli %16, %input_row_stride : i32 loc(#loc)
    %19 = tt.addptr %input_ptr, %18 : !tt.ptr<f32>, i32 loc(#loc)
    %20 = tt.splat %19 : !tt.ptr<f32> -> tensor<1024x!tt.ptr<f32>, #blocked> loc(#loc)
    %21 = tt.addptr %20, %2 : tensor<1024x!tt.ptr<f32>, #blocked>, tensor<1024xi32, #blocked> loc(#loc)
    %22 = ttg.memdesc_index %5[%c1_i32] : !ttg.memdesc<3x1024xf32, #shared, #smem, mutable> -> !ttg.memdesc<1024xf32, #shared, #smem, mutable, 3x1024> loc(#loc)
    %23 = tt.splat %17 : i1 -> tensor<1024xi1, #blocked> loc(#loc)
    %24 = arith.andi %23, %4 : tensor<1024xi1, #blocked> loc(#loc)
    %25 = ttg.async_copy_global_to_local %21, %22 mask %24 other %cst : tensor<1024x!tt.ptr<f32>, #blocked> -> <1024xf32, #shared, #smem, mutable, 3x1024> loc(#loc)
    %26 = ttg.async_commit_group tokens %25 loc(#loc)
    %27 = arith.muli %1, %c2_i32 : i32 loc(#loc)
    %28 = arith.addi %0, %27 : i32 loc(#loc)
    %29 = arith.cmpi slt, %28, %n_rows : i32 loc(#loc)
    %30 = arith.muli %28, %input_row_stride : i32 loc(#loc)
    %31 = tt.addptr %input_ptr, %30 : !tt.ptr<f32>, i32 loc(#loc)
    %32 = tt.splat %31 : !tt.ptr<f32> -> tensor<1024x!tt.ptr<f32>, #blocked> loc(#loc)
    %33 = tt.addptr %32, %2 : tensor<1024x!tt.ptr<f32>, #blocked>, tensor<1024xi32, #blocked> loc(#loc)
    %34 = ttg.memdesc_index %5[%c2_i32] : !ttg.memdesc<3x1024xf32, #shared, #smem, mutable> -> !ttg.memdesc<1024xf32, #shared, #smem, mutable, 3x1024> loc(#loc)
    %35 = tt.splat %29 : i1 -> tensor<1024xi1, #blocked> loc(#loc)
    %36 = arith.andi %35, %4 : tensor<1024xi1, #blocked> loc(#loc)
    %37 = ttg.async_copy_global_to_local %33, %34 mask %36 other %cst : tensor<1024x!tt.ptr<f32>, #blocked> -> <1024xf32, #shared, #smem, mutable, 3x1024> loc(#loc)
    %38 = ttg.async_commit_group tokens %37 loc(#loc)
    %39:5 = scf.for %arg6 = %0 to %n_rows step %1 iter_args(%arg7 = %c2_i32, %arg8 = %c-1_i32, %arg9 = %15, %arg10 = %26, %arg11 = %38) -> (i32, i32, !ttg.async.token, !ttg.async.token, !ttg.async.token)  : i32 {
      %41 = arith.muli %1, %c3_i32 : i32 loc(#loc)
      %42 = arith.subi %n_rows, %41 : i32 loc(#loc)
      %43 = arith.cmpi slt, %arg6, %42 : i32 loc(#loc)
      %44 = arith.addi %arg8, %c1_i32 : i32 loc(#loc)
      %45 = arith.cmpi sge, %44, %c3_i32 : i32 loc(#loc)
      %46 = arith.select %45, %c0_i32, %44 : i32 loc(#loc)
      %47 = ttg.async_wait %arg9 {num = 2 : i32} loc(#loc)
      %48 = ttg.memdesc_index %5[%46] : !ttg.memdesc<3x1024xf32, #shared, #smem, mutable> -> !ttg.memdesc<1024xf32, #shared, #smem, mutable, 3x1024> loc(#loc)
      %49 = ttg.local_load %48 token %47 : !ttg.memdesc<1024xf32, #shared, #smem, mutable, 3x1024> -> tensor<1024xf32, #blocked> loc(#loc)
      %50 = arith.select %4, %49, %cst : tensor<1024xi1, #blocked>, tensor<1024xf32, #blocked> loc(#loc)
      %51 = "tt.reduce"(%50) <{axis = 0 : i32}> ({
      ^bb0(%arg12: f32 loc(unknown), %arg13: f32 loc(unknown)):
        %75 = arith.maxnumf %arg12, %arg13 : f32 loc(#loc)
        tt.reduce.return %75 : f32 loc(#loc)
      }) : (tensor<1024xf32, #blocked>) -> f32 loc(#loc)
      %52 = tt.splat %51 : f32 -> tensor<1024xf32, #blocked> loc(#loc)
      %53 = arith.subf %50, %52 : tensor<1024xf32, #blocked> loc(#loc)
      %54 = math.exp %53 : tensor<1024xf32, #blocked> loc(#loc)
      %55 = "tt.reduce"(%54) <{axis = 0 : i32}> ({
      ^bb0(%arg12: f32 loc(unknown), %arg13: f32 loc(unknown)):
        %75 = arith.addf %arg12, %arg13 : f32 loc(#loc)
        tt.reduce.return %75 : f32 loc(#loc)
      }) : (tensor<1024xf32, #blocked>) -> f32 loc(#loc)
      %56 = tt.splat %55 : f32 -> tensor<1024xf32, #blocked> loc(#loc)
      %57 = arith.divf %54, %56 : tensor<1024xf32, #blocked> loc(#loc)
      %58 = arith.muli %arg6, %output_row_stride : i32 loc(#loc)
      %59 = tt.addptr %output_ptr, %58 : !tt.ptr<f32>, i32 loc(#loc)
      %60 = tt.splat %59 : !tt.ptr<f32> -> tensor<1024x!tt.ptr<f32>, #blocked> loc(#loc)
      %61 = tt.addptr %60, %2 : tensor<1024x!tt.ptr<f32>, #blocked>, tensor<1024xi32, #blocked> loc(#loc)
      tt.store %61, %57, %4 : tensor<1024x!tt.ptr<f32>, #blocked> loc(#loc)
      %62 = arith.addi %arg7, %c1_i32 : i32 loc(#loc)
      %63 = arith.cmpi sge, %62, %c3_i32 : i32 loc(#loc)
      %64 = arith.select %63, %c0_i32, %62 : i32 loc(#loc)
      %65 = arith.addi %arg6, %41 : i32 loc(#loc)
      %66 = arith.muli %65, %input_row_stride : i32 loc(#loc)
      %67 = tt.addptr %input_ptr, %66 : !tt.ptr<f32>, i32 loc(#loc)
      %68 = tt.splat %67 : !tt.ptr<f32> -> tensor<1024x!tt.ptr<f32>, #blocked> loc(#loc)
      %69 = tt.addptr %68, %2 : tensor<1024x!tt.ptr<f32>, #blocked>, tensor<1024xi32, #blocked> loc(#loc)
      %70 = ttg.memdesc_index %5[%64] : !ttg.memdesc<3x1024xf32, #shared, #smem, mutable> -> !ttg.memdesc<1024xf32, #shared, #smem, mutable, 3x1024> loc(#loc)
      %71 = tt.splat %43 : i1 -> tensor<1024xi1, #blocked> loc(#loc)
      %72 = arith.andi %71, %4 : tensor<1024xi1, #blocked> loc(#loc)
      %73 = ttg.async_copy_global_to_local %69, %70 mask %72 other %cst : tensor<1024x!tt.ptr<f32>, #blocked> -> <1024xf32, #shared, #smem, mutable, 3x1024> loc(#loc)
      %74 = ttg.async_commit_group tokens %73 loc(#loc)
      scf.yield %64, %46, %arg10, %arg11, %74 : i32, i32, !ttg.async.token, !ttg.async.token, !ttg.async.token loc(#loc)
    } {tt.num_stages = 4 : i32} loc(#loc)
    %40 = ttg.async_wait {num = 0 : i32} loc(#loc)
    ttg.local_dealloc %5 : !ttg.memdesc<3x1024xf32, #shared, #smem, mutable> loc(#loc)
    tt.return loc(#loc)
  } loc(#loc)
} loc(#loc)
