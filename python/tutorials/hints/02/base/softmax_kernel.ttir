#loc = loc(unknown)
#loc1 = loc("output_ptr")
#loc2 = loc("input_ptr")
#loc3 = loc("input_row_stride")
#loc4 = loc("output_row_stride")
#loc5 = loc("n_rows")
#loc6 = loc("n_cols")
module {
  tt.func public @softmax_kernel(%output_ptr: !tt.ptr<f32> {tt.divisibility = 16 : i32} loc("output_ptr"), %input_ptr: !tt.ptr<f32> {tt.divisibility = 16 : i32} loc("input_ptr"), %input_row_stride: i32 loc("input_row_stride"), %output_row_stride: i32 loc("output_row_stride"), %n_rows: i32 loc("n_rows"), %n_cols: i32 loc("n_cols")) attributes {noinline = false} {
    %cst = arith.constant dense<0xFF800000> : tensor<1024xf32> loc(#loc)
    %0 = tt.get_program_id x : i32 loc(#loc)
    %1 = tt.get_num_programs x : i32 loc(#loc)
    scf.for %row_idx = %0 to %n_rows step %1  : i32 {
      %2 = arith.muli %row_idx, %input_row_stride : i32 loc(#loc)
      %3 = tt.addptr %input_ptr, %2 : !tt.ptr<f32>, i32 loc(#loc)
      %4 = tt.make_range {end = 1024 : i32, start = 0 : i32} : tensor<1024xi32> loc(#loc)
      %5 = tt.splat %3 : !tt.ptr<f32> -> tensor<1024x!tt.ptr<f32>> loc(#loc)
      %6 = tt.addptr %5, %4 : tensor<1024x!tt.ptr<f32>>, tensor<1024xi32> loc(#loc)
      %7 = tt.splat %n_cols : i32 -> tensor<1024xi32> loc(#loc)
      %8 = arith.cmpi slt, %4, %7 : tensor<1024xi32> loc(#loc)
      %9 = tt.load %6, %8, %cst : tensor<1024x!tt.ptr<f32>> loc(#loc)
      %10 = "tt.reduce"(%9) <{axis = 0 : i32}> ({
      ^bb0(%arg7: f32 loc(unknown), %arg8: f32 loc(unknown)):
        %21 = arith.maxnumf %arg7, %arg8 : f32 loc(#loc)
        tt.reduce.return %21 : f32 loc(#loc)
      }) : (tensor<1024xf32>) -> f32 loc(#loc)
      %11 = tt.splat %10 : f32 -> tensor<1024xf32> loc(#loc)
      %12 = arith.subf %9, %11 : tensor<1024xf32> loc(#loc)
      %13 = math.exp %12 : tensor<1024xf32> loc(#loc)
      %14 = "tt.reduce"(%13) <{axis = 0 : i32}> ({
      ^bb0(%arg7: f32 loc(unknown), %arg8: f32 loc(unknown)):
        %21 = arith.addf %arg7, %arg8 : f32 loc(#loc)
        tt.reduce.return %21 : f32 loc(#loc)
      }) : (tensor<1024xf32>) -> f32 loc(#loc)
      %15 = tt.splat %14 : f32 -> tensor<1024xf32> loc(#loc)
      %16 = arith.divf %13, %15 : tensor<1024xf32> loc(#loc)
      %17 = arith.muli %row_idx, %output_row_stride : i32 loc(#loc)
      %18 = tt.addptr %output_ptr, %17 : !tt.ptr<f32>, i32 loc(#loc)
      %19 = tt.splat %18 : !tt.ptr<f32> -> tensor<1024x!tt.ptr<f32>> loc(#loc)
      %20 = tt.addptr %19, %4 : tensor<1024x!tt.ptr<f32>>, tensor<1024xi32> loc(#loc)
      tt.store %20, %16, %8 : tensor<1024x!tt.ptr<f32>> loc(#loc)
    } {tt.num_stages = 4 : i32} loc(#loc)
    tt.return loc(#loc)
  } loc(#loc)
} loc(#loc)
