#blocked = #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [8], order = [0]}>
#loc = loc(unknown)
#loc1 = loc("output_ptr")
#loc2 = loc("input_ptr")
#loc3 = loc("input_row_stride")
#loc4 = loc("output_row_stride")
#loc5 = loc("n_rows")
#loc6 = loc("n_cols")
#shared = #ttg.swizzled_shared<{vec = 1, perPhase = 1, maxPhase = 1, order = [0]}>
#smem = #ttg.shared_memory
module attributes {"ttg.num-ctas" = 1 : i32, "ttg.num-warps" = 8 : i32, ttg.target = "cuda:90", "ttg.threads-per-warp" = 32 : i32} {
  tt.func public @softmax_kernel(%output_ptr: !tt.ptr<f32> {tt.divisibility = 16 : i32} loc("output_ptr"), %input_ptr: !tt.ptr<f32> {tt.divisibility = 16 : i32} loc("input_ptr"), %input_row_stride: i32 loc("input_row_stride"), %output_row_stride: i32 loc("output_row_stride"), %n_rows: i32 loc("n_rows"), %n_cols: i32 loc("n_cols")) attributes {noinline = false} {
    %cst = arith.constant dense<0xFF800000> : tensor<1024xf32, #blocked> loc(#loc)
    %c0_i32 = arith.constant 0 : i32 loc(#loc)
    %0 = tt.get_program_id x : i32 loc(#loc)
    %1 = tt.get_num_programs x : i32 loc(#loc)
    %2 = tt.make_range {end = 1024 : i32, start = 0 : i32} : tensor<1024xi32, #blocked> loc(#loc)
    %3 = tt.splat %n_cols : i32 -> tensor<1024xi32, #blocked> loc(#loc)
    %4 = arith.cmpi slt, %2, %3 : tensor<1024xi32, #blocked> loc(#loc)
    scf.for %row_idx = %0 to %n_rows step %1  : i32 {
      %5 = arith.muli %row_idx, %input_row_stride : i32 loc(#loc)
      %6 = tt.addptr %input_ptr, %5 : !tt.ptr<f32>, i32 loc(#loc)
      %7 = tt.splat %6 : !tt.ptr<f32> -> tensor<1024x!tt.ptr<f32>, #blocked> loc(#loc)
      %8 = tt.addptr %7, %2 : tensor<1024x!tt.ptr<f32>, #blocked>, tensor<1024xi32, #blocked> loc(#loc)
      %9 = ttg.local_alloc : () -> !ttg.memdesc<1x1024xf32, #shared, #smem, mutable> loc(#loc)
      %10 = ttg.memdesc_index %9[%c0_i32] : !ttg.memdesc<1x1024xf32, #shared, #smem, mutable> -> !ttg.memdesc<1024xf32, #shared, #smem, mutable, 1x1024> loc(#loc)
      %11 = ttg.async_copy_global_to_local %8, %10 mask %4 other %cst : tensor<1024x!tt.ptr<f32>, #blocked> -> <1024xf32, #shared, #smem, mutable, 1x1024> loc(#loc)
      %12 = ttg.async_commit_group tokens %11 loc(#loc)
      %13 = ttg.async_wait %12 {num = 0 : i32} loc(#loc)
      %14 = ttg.local_load %10 token %13 : !ttg.memdesc<1024xf32, #shared, #smem, mutable, 1x1024> -> tensor<1024xf32, #blocked> loc(#loc)
      %15 = "tt.reduce"(%14) <{axis = 0 : i32}> ({
      ^bb0(%arg7: f32 loc(unknown), %arg8: f32 loc(unknown)):
        %26 = arith.maxnumf %arg7, %arg8 : f32 loc(#loc)
        tt.reduce.return %26 : f32 loc(#loc)
      }) : (tensor<1024xf32, #blocked>) -> f32 loc(#loc)
      %16 = tt.splat %15 : f32 -> tensor<1024xf32, #blocked> loc(#loc)
      %17 = arith.subf %14, %16 : tensor<1024xf32, #blocked> loc(#loc)
      %18 = math.exp %17 : tensor<1024xf32, #blocked> loc(#loc)
      %19 = "tt.reduce"(%18) <{axis = 0 : i32}> ({
      ^bb0(%arg7: f32 loc(unknown), %arg8: f32 loc(unknown)):
        %26 = arith.addf %arg7, %arg8 : f32 loc(#loc)
        tt.reduce.return %26 : f32 loc(#loc)
      }) : (tensor<1024xf32, #blocked>) -> f32 loc(#loc)
      %20 = tt.splat %19 : f32 -> tensor<1024xf32, #blocked> loc(#loc)
      %21 = arith.divf %18, %20 : tensor<1024xf32, #blocked> loc(#loc)
      %22 = arith.muli %row_idx, %output_row_stride : i32 loc(#loc)
      %23 = tt.addptr %output_ptr, %22 : !tt.ptr<f32>, i32 loc(#loc)
      %24 = tt.splat %23 : !tt.ptr<f32> -> tensor<1024x!tt.ptr<f32>, #blocked> loc(#loc)
      %25 = tt.addptr %24, %2 : tensor<1024x!tt.ptr<f32>, #blocked>, tensor<1024xi32, #blocked> loc(#loc)
      tt.store %25, %21, %4 : tensor<1024x!tt.ptr<f32>, #blocked> loc(#loc)
    } {tt.num_stages = 4 : i32} loc(#loc)
    tt.return loc(#loc)
  } loc(#loc)
} loc(#loc)
