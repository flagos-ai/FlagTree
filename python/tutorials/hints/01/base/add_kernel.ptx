//
// Generated by LLVM NVPTX Back-End
//

.version 8.7
.target sm_90a
.address_size 64

	// .globl	add_kernel              // -- Begin function add_kernel
                                        // @add_kernel
.visible .entry add_kernel(
	.param .u64 .ptr .global .align 1 add_kernel_param_0,
	.param .u64 .ptr .global .align 1 add_kernel_param_1,
	.param .u64 .ptr .global .align 1 add_kernel_param_2,
	.param .u32 add_kernel_param_3,
	.param .u64 .ptr .global .align 1 add_kernel_param_4,
	.param .u64 .ptr .global .align 1 add_kernel_param_5
)
.reqntid 128
{
	.reg .pred 	%p<7>;
	.reg .b32 	%r<33>;
	.reg .b64 	%rd<11>;

// %bb.0:
	ld.param.b64 	%rd7, [add_kernel_param_0];
	ld.param.b64 	%rd8, [add_kernel_param_1];
	mov.u32 	%r25, %ctaid.x;
	shl.b32 	%r26, %r25, 10;
	ld.param.b64 	%rd9, [add_kernel_param_2];
	ld.param.b32 	%r27, [add_kernel_param_3];
	mov.u32 	%r28, %tid.x;
	shl.b32 	%r29, %r28, 2;
	and.b32 	%r30, %r29, 508;
	or.b32 	%r31, %r30, %r26;
	or.b32 	%r32, %r31, 512;
	setp.lt.s32 	%p1, %r31, %r27;
	setp.lt.s32 	%p2, %r32, %r27;
	mul.wide.s32 	%rd10, %r31, 4;
	add.s64 	%rd1, %rd7, %rd10;
	add.s64 	%rd2, %rd1, 2048;
	// begin inline asm
	mov.u32 %r1, 0x0;
	mov.u32 %r2, 0x0;
	mov.u32 %r3, 0x0;
	mov.u32 %r4, 0x0;
	@%p1 ld.global.v4.b32 { %r1, %r2, %r3, %r4 }, [ %rd1 + 0 ];
	// end inline asm
	// begin inline asm
	mov.u32 %r5, 0x0;
	mov.u32 %r6, 0x0;
	mov.u32 %r7, 0x0;
	mov.u32 %r8, 0x0;
	@%p2 ld.global.v4.b32 { %r5, %r6, %r7, %r8 }, [ %rd2 + 0 ];
	// end inline asm
	add.s64 	%rd3, %rd8, %rd10;
	add.s64 	%rd4, %rd3, 2048;
	// begin inline asm
	mov.u32 %r9, 0x0;
	mov.u32 %r10, 0x0;
	mov.u32 %r11, 0x0;
	mov.u32 %r12, 0x0;
	@%p1 ld.global.v4.b32 { %r9, %r10, %r11, %r12 }, [ %rd3 + 0 ];
	// end inline asm
	// begin inline asm
	mov.u32 %r13, 0x0;
	mov.u32 %r14, 0x0;
	mov.u32 %r15, 0x0;
	mov.u32 %r16, 0x0;
	@%p2 ld.global.v4.b32 { %r13, %r14, %r15, %r16 }, [ %rd4 + 0 ];
	// end inline asm
	add.f32 	%r17, %r1, %r9;
	add.f32 	%r18, %r2, %r10;
	add.f32 	%r19, %r3, %r11;
	add.f32 	%r20, %r4, %r12;
	add.f32 	%r21, %r5, %r13;
	add.f32 	%r22, %r6, %r14;
	add.f32 	%r23, %r7, %r15;
	add.f32 	%r24, %r8, %r16;
	add.s64 	%rd5, %rd9, %rd10;
	add.s64 	%rd6, %rd5, 2048;
	// begin inline asm
	@%p1 st.global.v4.b32 [ %rd5 + 0 ], { %r17, %r18, %r19, %r20 };
	// end inline asm
	// begin inline asm
	@%p2 st.global.v4.b32 [ %rd6 + 0 ], { %r21, %r22, %r23, %r24 };
	// end inline asm
	ret;
                                        // -- End function
}
