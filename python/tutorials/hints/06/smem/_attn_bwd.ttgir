#blocked = #ttg.blocked<{sizePerThread = [1, 8], threadsPerWarp = [4, 8], warpsPerCTA = [4, 1], order = [1, 0]}>
#blocked1 = #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>
#blocked2 = #ttg.blocked<{sizePerThread = [8, 1], threadsPerWarp = [8, 4], warpsPerCTA = [1, 4], order = [0, 1]}>
#blocked3 = #ttg.blocked<{sizePerThread = [4, 4], threadsPerWarp = [8, 4], warpsPerCTA = [4, 1], order = [1, 0]}>
#blocked4 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>
#blocked5 = #ttg.blocked<{sizePerThread = [4, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>
#blocked6 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>
#blocked7 = #ttg.blocked<{sizePerThread = [4, 4], threadsPerWarp = [4, 8], warpsPerCTA = [4, 1], order = [1, 0]}>
#blocked8 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [4, 1], order = [1, 0]}>
#loc1 = loc("Q")
#loc2 = loc("K")
#loc3 = loc("V")
#loc4 = loc("sm_scale")
#loc5 = loc("DO")
#loc6 = loc("DQ")
#loc7 = loc("DK")
#loc8 = loc("DV")
#loc9 = loc("M")
#loc10 = loc("D")
#loc11 = loc("stride_z")
#loc12 = loc("stride_h")
#loc13 = loc("stride_tok")
#loc14 = loc("H")
#loc15 = loc("N_CTX")
#mma = #ttg.nvidia_mma<{versionMajor = 3, versionMinor = 0, warpsPerCTA = [4, 1], instrShape = [16, 64, 16]}>
#mma1 = #ttg.nvidia_mma<{versionMajor = 3, versionMinor = 0, warpsPerCTA = [4, 1], instrShape = [16, 16, 16]}>
#mma2 = #ttg.nvidia_mma<{versionMajor = 3, versionMinor = 0, warpsPerCTA = [4, 1], instrShape = [16, 32, 16]}>
#shared = #ttg.swizzled_shared<{vec = 1, perPhase = 1, maxPhase = 1, order = [1, 0]}>
#shared1 = #ttg.swizzled_shared<{vec = 1, perPhase = 1, maxPhase = 1, order = [0, 1]}>
#shared2 = #ttg.nvmma_shared<{swizzlingByteWidth = 32, transposed = false, elementBitWidth = 16}>
#shared3 = #ttg.nvmma_shared<{swizzlingByteWidth = 32, transposed = true, elementBitWidth = 16}>
#shared4 = #ttg.swizzled_shared<{vec = 1, perPhase = 1, maxPhase = 1, order = [0]}>
#shared5 = #ttg.nvmma_shared<{swizzlingByteWidth = 128, transposed = false, elementBitWidth = 16}>
#shared6 = #ttg.nvmma_shared<{swizzlingByteWidth = 128, transposed = true, elementBitWidth = 16}>
#shared7 = #ttg.nvmma_shared<{swizzlingByteWidth = 64, transposed = false, elementBitWidth = 16}>
#shared8 = #ttg.nvmma_shared<{swizzlingByteWidth = 64, transposed = true, elementBitWidth = 16}>
#smem = #ttg.shared_memory
module attributes {"ttg.num-ctas" = 1 : i32, "ttg.num-warps" = 4 : i32, ttg.target = "cuda:90", "ttg.threads-per-warp" = 32 : i32} {
  tt.func public @_attn_bwd(%Q: !tt.ptr<f16> {tt.divisibility = 16 : i32} loc("Q"), %K: !tt.ptr<f16> {tt.divisibility = 16 : i32} loc("K"), %V: !tt.ptr<f16> {tt.divisibility = 16 : i32} loc("V"), %sm_scale: f32 loc("sm_scale"), %DO: !tt.ptr<f16> {tt.divisibility = 16 : i32} loc("DO"), %DQ: !tt.ptr<f16> {tt.divisibility = 16 : i32} loc("DQ"), %DK: !tt.ptr<f16> {tt.divisibility = 16 : i32} loc("DK"), %DV: !tt.ptr<f16> {tt.divisibility = 16 : i32} loc("DV"), %M: !tt.ptr<f32> {tt.divisibility = 16 : i32} loc("M"), %D: !tt.ptr<f32> {tt.divisibility = 16 : i32} loc("D"), %stride_z: i32 {tt.divisibility = 16 : i32} loc("stride_z"), %stride_h: i32 {tt.divisibility = 16 : i32} loc("stride_h"), %stride_tok: i32 {tt.divisibility = 16 : i32} loc("stride_tok"), %H: i32 {tt.divisibility = 16 : i32} loc("H"), %N_CTX: i32 {tt.divisibility = 16 : i32} loc("N_CTX")) attributes {noinline = false} {
    %cst = arith.constant dense<0.693147182> : tensor<128x64xf32, #mma> loc(#loc)
    %cst_0 = arith.constant dense<0.000000e+00> : tensor<128x64xf32, #mma> loc(#loc)
    %cst_1 = arith.constant dense<0.000000e+00> : tensor<128x16xf32, #mma1> loc(#loc)
    %c16_i32 = arith.constant 16 : i32 loc(#loc)
    %c0_i32 = arith.constant 0 : i32 loc(#loc)
    %c1_i32 = arith.constant 1 : i32 loc(#loc)
    %c32_i32 = arith.constant 32 : i32 loc(#loc)
    %c8_i32 = arith.constant 8 : i32 loc(#loc)
    %c128_i32 = arith.constant 128 : i32 loc(#loc)
    %cst_2 = arith.constant dense<0.000000e+00> : tensor<128x32xf32, #mma2> loc(#loc)
    %0 = tt.get_program_id z : i32 loc(#loc)
    %1 = arith.muli %0, %N_CTX : i32 loc(#loc)
    %2 = arith.extsi %1 : i32 to i64 loc(#loc)
    %3 = arith.remsi %0, %H : i32 loc(#loc)
    %4 = arith.muli %stride_h, %3 : i32 loc(#loc)
    %5 = arith.divsi %0, %H : i32 loc(#loc)
    %6 = arith.muli %stride_z, %5 : i32 loc(#loc)
    %7 = arith.addi %4, %6 : i32 loc(#loc)
    %8 = arith.extsi %7 : i32 to i64 loc(#loc)
    %9 = tt.get_program_id x : i32 loc(#loc)
    %10 = tt.addptr %Q, %8 : !tt.ptr<f16>, i64 loc(#loc)
    %11 = tt.addptr %K, %8 : !tt.ptr<f16>, i64 loc(#loc)
    %12 = tt.addptr %V, %8 : !tt.ptr<f16>, i64 loc(#loc)
    %13 = tt.addptr %DO, %8 : !tt.ptr<f16>, i64 loc(#loc)
    %14 = tt.addptr %DQ, %8 : !tt.ptr<f16>, i64 loc(#loc)
    %15 = tt.addptr %DK, %8 : !tt.ptr<f16>, i64 loc(#loc)
    %16 = tt.addptr %DV, %8 : !tt.ptr<f16>, i64 loc(#loc)
    %17 = tt.addptr %M, %2 : !tt.ptr<f32>, i64 loc(#loc)
    %18 = tt.addptr %D, %2 : !tt.ptr<f32>, i64 loc(#loc)
    %19 = arith.muli %9, %c128_i32 : i32 loc(#loc)
    %20 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32, #ttg.slice<{dim = 1, parent = #blocked}>> loc(#loc)
    %21 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32, #ttg.slice<{dim = 1, parent = #mma1}>> loc(#loc)
    %22 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32, #blocked1> loc(#loc)
    %23 = tt.splat %19 : i32 -> tensor<128xi32, #ttg.slice<{dim = 1, parent = #blocked}>> loc(#loc)
    %24 = tt.splat %19 : i32 -> tensor<128xi32, #ttg.slice<{dim = 1, parent = #mma1}>> loc(#loc)
    %25 = tt.splat %19 : i32 -> tensor<128xi32, #blocked1> loc(#loc)
    %26 = arith.addi %23, %20 : tensor<128xi32, #ttg.slice<{dim = 1, parent = #blocked}>> loc(#loc)
    %27 = arith.addi %24, %21 : tensor<128xi32, #ttg.slice<{dim = 1, parent = #mma1}>> loc(#loc)
    %28 = arith.addi %25, %22 : tensor<128xi32, #blocked1> loc(#loc)
    %29 = tt.expand_dims %26 {axis = 1 : i32} : tensor<128xi32, #ttg.slice<{dim = 1, parent = #blocked}>> -> tensor<128x1xi32, #blocked> loc(#loc)
    %30 = tt.expand_dims %27 {axis = 1 : i32} : tensor<128xi32, #ttg.slice<{dim = 1, parent = #mma1}>> -> tensor<128x1xi32, #mma1> loc(#loc)
    %31 = tt.splat %stride_tok : i32 -> tensor<128x1xi32, #blocked> loc(#loc)
    %32 = arith.muli %29, %31 : tensor<128x1xi32, #blocked> loc(#loc)
    %33 = tt.splat %11 : !tt.ptr<f16> -> tensor<128x1x!tt.ptr<f16>, #blocked> loc(#loc)
    %34 = tt.addptr %33, %32 : tensor<128x1x!tt.ptr<f16>, #blocked>, tensor<128x1xi32, #blocked> loc(#loc)
    %35 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32, #ttg.slice<{dim = 0, parent = #blocked}>> loc(#loc)
    %36 = tt.expand_dims %35 {axis = 0 : i32} : tensor<64xi32, #ttg.slice<{dim = 0, parent = #blocked}>> -> tensor<1x64xi32, #blocked> loc(#loc)
    %37 = tt.broadcast %34 : tensor<128x1x!tt.ptr<f16>, #blocked> -> tensor<128x64x!tt.ptr<f16>, #blocked> loc(#loc)
    %38 = tt.broadcast %36 : tensor<1x64xi32, #blocked> -> tensor<128x64xi32, #blocked> loc(#loc)
    %39 = tt.addptr %37, %38 : tensor<128x64x!tt.ptr<f16>, #blocked>, tensor<128x64xi32, #blocked> loc(#loc)
    %40 = ttg.local_alloc : () -> !ttg.memdesc<1x128x64xf16, #shared, #smem, mutable> loc(#loc)
    %41 = ttg.memdesc_index %40[%c0_i32] : !ttg.memdesc<1x128x64xf16, #shared, #smem, mutable> -> !ttg.memdesc<128x64xf16, #shared, #smem, mutable, 1x128x64> loc(#loc)
    %42 = ttg.async_copy_global_to_local %39, %41 : tensor<128x64x!tt.ptr<f16>, #blocked> -> <128x64xf16, #shared, #smem, mutable, 1x128x64> loc(#loc)
    %43 = ttg.async_commit_group tokens %42 loc(#loc)
    %44 = ttg.async_wait %43 {num = 0 : i32} loc(#loc)
    %45 = tt.splat %12 : !tt.ptr<f16> -> tensor<128x1x!tt.ptr<f16>, #blocked> loc(#loc)
    %46 = tt.addptr %45, %32 : tensor<128x1x!tt.ptr<f16>, #blocked>, tensor<128x1xi32, #blocked> loc(#loc)
    %47 = tt.broadcast %46 : tensor<128x1x!tt.ptr<f16>, #blocked> -> tensor<128x64x!tt.ptr<f16>, #blocked> loc(#loc)
    %48 = tt.addptr %47, %38 : tensor<128x64x!tt.ptr<f16>, #blocked>, tensor<128x64xi32, #blocked> loc(#loc)
    %49 = ttg.local_alloc : () -> !ttg.memdesc<1x128x64xf16, #shared, #smem, mutable> loc(#loc)
    %50 = ttg.memdesc_index %49[%c0_i32] : !ttg.memdesc<1x128x64xf16, #shared, #smem, mutable> -> !ttg.memdesc<128x64xf16, #shared, #smem, mutable, 1x128x64> loc(#loc)
    %51 = ttg.async_copy_global_to_local %48, %50 : tensor<128x64x!tt.ptr<f16>, #blocked> -> <128x64xf16, #shared, #smem, mutable, 1x128x64> loc(#loc)
    %52 = ttg.async_commit_group tokens %51 loc(#loc)
    %53 = ttg.async_wait %52 {num = 0 : i32} loc(#loc)
    %54 = tt.make_range {end = 16 : i32, start = 0 : i32} : tensor<16xi32, #ttg.slice<{dim = 0, parent = #blocked2}>> loc(#loc)
    %55 = tt.make_range {end = 16 : i32, start = 0 : i32} : tensor<16xi32, #ttg.slice<{dim = 0, parent = #mma1}>> loc(#loc)
    %56 = tt.make_range {end = 16 : i32, start = 0 : i32} : tensor<16xi32, #ttg.slice<{dim = 1, parent = #blocked}>> loc(#loc)
    %57 = tt.make_range {end = 16 : i32, start = 0 : i32} : tensor<16xi32, #blocked1> loc(#loc)
    %58 = tt.splat %19 : i32 -> tensor<16xi32, #ttg.slice<{dim = 0, parent = #blocked2}>> loc(#loc)
    %59 = tt.splat %19 : i32 -> tensor<16xi32, #ttg.slice<{dim = 1, parent = #blocked}>> loc(#loc)
    %60 = arith.addi %58, %54 : tensor<16xi32, #ttg.slice<{dim = 0, parent = #blocked2}>> loc(#loc)
    %61 = arith.addi %59, %56 : tensor<16xi32, #ttg.slice<{dim = 1, parent = #blocked}>> loc(#loc)
    %62 = tt.expand_dims %60 {axis = 0 : i32} : tensor<16xi32, #ttg.slice<{dim = 0, parent = #blocked2}>> -> tensor<1x16xi32, #blocked2> loc(#loc)
    %63 = tt.splat %stride_tok : i32 -> tensor<1x16xi32, #blocked2> loc(#loc)
    %64 = arith.muli %62, %63 : tensor<1x16xi32, #blocked2> loc(#loc)
    %65 = tt.splat %10 : !tt.ptr<f16> -> tensor<1x16x!tt.ptr<f16>, #blocked2> loc(#loc)
    %66 = tt.addptr %65, %64 : tensor<1x16x!tt.ptr<f16>, #blocked2>, tensor<1x16xi32, #blocked2> loc(#loc)
    %67 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32, #ttg.slice<{dim = 1, parent = #blocked2}>> loc(#loc)
    %68 = tt.expand_dims %67 {axis = 1 : i32} : tensor<64xi32, #ttg.slice<{dim = 1, parent = #blocked2}>> -> tensor<64x1xi32, #blocked2> loc(#loc)
    %69 = tt.broadcast %66 : tensor<1x16x!tt.ptr<f16>, #blocked2> -> tensor<64x16x!tt.ptr<f16>, #blocked2> loc(#loc)
    %70 = tt.broadcast %68 : tensor<64x1xi32, #blocked2> -> tensor<64x16xi32, #blocked2> loc(#loc)
    %71 = tt.addptr %69, %70 : tensor<64x16x!tt.ptr<f16>, #blocked2>, tensor<64x16xi32, #blocked2> loc(#loc)
    %72 = tt.expand_dims %61 {axis = 1 : i32} : tensor<16xi32, #ttg.slice<{dim = 1, parent = #blocked}>> -> tensor<16x1xi32, #blocked> loc(#loc)
    %73 = tt.splat %stride_tok : i32 -> tensor<16x1xi32, #blocked> loc(#loc)
    %74 = arith.muli %72, %73 : tensor<16x1xi32, #blocked> loc(#loc)
    %75 = tt.splat %13 : !tt.ptr<f16> -> tensor<16x1x!tt.ptr<f16>, #blocked> loc(#loc)
    %76 = tt.addptr %75, %74 : tensor<16x1x!tt.ptr<f16>, #blocked>, tensor<16x1xi32, #blocked> loc(#loc)
    %77 = tt.broadcast %76 : tensor<16x1x!tt.ptr<f16>, #blocked> -> tensor<16x64x!tt.ptr<f16>, #blocked> loc(#loc)
    %78 = tt.broadcast %36 : tensor<1x64xi32, #blocked> -> tensor<16x64xi32, #blocked> loc(#loc)
    %79 = tt.addptr %77, %78 : tensor<16x64x!tt.ptr<f16>, #blocked>, tensor<16x64xi32, #blocked> loc(#loc)
    %80 = tt.splat %17 : !tt.ptr<f32> -> tensor<16x!tt.ptr<f32>, #blocked1> loc(#loc)
    %81 = tt.broadcast %30 : tensor<128x1xi32, #mma1> -> tensor<128x16xi32, #mma1> loc(#loc)
    %82 = tt.splat %18 : !tt.ptr<f32> -> tensor<16x!tt.ptr<f32>, #blocked1> loc(#loc)
    %83 = arith.muli %stride_tok, %c16_i32 : i32 loc(#loc)
    %84 = tt.splat %83 : i32 -> tensor<64x16xi32, #blocked2> loc(#loc)
    %85 = tt.splat %83 : i32 -> tensor<16x64xi32, #blocked> loc(#loc)
    %curr_m:5 = scf.for %curr_m_5 = %c0_i32 to %c8_i32 step %c1_i32 iter_args(%arg16 = %cst_0, %arg17 = %cst_0, %arg18 = %19, %arg19 = %71, %arg20 = %79) -> (tensor<128x64xf32, #mma>, tensor<128x64xf32, #mma>, i32, tensor<64x16x!tt.ptr<f16>, #blocked2>, tensor<16x64x!tt.ptr<f16>, #blocked>)  : i32 {
      %205 = ttg.local_alloc : () -> !ttg.memdesc<1x64x16xf16, #shared1, #smem, mutable> loc(#loc)
      %206 = ttg.memdesc_index %205[%c0_i32] : !ttg.memdesc<1x64x16xf16, #shared1, #smem, mutable> -> !ttg.memdesc<64x16xf16, #shared1, #smem, mutable, 1x64x16> loc(#loc)
      %207 = ttg.async_copy_global_to_local %arg19, %206 : tensor<64x16x!tt.ptr<f16>, #blocked2> -> <64x16xf16, #shared1, #smem, mutable, 1x64x16> loc(#loc)
      %208 = ttg.async_commit_group tokens %207 loc(#loc)
      %209 = ttg.async_wait %208 {num = 0 : i32} loc(#loc)
      %210 = ttg.local_load %206 token %209 : !ttg.memdesc<64x16xf16, #shared1, #smem, mutable, 1x64x16> -> tensor<64x16xf16, #ttg.dot_op<{opIdx = 1, parent = #blocked3}>> loc(#loc)
      %211 = ttg.local_alloc %210 : (tensor<64x16xf16, #ttg.dot_op<{opIdx = 1, parent = #blocked3}>>) -> !ttg.memdesc<64x16xf16, #shared2, #smem> loc(#loc)
      %212 = ttg.local_load %206 token %209 : !ttg.memdesc<64x16xf16, #shared1, #smem, mutable, 1x64x16> -> tensor<64x16xf16, #blocked4> loc(#loc)
      %213 = ttg.local_alloc %212 : (tensor<64x16xf16, #blocked4>) -> !ttg.memdesc<64x16xf16, #shared2, #smem> loc(#loc)
      %214 = ttg.memdesc_trans %213 {order = array<i32: 1, 0>} : !ttg.memdesc<64x16xf16, #shared2, #smem> -> !ttg.memdesc<16x64xf16, #shared3, #smem> loc(#loc)
      %215 = tt.splat %arg18 : i32 -> tensor<16xi32, #ttg.slice<{dim = 0, parent = #mma1}>> loc(#loc)
      %216 = tt.splat %arg18 : i32 -> tensor<16xi32, #blocked1> loc(#loc)
      %217 = arith.addi %215, %55 : tensor<16xi32, #ttg.slice<{dim = 0, parent = #mma1}>> loc(#loc)
      %218 = arith.addi %216, %57 : tensor<16xi32, #blocked1> loc(#loc)
      %219 = tt.addptr %80, %218 : tensor<16x!tt.ptr<f32>, #blocked1>, tensor<16xi32, #blocked1> loc(#loc)
      %220 = ttg.local_alloc : () -> !ttg.memdesc<1x16xf32, #shared4, #smem, mutable> loc(#loc)
      %221 = ttg.memdesc_index %220[%c0_i32] : !ttg.memdesc<1x16xf32, #shared4, #smem, mutable> -> !ttg.memdesc<16xf32, #shared4, #smem, mutable, 1x16> loc(#loc)
      %222 = ttg.async_copy_global_to_local %219, %221 : tensor<16x!tt.ptr<f32>, #blocked1> -> <16xf32, #shared4, #smem, mutable, 1x16> loc(#loc)
      %223 = ttg.async_commit_group tokens %222 loc(#loc)
      %224 = ttg.async_wait %223 {num = 0 : i32} loc(#loc)
      %225 = ttg.local_load %221 token %224 : !ttg.memdesc<16xf32, #shared4, #smem, mutable, 1x16> -> tensor<16xf32, #ttg.slice<{dim = 0, parent = #mma1}>> loc(#loc)
      %226 = ttg.local_load %41 token %44 : !ttg.memdesc<128x64xf16, #shared, #smem, mutable, 1x128x64> -> tensor<128x64xf16, #ttg.dot_op<{opIdx = 0, parent = #mma1, kWidth = 2}>> loc(#loc)
      ttng.fence_async_shared {bCluster = false} loc(#loc)
      %227 = ttng.warp_group_dot %226, %211, %cst_1 {inputPrecision = 0 : i32, isAsync = true} : tensor<128x64xf16, #ttg.dot_op<{opIdx = 0, parent = #mma1, kWidth = 2}>> * !ttg.memdesc<64x16xf16, #shared2, #smem> -> tensor<128x16xf32, #mma1> loc(#loc)
      %228:2 = ttng.warp_group_dot_wait %227, %211 {pendings = 0 : i32} : tensor<128x16xf32, #mma1>, !ttg.memdesc<64x16xf16, #shared2, #smem> loc(#loc)
      %229 = tt.expand_dims %225 {axis = 0 : i32} : tensor<16xf32, #ttg.slice<{dim = 0, parent = #mma1}>> -> tensor<1x16xf32, #mma1> loc(#loc)
      %230 = tt.broadcast %229 : tensor<1x16xf32, #mma1> -> tensor<128x16xf32, #mma1> loc(#loc)
      %231 = arith.subf %228#0, %230 : tensor<128x16xf32, #mma1> loc(#loc)
      %232 = math.exp2 %231 : tensor<128x16xf32, #mma1> loc(#loc)
      %233 = tt.expand_dims %217 {axis = 0 : i32} : tensor<16xi32, #ttg.slice<{dim = 0, parent = #mma1}>> -> tensor<1x16xi32, #mma1> loc(#loc)
      %234 = tt.broadcast %233 : tensor<1x16xi32, #mma1> -> tensor<128x16xi32, #mma1> loc(#loc)
      %235 = arith.cmpi sge, %234, %81 : tensor<128x16xi32, #mma1> loc(#loc)
      %236 = arith.select %235, %232, %cst_1 : tensor<128x16xi1, #mma1>, tensor<128x16xf32, #mma1> loc(#loc)
      %237 = ttg.local_alloc : () -> !ttg.memdesc<1x16x64xf16, #shared, #smem, mutable> loc(#loc)
      %238 = ttg.memdesc_index %237[%c0_i32] : !ttg.memdesc<1x16x64xf16, #shared, #smem, mutable> -> !ttg.memdesc<16x64xf16, #shared, #smem, mutable, 1x16x64> loc(#loc)
      %239 = ttg.async_copy_global_to_local %arg20, %238 : tensor<16x64x!tt.ptr<f16>, #blocked> -> <16x64xf16, #shared, #smem, mutable, 1x16x64> loc(#loc)
      %240 = ttg.async_commit_group tokens %239 loc(#loc)
      %241 = ttg.async_wait %240 {num = 0 : i32} loc(#loc)
      %242 = ttg.local_load %238 token %241 : !ttg.memdesc<16x64xf16, #shared, #smem, mutable, 1x16x64> -> tensor<16x64xf16, #ttg.dot_op<{opIdx = 1, parent = #blocked5}>> loc(#loc)
      %243 = ttg.local_alloc %242 : (tensor<16x64xf16, #ttg.dot_op<{opIdx = 1, parent = #blocked5}>>) -> !ttg.memdesc<16x64xf16, #shared5, #smem> loc(#loc)
      %244 = ttg.local_load %238 token %241 : !ttg.memdesc<16x64xf16, #shared, #smem, mutable, 1x16x64> -> tensor<16x64xf16, #blocked6> loc(#loc)
      %245 = ttg.local_alloc %244 : (tensor<16x64xf16, #blocked6>) -> !ttg.memdesc<16x64xf16, #shared5, #smem> loc(#loc)
      %246 = ttg.memdesc_trans %245 {order = array<i32: 1, 0>} : !ttg.memdesc<16x64xf16, #shared5, #smem> -> !ttg.memdesc<64x16xf16, #shared6, #smem> loc(#loc)
      %247 = arith.truncf %236 : tensor<128x16xf32, #mma1> to tensor<128x16xf16, #mma1> loc(#loc)
      %248 = ttg.convert_layout %247 : tensor<128x16xf16, #mma1> -> tensor<128x16xf16, #ttg.dot_op<{opIdx = 0, parent = #mma, kWidth = 2}>> loc(#loc)
      ttng.fence_async_shared {bCluster = false} loc(#loc)
      %249 = ttng.warp_group_dot %248, %243, %arg17 {inputPrecision = 0 : i32, isAsync = true} : tensor<128x16xf16, #ttg.dot_op<{opIdx = 0, parent = #mma, kWidth = 2}>> * !ttg.memdesc<16x64xf16, #shared5, #smem> -> tensor<128x64xf32, #mma> loc(#loc)
      %250:2 = ttng.warp_group_dot_wait %249, %243 {pendings = 0 : i32} : tensor<128x64xf32, #mma>, !ttg.memdesc<16x64xf16, #shared5, #smem> loc(#loc)
      %251 = tt.addptr %82, %218 : tensor<16x!tt.ptr<f32>, #blocked1>, tensor<16xi32, #blocked1> loc(#loc)
      %252 = ttg.local_alloc : () -> !ttg.memdesc<1x16xf32, #shared4, #smem, mutable> loc(#loc)
      %253 = ttg.memdesc_index %252[%c0_i32] : !ttg.memdesc<1x16xf32, #shared4, #smem, mutable> -> !ttg.memdesc<16xf32, #shared4, #smem, mutable, 1x16> loc(#loc)
      %254 = ttg.async_copy_global_to_local %251, %253 : tensor<16x!tt.ptr<f32>, #blocked1> -> <16xf32, #shared4, #smem, mutable, 1x16> loc(#loc)
      %255 = ttg.async_commit_group tokens %254 loc(#loc)
      %256 = ttg.async_wait %255 {num = 0 : i32} loc(#loc)
      %257 = ttg.local_load %253 token %256 : !ttg.memdesc<16xf32, #shared4, #smem, mutable, 1x16> -> tensor<16xf32, #ttg.slice<{dim = 0, parent = #mma1}>> loc(#loc)
      %258 = ttg.local_load %50 token %53 : !ttg.memdesc<128x64xf16, #shared, #smem, mutable, 1x128x64> -> tensor<128x64xf16, #ttg.dot_op<{opIdx = 0, parent = #mma1, kWidth = 2}>> loc(#loc)
      ttng.fence_async_shared {bCluster = false} loc(#loc)
      %259 = ttng.warp_group_dot %258, %246, %cst_1 {inputPrecision = 0 : i32, isAsync = true} : tensor<128x64xf16, #ttg.dot_op<{opIdx = 0, parent = #mma1, kWidth = 2}>> * !ttg.memdesc<64x16xf16, #shared6, #smem> -> tensor<128x16xf32, #mma1> loc(#loc)
      %260:2 = ttng.warp_group_dot_wait %259, %246 {pendings = 0 : i32} : tensor<128x16xf32, #mma1>, !ttg.memdesc<64x16xf16, #shared6, #smem> loc(#loc)
      %261 = tt.expand_dims %257 {axis = 0 : i32} : tensor<16xf32, #ttg.slice<{dim = 0, parent = #mma1}>> -> tensor<1x16xf32, #mma1> loc(#loc)
      %262 = tt.broadcast %261 : tensor<1x16xf32, #mma1> -> tensor<128x16xf32, #mma1> loc(#loc)
      %263 = arith.subf %260#0, %262 : tensor<128x16xf32, #mma1> loc(#loc)
      %264 = arith.mulf %236, %263 : tensor<128x16xf32, #mma1> loc(#loc)
      %265 = arith.truncf %264 : tensor<128x16xf32, #mma1> to tensor<128x16xf16, #mma1> loc(#loc)
      %266 = ttg.convert_layout %265 : tensor<128x16xf16, #mma1> -> tensor<128x16xf16, #ttg.dot_op<{opIdx = 0, parent = #mma, kWidth = 2}>> loc(#loc)
      ttng.fence_async_shared {bCluster = false} loc(#loc)
      %267 = ttng.warp_group_dot %266, %214, %arg16 {inputPrecision = 0 : i32, isAsync = true} : tensor<128x16xf16, #ttg.dot_op<{opIdx = 0, parent = #mma, kWidth = 2}>> * !ttg.memdesc<16x64xf16, #shared3, #smem> -> tensor<128x64xf32, #mma> loc(#loc)
      %268:2 = ttng.warp_group_dot_wait %267, %214 {pendings = 0 : i32} : tensor<128x64xf32, #mma>, !ttg.memdesc<16x64xf16, #shared3, #smem> loc(#loc)
      %269 = arith.addi %arg18, %c16_i32 : i32 loc(#loc)
      %270 = tt.addptr %arg19, %84 : tensor<64x16x!tt.ptr<f16>, #blocked2>, tensor<64x16xi32, #blocked2> loc(#loc)
      %271 = tt.addptr %arg20, %85 : tensor<16x64x!tt.ptr<f16>, #blocked>, tensor<16x64xi32, #blocked> loc(#loc)
      scf.yield %268#0, %250#0, %269, %270, %271 : tensor<128x64xf32, #mma>, tensor<128x64xf32, #mma>, i32, tensor<64x16x!tt.ptr<f16>, #blocked2>, tensor<16x64x!tt.ptr<f16>, #blocked> loc(#loc)
    } loc(#loc29)
    %86 = arith.addi %19, %c128_i32 : i32 loc(#loc)
    %87 = arith.subi %N_CTX, %86 : i32 loc(#loc)
    %88 = arith.divsi %87, %c32_i32 : i32 loc(#loc)
    %89 = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked2}>> loc(#loc)
    %90 = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32, #ttg.slice<{dim = 1, parent = #blocked}>> loc(#loc)
    %91 = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32, #blocked1> loc(#loc)
    %92 = tt.splat %86 : i32 -> tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked2}>> loc(#loc)
    %93 = tt.splat %86 : i32 -> tensor<32xi32, #ttg.slice<{dim = 1, parent = #blocked}>> loc(#loc)
    %94 = arith.addi %92, %89 : tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked2}>> loc(#loc)
    %95 = arith.addi %93, %90 : tensor<32xi32, #ttg.slice<{dim = 1, parent = #blocked}>> loc(#loc)
    %96 = tt.expand_dims %94 {axis = 0 : i32} : tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked2}>> -> tensor<1x32xi32, #blocked2> loc(#loc)
    %97 = tt.splat %stride_tok : i32 -> tensor<1x32xi32, #blocked2> loc(#loc)
    %98 = arith.muli %96, %97 : tensor<1x32xi32, #blocked2> loc(#loc)
    %99 = tt.splat %10 : !tt.ptr<f16> -> tensor<1x32x!tt.ptr<f16>, #blocked2> loc(#loc)
    %100 = tt.addptr %99, %98 : tensor<1x32x!tt.ptr<f16>, #blocked2>, tensor<1x32xi32, #blocked2> loc(#loc)
    %101 = tt.broadcast %100 : tensor<1x32x!tt.ptr<f16>, #blocked2> -> tensor<64x32x!tt.ptr<f16>, #blocked2> loc(#loc)
    %102 = tt.broadcast %68 : tensor<64x1xi32, #blocked2> -> tensor<64x32xi32, #blocked2> loc(#loc)
    %103 = tt.addptr %101, %102 : tensor<64x32x!tt.ptr<f16>, #blocked2>, tensor<64x32xi32, #blocked2> loc(#loc)
    %104 = tt.expand_dims %95 {axis = 1 : i32} : tensor<32xi32, #ttg.slice<{dim = 1, parent = #blocked}>> -> tensor<32x1xi32, #blocked> loc(#loc)
    %105 = tt.splat %stride_tok : i32 -> tensor<32x1xi32, #blocked> loc(#loc)
    %106 = arith.muli %104, %105 : tensor<32x1xi32, #blocked> loc(#loc)
    %107 = tt.splat %13 : !tt.ptr<f16> -> tensor<32x1x!tt.ptr<f16>, #blocked> loc(#loc)
    %108 = tt.addptr %107, %106 : tensor<32x1x!tt.ptr<f16>, #blocked>, tensor<32x1xi32, #blocked> loc(#loc)
    %109 = tt.broadcast %108 : tensor<32x1x!tt.ptr<f16>, #blocked> -> tensor<32x64x!tt.ptr<f16>, #blocked> loc(#loc)
    %110 = tt.broadcast %36 : tensor<1x64xi32, #blocked> -> tensor<32x64xi32, #blocked> loc(#loc)
    %111 = tt.addptr %109, %110 : tensor<32x64x!tt.ptr<f16>, #blocked>, tensor<32x64xi32, #blocked> loc(#loc)
    %112 = tt.splat %17 : !tt.ptr<f32> -> tensor<32x!tt.ptr<f32>, #blocked1> loc(#loc)
    %113 = tt.splat %18 : !tt.ptr<f32> -> tensor<32x!tt.ptr<f32>, #blocked1> loc(#loc)
    %114 = arith.muli %stride_tok, %c32_i32 : i32 loc(#loc)
    %115 = tt.splat %114 : i32 -> tensor<64x32xi32, #blocked2> loc(#loc)
    %116 = tt.splat %114 : i32 -> tensor<32x64xi32, #blocked> loc(#loc)
    %curr_m_3:5 = scf.for %curr_m_5 = %c0_i32 to %88 step %c1_i32 iter_args(%curr_m_6 = %curr_m#0, %curr_m_7 = %curr_m#1, %arg18 = %86, %arg19 = %103, %arg20 = %111) -> (tensor<128x64xf32, #mma>, tensor<128x64xf32, #mma>, i32, tensor<64x32x!tt.ptr<f16>, #blocked2>, tensor<32x64x!tt.ptr<f16>, #blocked>)  : i32 {
      %205 = ttg.local_alloc : () -> !ttg.memdesc<1x64x32xf16, #shared1, #smem, mutable> loc(#loc)
      %206 = ttg.memdesc_index %205[%c0_i32] : !ttg.memdesc<1x64x32xf16, #shared1, #smem, mutable> -> !ttg.memdesc<64x32xf16, #shared1, #smem, mutable, 1x64x32> loc(#loc)
      %207 = ttg.async_copy_global_to_local %arg19, %206 : tensor<64x32x!tt.ptr<f16>, #blocked2> -> <64x32xf16, #shared1, #smem, mutable, 1x64x32> loc(#loc)
      %208 = ttg.async_commit_group tokens %207 loc(#loc)
      %209 = ttg.async_wait %208 {num = 0 : i32} loc(#loc)
      %210 = ttg.local_load %206 token %209 : !ttg.memdesc<64x32xf16, #shared1, #smem, mutable, 1x64x32> -> tensor<64x32xf16, #ttg.dot_op<{opIdx = 1, parent = #blocked7}>> loc(#loc)
      %211 = ttg.local_alloc %210 : (tensor<64x32xf16, #ttg.dot_op<{opIdx = 1, parent = #blocked7}>>) -> !ttg.memdesc<64x32xf16, #shared7, #smem> loc(#loc)
      %212 = ttg.local_load %206 token %209 : !ttg.memdesc<64x32xf16, #shared1, #smem, mutable, 1x64x32> -> tensor<64x32xf16, #blocked8> loc(#loc)
      %213 = ttg.local_alloc %212 : (tensor<64x32xf16, #blocked8>) -> !ttg.memdesc<64x32xf16, #shared7, #smem> loc(#loc)
      %214 = ttg.memdesc_trans %213 {order = array<i32: 1, 0>} : !ttg.memdesc<64x32xf16, #shared7, #smem> -> !ttg.memdesc<32x64xf16, #shared8, #smem> loc(#loc)
      %215 = tt.splat %arg18 : i32 -> tensor<32xi32, #blocked1> loc(#loc)
      %216 = arith.addi %215, %91 : tensor<32xi32, #blocked1> loc(#loc)
      %217 = tt.addptr %112, %216 : tensor<32x!tt.ptr<f32>, #blocked1>, tensor<32xi32, #blocked1> loc(#loc)
      %218 = ttg.local_alloc : () -> !ttg.memdesc<1x32xf32, #shared4, #smem, mutable> loc(#loc)
      %219 = ttg.memdesc_index %218[%c0_i32] : !ttg.memdesc<1x32xf32, #shared4, #smem, mutable> -> !ttg.memdesc<32xf32, #shared4, #smem, mutable, 1x32> loc(#loc)
      %220 = ttg.async_copy_global_to_local %217, %219 : tensor<32x!tt.ptr<f32>, #blocked1> -> <32xf32, #shared4, #smem, mutable, 1x32> loc(#loc)
      %221 = ttg.async_commit_group tokens %220 loc(#loc)
      %222 = ttg.async_wait %221 {num = 0 : i32} loc(#loc)
      %223 = ttg.local_load %219 token %222 : !ttg.memdesc<32xf32, #shared4, #smem, mutable, 1x32> -> tensor<32xf32, #ttg.slice<{dim = 0, parent = #mma2}>> loc(#loc)
      %224 = ttg.local_load %41 token %44 : !ttg.memdesc<128x64xf16, #shared, #smem, mutable, 1x128x64> -> tensor<128x64xf16, #ttg.dot_op<{opIdx = 0, parent = #mma2, kWidth = 2}>> loc(#loc)
      ttng.fence_async_shared {bCluster = false} loc(#loc)
      %225 = ttng.warp_group_dot %224, %211, %cst_2 {inputPrecision = 0 : i32, isAsync = true} : tensor<128x64xf16, #ttg.dot_op<{opIdx = 0, parent = #mma2, kWidth = 2}>> * !ttg.memdesc<64x32xf16, #shared7, #smem> -> tensor<128x32xf32, #mma2> loc(#loc)
      %226:2 = ttng.warp_group_dot_wait %225, %211 {pendings = 0 : i32} : tensor<128x32xf32, #mma2>, !ttg.memdesc<64x32xf16, #shared7, #smem> loc(#loc)
      %227 = tt.expand_dims %223 {axis = 0 : i32} : tensor<32xf32, #ttg.slice<{dim = 0, parent = #mma2}>> -> tensor<1x32xf32, #mma2> loc(#loc)
      %228 = tt.broadcast %227 : tensor<1x32xf32, #mma2> -> tensor<128x32xf32, #mma2> loc(#loc)
      %229 = arith.subf %226#0, %228 : tensor<128x32xf32, #mma2> loc(#loc)
      %230 = math.exp2 %229 : tensor<128x32xf32, #mma2> loc(#loc)
      %231 = ttg.local_alloc : () -> !ttg.memdesc<1x32x64xf16, #shared, #smem, mutable> loc(#loc)
      %232 = ttg.memdesc_index %231[%c0_i32] : !ttg.memdesc<1x32x64xf16, #shared, #smem, mutable> -> !ttg.memdesc<32x64xf16, #shared, #smem, mutable, 1x32x64> loc(#loc)
      %233 = ttg.async_copy_global_to_local %arg20, %232 : tensor<32x64x!tt.ptr<f16>, #blocked> -> <32x64xf16, #shared, #smem, mutable, 1x32x64> loc(#loc)
      %234 = ttg.async_commit_group tokens %233 loc(#loc)
      %235 = ttg.async_wait %234 {num = 0 : i32} loc(#loc)
      %236 = ttg.local_load %232 token %235 : !ttg.memdesc<32x64xf16, #shared, #smem, mutable, 1x32x64> -> tensor<32x64xf16, #ttg.dot_op<{opIdx = 1, parent = #blocked5}>> loc(#loc)
      %237 = ttg.local_alloc %236 : (tensor<32x64xf16, #ttg.dot_op<{opIdx = 1, parent = #blocked5}>>) -> !ttg.memdesc<32x64xf16, #shared5, #smem> loc(#loc)
      %238 = ttg.local_load %232 token %235 : !ttg.memdesc<32x64xf16, #shared, #smem, mutable, 1x32x64> -> tensor<32x64xf16, #blocked6> loc(#loc)
      %239 = ttg.local_alloc %238 : (tensor<32x64xf16, #blocked6>) -> !ttg.memdesc<32x64xf16, #shared5, #smem> loc(#loc)
      %240 = ttg.memdesc_trans %239 {order = array<i32: 1, 0>} : !ttg.memdesc<32x64xf16, #shared5, #smem> -> !ttg.memdesc<64x32xf16, #shared6, #smem> loc(#loc)
      %241 = arith.truncf %230 : tensor<128x32xf32, #mma2> to tensor<128x32xf16, #mma2> loc(#loc)
      %242 = ttg.convert_layout %241 : tensor<128x32xf16, #mma2> -> tensor<128x32xf16, #ttg.dot_op<{opIdx = 0, parent = #mma, kWidth = 2}>> loc(#loc)
      ttng.fence_async_shared {bCluster = false} loc(#loc)
      %243 = ttng.warp_group_dot %242, %237, %curr_m_7 {inputPrecision = 0 : i32, isAsync = true} : tensor<128x32xf16, #ttg.dot_op<{opIdx = 0, parent = #mma, kWidth = 2}>> * !ttg.memdesc<32x64xf16, #shared5, #smem> -> tensor<128x64xf32, #mma> loc(#loc)
      %244:2 = ttng.warp_group_dot_wait %243, %237 {pendings = 0 : i32} : tensor<128x64xf32, #mma>, !ttg.memdesc<32x64xf16, #shared5, #smem> loc(#loc)
      %245 = tt.addptr %113, %216 : tensor<32x!tt.ptr<f32>, #blocked1>, tensor<32xi32, #blocked1> loc(#loc)
      %246 = ttg.local_alloc : () -> !ttg.memdesc<1x32xf32, #shared4, #smem, mutable> loc(#loc)
      %247 = ttg.memdesc_index %246[%c0_i32] : !ttg.memdesc<1x32xf32, #shared4, #smem, mutable> -> !ttg.memdesc<32xf32, #shared4, #smem, mutable, 1x32> loc(#loc)
      %248 = ttg.async_copy_global_to_local %245, %247 : tensor<32x!tt.ptr<f32>, #blocked1> -> <32xf32, #shared4, #smem, mutable, 1x32> loc(#loc)
      %249 = ttg.async_commit_group tokens %248 loc(#loc)
      %250 = ttg.async_wait %249 {num = 0 : i32} loc(#loc)
      %251 = ttg.local_load %247 token %250 : !ttg.memdesc<32xf32, #shared4, #smem, mutable, 1x32> -> tensor<32xf32, #ttg.slice<{dim = 0, parent = #mma2}>> loc(#loc)
      %252 = ttg.local_load %50 token %53 : !ttg.memdesc<128x64xf16, #shared, #smem, mutable, 1x128x64> -> tensor<128x64xf16, #ttg.dot_op<{opIdx = 0, parent = #mma2, kWidth = 2}>> loc(#loc)
      ttng.fence_async_shared {bCluster = false} loc(#loc)
      %253 = ttng.warp_group_dot %252, %240, %cst_2 {inputPrecision = 0 : i32, isAsync = true} : tensor<128x64xf16, #ttg.dot_op<{opIdx = 0, parent = #mma2, kWidth = 2}>> * !ttg.memdesc<64x32xf16, #shared6, #smem> -> tensor<128x32xf32, #mma2> loc(#loc)
      %254:2 = ttng.warp_group_dot_wait %253, %240 {pendings = 0 : i32} : tensor<128x32xf32, #mma2>, !ttg.memdesc<64x32xf16, #shared6, #smem> loc(#loc)
      %255 = tt.expand_dims %251 {axis = 0 : i32} : tensor<32xf32, #ttg.slice<{dim = 0, parent = #mma2}>> -> tensor<1x32xf32, #mma2> loc(#loc)
      %256 = tt.broadcast %255 : tensor<1x32xf32, #mma2> -> tensor<128x32xf32, #mma2> loc(#loc)
      %257 = arith.subf %254#0, %256 : tensor<128x32xf32, #mma2> loc(#loc)
      %258 = arith.mulf %230, %257 : tensor<128x32xf32, #mma2> loc(#loc)
      %259 = arith.truncf %258 : tensor<128x32xf32, #mma2> to tensor<128x32xf16, #mma2> loc(#loc)
      %260 = ttg.convert_layout %259 : tensor<128x32xf16, #mma2> -> tensor<128x32xf16, #ttg.dot_op<{opIdx = 0, parent = #mma, kWidth = 2}>> loc(#loc)
      ttng.fence_async_shared {bCluster = false} loc(#loc)
      %261 = ttng.warp_group_dot %260, %214, %curr_m_6 {inputPrecision = 0 : i32, isAsync = true} : tensor<128x32xf16, #ttg.dot_op<{opIdx = 0, parent = #mma, kWidth = 2}>> * !ttg.memdesc<32x64xf16, #shared8, #smem> -> tensor<128x64xf32, #mma> loc(#loc)
      %262:2 = ttng.warp_group_dot_wait %261, %214 {pendings = 0 : i32} : tensor<128x64xf32, #mma>, !ttg.memdesc<32x64xf16, #shared8, #smem> loc(#loc)
      %263 = arith.addi %arg18, %c32_i32 : i32 loc(#loc)
      %264 = tt.addptr %arg19, %115 : tensor<64x32x!tt.ptr<f16>, #blocked2>, tensor<64x32xi32, #blocked2> loc(#loc)
      %265 = tt.addptr %arg20, %116 : tensor<32x64x!tt.ptr<f16>, #blocked>, tensor<32x64xi32, #blocked> loc(#loc)
      scf.yield %262#0, %244#0, %263, %264, %265 : tensor<128x64xf32, #mma>, tensor<128x64xf32, #mma>, i32, tensor<64x32x!tt.ptr<f16>, #blocked2>, tensor<32x64x!tt.ptr<f16>, #blocked> loc(#loc)
    } loc(#loc29)
    %117 = tt.splat %16 : !tt.ptr<f16> -> tensor<128x1x!tt.ptr<f16>, #blocked> loc(#loc)
    %118 = tt.addptr %117, %32 : tensor<128x1x!tt.ptr<f16>, #blocked>, tensor<128x1xi32, #blocked> loc(#loc)
    %119 = tt.broadcast %118 : tensor<128x1x!tt.ptr<f16>, #blocked> -> tensor<128x64x!tt.ptr<f16>, #blocked> loc(#loc)
    %120 = tt.addptr %119, %38 : tensor<128x64x!tt.ptr<f16>, #blocked>, tensor<128x64xi32, #blocked> loc(#loc)
    %121 = arith.truncf %curr_m_3#1 : tensor<128x64xf32, #mma> to tensor<128x64xf16, #mma> loc(#loc)
    %122 = ttg.convert_layout %121 : tensor<128x64xf16, #mma> -> tensor<128x64xf16, #blocked> loc(#loc)
    tt.store %120, %122 : tensor<128x64x!tt.ptr<f16>, #blocked> loc(#loc)
    %123 = tt.splat %sm_scale : f32 -> tensor<128x64xf32, #mma> loc(#loc)
    %124 = arith.mulf %curr_m_3#0, %123 : tensor<128x64xf32, #mma> loc(#loc)
    %125 = tt.splat %15 : !tt.ptr<f16> -> tensor<128x1x!tt.ptr<f16>, #blocked> loc(#loc)
    %126 = tt.addptr %125, %32 : tensor<128x1x!tt.ptr<f16>, #blocked>, tensor<128x1xi32, #blocked> loc(#loc)
    %127 = tt.broadcast %126 : tensor<128x1x!tt.ptr<f16>, #blocked> -> tensor<128x64x!tt.ptr<f16>, #blocked> loc(#loc)
    %128 = tt.addptr %127, %38 : tensor<128x64x!tt.ptr<f16>, #blocked>, tensor<128x64xi32, #blocked> loc(#loc)
    %129 = arith.truncf %124 : tensor<128x64xf32, #mma> to tensor<128x64xf16, #mma> loc(#loc)
    %130 = ttg.convert_layout %129 : tensor<128x64xf16, #mma> -> tensor<128x64xf16, #blocked> loc(#loc)
    tt.store %128, %130 : tensor<128x64x!tt.ptr<f16>, #blocked> loc(#loc)
    %131 = tt.splat %10 : !tt.ptr<f16> -> tensor<128x1x!tt.ptr<f16>, #blocked> loc(#loc)
    %132 = tt.addptr %131, %32 : tensor<128x1x!tt.ptr<f16>, #blocked>, tensor<128x1xi32, #blocked> loc(#loc)
    %133 = tt.broadcast %132 : tensor<128x1x!tt.ptr<f16>, #blocked> -> tensor<128x64x!tt.ptr<f16>, #blocked> loc(#loc)
    %134 = tt.addptr %133, %38 : tensor<128x64x!tt.ptr<f16>, #blocked>, tensor<128x64xi32, #blocked> loc(#loc)
    %135 = ttg.local_alloc : () -> !ttg.memdesc<1x128x64xf16, #shared, #smem, mutable> loc(#loc)
    %136 = ttg.memdesc_index %135[%c0_i32] : !ttg.memdesc<1x128x64xf16, #shared, #smem, mutable> -> !ttg.memdesc<128x64xf16, #shared, #smem, mutable, 1x128x64> loc(#loc)
    %137 = ttg.async_copy_global_to_local %134, %136 : tensor<128x64x!tt.ptr<f16>, #blocked> -> <128x64xf16, #shared, #smem, mutable, 1x128x64> loc(#loc)
    %138 = ttg.async_commit_group tokens %137 loc(#loc)
    %139 = ttg.async_wait %138 {num = 0 : i32} loc(#loc)
    %140 = tt.splat %13 : !tt.ptr<f16> -> tensor<128x1x!tt.ptr<f16>, #blocked> loc(#loc)
    %141 = tt.addptr %140, %32 : tensor<128x1x!tt.ptr<f16>, #blocked>, tensor<128x1xi32, #blocked> loc(#loc)
    %142 = tt.broadcast %141 : tensor<128x1x!tt.ptr<f16>, #blocked> -> tensor<128x64x!tt.ptr<f16>, #blocked> loc(#loc)
    %143 = tt.addptr %142, %38 : tensor<128x64x!tt.ptr<f16>, #blocked>, tensor<128x64xi32, #blocked> loc(#loc)
    %144 = ttg.local_alloc : () -> !ttg.memdesc<1x128x64xf16, #shared, #smem, mutable> loc(#loc)
    %145 = ttg.memdesc_index %144[%c0_i32] : !ttg.memdesc<1x128x64xf16, #shared, #smem, mutable> -> !ttg.memdesc<128x64xf16, #shared, #smem, mutable, 1x128x64> loc(#loc)
    %146 = ttg.async_copy_global_to_local %143, %145 : tensor<128x64x!tt.ptr<f16>, #blocked> -> <128x64xf16, #shared, #smem, mutable, 1x128x64> loc(#loc)
    %147 = ttg.async_commit_group tokens %146 loc(#loc)
    %148 = ttg.async_wait %147 {num = 0 : i32} loc(#loc)
    %149 = tt.splat %17 : !tt.ptr<f32> -> tensor<128x!tt.ptr<f32>, #blocked1> loc(#loc)
    %150 = tt.addptr %149, %28 : tensor<128x!tt.ptr<f32>, #blocked1>, tensor<128xi32, #blocked1> loc(#loc)
    %151 = ttg.local_alloc : () -> !ttg.memdesc<1x128xf32, #shared4, #smem, mutable> loc(#loc)
    %152 = ttg.memdesc_index %151[%c0_i32] : !ttg.memdesc<1x128xf32, #shared4, #smem, mutable> -> !ttg.memdesc<128xf32, #shared4, #smem, mutable, 1x128> loc(#loc)
    %153 = ttg.async_copy_global_to_local %150, %152 : tensor<128x!tt.ptr<f32>, #blocked1> -> <128xf32, #shared4, #smem, mutable, 1x128> loc(#loc)
    %154 = ttg.async_commit_group tokens %153 loc(#loc)
    %155 = ttg.async_wait %154 {num = 0 : i32} loc(#loc)
    %156 = ttg.local_load %152 token %155 : !ttg.memdesc<128xf32, #shared4, #smem, mutable, 1x128> -> tensor<128xf32, #ttg.slice<{dim = 1, parent = #mma1}>> loc(#loc)
    %157 = ttg.local_load %152 token %155 : !ttg.memdesc<128xf32, #shared4, #smem, mutable, 1x128> -> tensor<128xf32, #ttg.slice<{dim = 1, parent = #mma2}>> loc(#loc)
    %158 = tt.expand_dims %156 {axis = 1 : i32} : tensor<128xf32, #ttg.slice<{dim = 1, parent = #mma1}>> -> tensor<128x1xf32, #mma1> loc(#loc)
    %159 = tt.expand_dims %157 {axis = 1 : i32} : tensor<128xf32, #ttg.slice<{dim = 1, parent = #mma2}>> -> tensor<128x1xf32, #mma2> loc(#loc)
    %160 = tt.splat %11 : !tt.ptr<f16> -> tensor<1x16x!tt.ptr<f16>, #blocked2> loc(#loc)
    %161 = tt.addptr %160, %64 : tensor<1x16x!tt.ptr<f16>, #blocked2>, tensor<1x16xi32, #blocked2> loc(#loc)
    %162 = tt.broadcast %161 : tensor<1x16x!tt.ptr<f16>, #blocked2> -> tensor<64x16x!tt.ptr<f16>, #blocked2> loc(#loc)
    %163 = tt.addptr %162, %70 : tensor<64x16x!tt.ptr<f16>, #blocked2>, tensor<64x16xi32, #blocked2> loc(#loc)
    %164 = tt.splat %12 : !tt.ptr<f16> -> tensor<1x16x!tt.ptr<f16>, #blocked2> loc(#loc)
    %165 = tt.addptr %164, %64 : tensor<1x16x!tt.ptr<f16>, #blocked2>, tensor<1x16xi32, #blocked2> loc(#loc)
    %166 = tt.broadcast %165 : tensor<1x16x!tt.ptr<f16>, #blocked2> -> tensor<64x16x!tt.ptr<f16>, #blocked2> loc(#loc)
    %167 = tt.addptr %166, %70 : tensor<64x16x!tt.ptr<f16>, #blocked2>, tensor<64x16xi32, #blocked2> loc(#loc)
    %168 = tt.splat %18 : !tt.ptr<f32> -> tensor<128x!tt.ptr<f32>, #blocked1> loc(#loc)
    %169 = tt.addptr %168, %28 : tensor<128x!tt.ptr<f32>, #blocked1>, tensor<128xi32, #blocked1> loc(#loc)
    %170 = ttg.local_alloc : () -> !ttg.memdesc<1x128xf32, #shared4, #smem, mutable> loc(#loc)
    %171 = ttg.memdesc_index %170[%c0_i32] : !ttg.memdesc<1x128xf32, #shared4, #smem, mutable> -> !ttg.memdesc<128xf32, #shared4, #smem, mutable, 1x128> loc(#loc)
    %172 = ttg.async_copy_global_to_local %169, %171 : tensor<128x!tt.ptr<f32>, #blocked1> -> <128xf32, #shared4, #smem, mutable, 1x128> loc(#loc)
    %173 = ttg.async_commit_group tokens %172 loc(#loc)
    %174 = ttg.async_wait %173 {num = 0 : i32} loc(#loc)
    %175 = ttg.local_load %171 token %174 : !ttg.memdesc<128xf32, #shared4, #smem, mutable, 1x128> -> tensor<128xf32, #ttg.slice<{dim = 1, parent = #mma2}>> loc(#loc)
    %176 = ttg.local_load %171 token %174 : !ttg.memdesc<128xf32, #shared4, #smem, mutable, 1x128> -> tensor<128xf32, #ttg.slice<{dim = 1, parent = #mma1}>> loc(#loc)
    %177 = tt.broadcast %158 : tensor<128x1xf32, #mma1> -> tensor<128x16xf32, #mma1> loc(#loc)
    %178 = tt.expand_dims %176 {axis = 1 : i32} : tensor<128xf32, #ttg.slice<{dim = 1, parent = #mma1}>> -> tensor<128x1xf32, #mma1> loc(#loc)
    %179 = tt.broadcast %178 : tensor<128x1xf32, #mma1> -> tensor<128x16xf32, #mma1> loc(#loc)
    %curr_n:4 = scf.for %curr_n_5 = %c0_i32 to %c8_i32 step %c1_i32 iter_args(%arg16 = %cst_0, %arg17 = %19, %arg18 = %163, %arg19 = %167) -> (tensor<128x64xf32, #mma>, i32, tensor<64x16x!tt.ptr<f16>, #blocked2>, tensor<64x16x!tt.ptr<f16>, #blocked2>)  : i32 {
      %205 = ttg.local_alloc : () -> !ttg.memdesc<1x64x16xf16, #shared1, #smem, mutable> loc(#loc)
      %206 = ttg.memdesc_index %205[%c0_i32] : !ttg.memdesc<1x64x16xf16, #shared1, #smem, mutable> -> !ttg.memdesc<64x16xf16, #shared1, #smem, mutable, 1x64x16> loc(#loc)
      %207 = ttg.async_copy_global_to_local %arg18, %206 : tensor<64x16x!tt.ptr<f16>, #blocked2> -> <64x16xf16, #shared1, #smem, mutable, 1x64x16> loc(#loc)
      %208 = ttg.async_commit_group tokens %207 loc(#loc)
      %209 = ttg.async_wait %208 {num = 0 : i32} loc(#loc)
      %210 = ttg.local_load %206 token %209 : !ttg.memdesc<64x16xf16, #shared1, #smem, mutable, 1x64x16> -> tensor<64x16xf16, #ttg.dot_op<{opIdx = 1, parent = #blocked3}>> loc(#loc)
      %211 = ttg.local_alloc %210 : (tensor<64x16xf16, #ttg.dot_op<{opIdx = 1, parent = #blocked3}>>) -> !ttg.memdesc<64x16xf16, #shared2, #smem> loc(#loc)
      %212 = ttg.local_load %206 token %209 : !ttg.memdesc<64x16xf16, #shared1, #smem, mutable, 1x64x16> -> tensor<64x16xf16, #blocked4> loc(#loc)
      %213 = ttg.local_alloc %212 : (tensor<64x16xf16, #blocked4>) -> !ttg.memdesc<64x16xf16, #shared2, #smem> loc(#loc)
      %214 = ttg.memdesc_trans %213 {order = array<i32: 1, 0>} : !ttg.memdesc<64x16xf16, #shared2, #smem> -> !ttg.memdesc<16x64xf16, #shared3, #smem> loc(#loc)
      %215 = ttg.local_alloc : () -> !ttg.memdesc<1x64x16xf16, #shared1, #smem, mutable> loc(#loc)
      %216 = ttg.memdesc_index %215[%c0_i32] : !ttg.memdesc<1x64x16xf16, #shared1, #smem, mutable> -> !ttg.memdesc<64x16xf16, #shared1, #smem, mutable, 1x64x16> loc(#loc)
      %217 = ttg.async_copy_global_to_local %arg19, %216 : tensor<64x16x!tt.ptr<f16>, #blocked2> -> <64x16xf16, #shared1, #smem, mutable, 1x64x16> loc(#loc)
      %218 = ttg.async_commit_group tokens %217 loc(#loc)
      %219 = ttg.async_wait %218 {num = 0 : i32} loc(#loc)
      %220 = ttg.local_load %216 token %219 : !ttg.memdesc<64x16xf16, #shared1, #smem, mutable, 1x64x16> -> tensor<64x16xf16, #ttg.dot_op<{opIdx = 1, parent = #blocked3}>> loc(#loc)
      %221 = ttg.local_alloc %220 : (tensor<64x16xf16, #ttg.dot_op<{opIdx = 1, parent = #blocked3}>>) -> !ttg.memdesc<64x16xf16, #shared2, #smem> loc(#loc)
      %222 = ttg.local_load %136 token %139 : !ttg.memdesc<128x64xf16, #shared, #smem, mutable, 1x128x64> -> tensor<128x64xf16, #ttg.dot_op<{opIdx = 0, parent = #mma1, kWidth = 2}>> loc(#loc)
      ttng.fence_async_shared {bCluster = false} loc(#loc)
      %223 = ttng.warp_group_dot %222, %211, %cst_1 {inputPrecision = 0 : i32, isAsync = true} : tensor<128x64xf16, #ttg.dot_op<{opIdx = 0, parent = #mma1, kWidth = 2}>> * !ttg.memdesc<64x16xf16, #shared2, #smem> -> tensor<128x16xf32, #mma1> loc(#loc)
      %224:2 = ttng.warp_group_dot_wait %223, %211 {pendings = 0 : i32} : tensor<128x16xf32, #mma1>, !ttg.memdesc<64x16xf16, #shared2, #smem> loc(#loc)
      %225 = arith.subf %224#0, %177 : tensor<128x16xf32, #mma1> loc(#loc)
      %226 = math.exp2 %225 : tensor<128x16xf32, #mma1> loc(#loc)
      %227 = tt.splat %arg17 : i32 -> tensor<16xi32, #ttg.slice<{dim = 0, parent = #mma1}>> loc(#loc)
      %228 = arith.addi %227, %55 : tensor<16xi32, #ttg.slice<{dim = 0, parent = #mma1}>> loc(#loc)
      %229 = tt.expand_dims %228 {axis = 0 : i32} : tensor<16xi32, #ttg.slice<{dim = 0, parent = #mma1}>> -> tensor<1x16xi32, #mma1> loc(#loc)
      %230 = tt.broadcast %229 : tensor<1x16xi32, #mma1> -> tensor<128x16xi32, #mma1> loc(#loc)
      %231 = arith.cmpi sge, %81, %230 : tensor<128x16xi32, #mma1> loc(#loc)
      %232 = arith.select %231, %226, %cst_1 : tensor<128x16xi1, #mma1>, tensor<128x16xf32, #mma1> loc(#loc)
      %233 = ttg.local_load %145 token %148 : !ttg.memdesc<128x64xf16, #shared, #smem, mutable, 1x128x64> -> tensor<128x64xf16, #ttg.dot_op<{opIdx = 0, parent = #mma1, kWidth = 2}>> loc(#loc)
      ttng.fence_async_shared {bCluster = false} loc(#loc)
      %234 = ttng.warp_group_dot %233, %221, %cst_1 {inputPrecision = 0 : i32, isAsync = true} : tensor<128x64xf16, #ttg.dot_op<{opIdx = 0, parent = #mma1, kWidth = 2}>> * !ttg.memdesc<64x16xf16, #shared2, #smem> -> tensor<128x16xf32, #mma1> loc(#loc)
      %235:2 = ttng.warp_group_dot_wait %234, %221 {pendings = 0 : i32} : tensor<128x16xf32, #mma1>, !ttg.memdesc<64x16xf16, #shared2, #smem> loc(#loc)
      %236 = arith.subf %235#0, %179 : tensor<128x16xf32, #mma1> loc(#loc)
      %237 = arith.mulf %232, %236 : tensor<128x16xf32, #mma1> loc(#loc)
      %238 = arith.truncf %237 : tensor<128x16xf32, #mma1> to tensor<128x16xf16, #mma1> loc(#loc)
      %239 = ttg.convert_layout %238 : tensor<128x16xf16, #mma1> -> tensor<128x16xf16, #ttg.dot_op<{opIdx = 0, parent = #mma, kWidth = 2}>> loc(#loc)
      ttng.fence_async_shared {bCluster = false} loc(#loc)
      %240 = ttng.warp_group_dot %239, %214, %arg16 {inputPrecision = 0 : i32, isAsync = true} : tensor<128x16xf16, #ttg.dot_op<{opIdx = 0, parent = #mma, kWidth = 2}>> * !ttg.memdesc<16x64xf16, #shared3, #smem> -> tensor<128x64xf32, #mma> loc(#loc)
      %241:2 = ttng.warp_group_dot_wait %240, %214 {pendings = 0 : i32} : tensor<128x64xf32, #mma>, !ttg.memdesc<16x64xf16, #shared3, #smem> loc(#loc)
      %242 = arith.addi %arg17, %c16_i32 : i32 loc(#loc)
      %243 = tt.addptr %arg18, %84 : tensor<64x16x!tt.ptr<f16>, #blocked2>, tensor<64x16xi32, #blocked2> loc(#loc)
      %244 = tt.addptr %arg19, %84 : tensor<64x16x!tt.ptr<f16>, #blocked2>, tensor<64x16xi32, #blocked2> loc(#loc)
      scf.yield %241#0, %242, %243, %244 : tensor<128x64xf32, #mma>, i32, tensor<64x16x!tt.ptr<f16>, #blocked2>, tensor<64x16x!tt.ptr<f16>, #blocked2> loc(#loc)
    } loc(#loc28)
    %180 = arith.divsi %19, %c32_i32 : i32 loc(#loc)
    %181 = arith.muli %180, %c32_i32 : i32 loc(#loc)
    %182 = arith.subi %19, %181 : i32 loc(#loc)
    %183 = tt.splat %182 : i32 -> tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked2}>> loc(#loc)
    %184 = arith.addi %183, %89 : tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked2}>> loc(#loc)
    %185 = tt.expand_dims %184 {axis = 0 : i32} : tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked2}>> -> tensor<1x32xi32, #blocked2> loc(#loc)
    %186 = arith.muli %185, %97 : tensor<1x32xi32, #blocked2> loc(#loc)
    %187 = tt.splat %11 : !tt.ptr<f16> -> tensor<1x32x!tt.ptr<f16>, #blocked2> loc(#loc)
    %188 = tt.addptr %187, %186 : tensor<1x32x!tt.ptr<f16>, #blocked2>, tensor<1x32xi32, #blocked2> loc(#loc)
    %189 = tt.broadcast %188 : tensor<1x32x!tt.ptr<f16>, #blocked2> -> tensor<64x32x!tt.ptr<f16>, #blocked2> loc(#loc)
    %190 = tt.addptr %189, %102 : tensor<64x32x!tt.ptr<f16>, #blocked2>, tensor<64x32xi32, #blocked2> loc(#loc)
    %191 = tt.splat %12 : !tt.ptr<f16> -> tensor<1x32x!tt.ptr<f16>, #blocked2> loc(#loc)
    %192 = tt.addptr %191, %186 : tensor<1x32x!tt.ptr<f16>, #blocked2>, tensor<1x32xi32, #blocked2> loc(#loc)
    %193 = tt.broadcast %192 : tensor<1x32x!tt.ptr<f16>, #blocked2> -> tensor<64x32x!tt.ptr<f16>, #blocked2> loc(#loc)
    %194 = tt.addptr %193, %102 : tensor<64x32x!tt.ptr<f16>, #blocked2>, tensor<64x32xi32, #blocked2> loc(#loc)
    %195 = tt.broadcast %159 : tensor<128x1xf32, #mma2> -> tensor<128x32xf32, #mma2> loc(#loc)
    %196 = tt.expand_dims %175 {axis = 1 : i32} : tensor<128xf32, #ttg.slice<{dim = 1, parent = #mma2}>> -> tensor<128x1xf32, #mma2> loc(#loc)
    %197 = tt.broadcast %196 : tensor<128x1xf32, #mma2> -> tensor<128x32xf32, #mma2> loc(#loc)
    %curr_n_4:3 = scf.for %curr_n_5 = %c0_i32 to %180 step %c1_i32 iter_args(%curr_n_6 = %curr_n#0, %arg17 = %190, %arg18 = %194) -> (tensor<128x64xf32, #mma>, tensor<64x32x!tt.ptr<f16>, #blocked2>, tensor<64x32x!tt.ptr<f16>, #blocked2>)  : i32 {
      %205 = ttg.local_alloc : () -> !ttg.memdesc<1x64x32xf16, #shared1, #smem, mutable> loc(#loc)
      %206 = ttg.memdesc_index %205[%c0_i32] : !ttg.memdesc<1x64x32xf16, #shared1, #smem, mutable> -> !ttg.memdesc<64x32xf16, #shared1, #smem, mutable, 1x64x32> loc(#loc)
      %207 = ttg.async_copy_global_to_local %arg17, %206 : tensor<64x32x!tt.ptr<f16>, #blocked2> -> <64x32xf16, #shared1, #smem, mutable, 1x64x32> loc(#loc)
      %208 = ttg.async_commit_group tokens %207 loc(#loc)
      %209 = ttg.async_wait %208 {num = 0 : i32} loc(#loc)
      %210 = ttg.local_load %206 token %209 : !ttg.memdesc<64x32xf16, #shared1, #smem, mutable, 1x64x32> -> tensor<64x32xf16, #ttg.dot_op<{opIdx = 1, parent = #blocked7}>> loc(#loc)
      %211 = ttg.local_alloc %210 : (tensor<64x32xf16, #ttg.dot_op<{opIdx = 1, parent = #blocked7}>>) -> !ttg.memdesc<64x32xf16, #shared7, #smem> loc(#loc)
      %212 = ttg.local_load %206 token %209 : !ttg.memdesc<64x32xf16, #shared1, #smem, mutable, 1x64x32> -> tensor<64x32xf16, #blocked8> loc(#loc)
      %213 = ttg.local_alloc %212 : (tensor<64x32xf16, #blocked8>) -> !ttg.memdesc<64x32xf16, #shared7, #smem> loc(#loc)
      %214 = ttg.memdesc_trans %213 {order = array<i32: 1, 0>} : !ttg.memdesc<64x32xf16, #shared7, #smem> -> !ttg.memdesc<32x64xf16, #shared8, #smem> loc(#loc)
      %215 = ttg.local_alloc : () -> !ttg.memdesc<1x64x32xf16, #shared1, #smem, mutable> loc(#loc)
      %216 = ttg.memdesc_index %215[%c0_i32] : !ttg.memdesc<1x64x32xf16, #shared1, #smem, mutable> -> !ttg.memdesc<64x32xf16, #shared1, #smem, mutable, 1x64x32> loc(#loc)
      %217 = ttg.async_copy_global_to_local %arg18, %216 : tensor<64x32x!tt.ptr<f16>, #blocked2> -> <64x32xf16, #shared1, #smem, mutable, 1x64x32> loc(#loc)
      %218 = ttg.async_commit_group tokens %217 loc(#loc)
      %219 = ttg.async_wait %218 {num = 0 : i32} loc(#loc)
      %220 = ttg.local_load %216 token %219 : !ttg.memdesc<64x32xf16, #shared1, #smem, mutable, 1x64x32> -> tensor<64x32xf16, #ttg.dot_op<{opIdx = 1, parent = #blocked7}>> loc(#loc)
      %221 = ttg.local_alloc %220 : (tensor<64x32xf16, #ttg.dot_op<{opIdx = 1, parent = #blocked7}>>) -> !ttg.memdesc<64x32xf16, #shared7, #smem> loc(#loc)
      %222 = ttg.local_load %136 token %139 : !ttg.memdesc<128x64xf16, #shared, #smem, mutable, 1x128x64> -> tensor<128x64xf16, #ttg.dot_op<{opIdx = 0, parent = #mma2, kWidth = 2}>> loc(#loc)
      ttng.fence_async_shared {bCluster = false} loc(#loc)
      %223 = ttng.warp_group_dot %222, %211, %cst_2 {inputPrecision = 0 : i32, isAsync = true} : tensor<128x64xf16, #ttg.dot_op<{opIdx = 0, parent = #mma2, kWidth = 2}>> * !ttg.memdesc<64x32xf16, #shared7, #smem> -> tensor<128x32xf32, #mma2> loc(#loc)
      %224:2 = ttng.warp_group_dot_wait %223, %211 {pendings = 0 : i32} : tensor<128x32xf32, #mma2>, !ttg.memdesc<64x32xf16, #shared7, #smem> loc(#loc)
      %225 = arith.subf %224#0, %195 : tensor<128x32xf32, #mma2> loc(#loc)
      %226 = math.exp2 %225 : tensor<128x32xf32, #mma2> loc(#loc)
      %227 = ttg.local_load %145 token %148 : !ttg.memdesc<128x64xf16, #shared, #smem, mutable, 1x128x64> -> tensor<128x64xf16, #ttg.dot_op<{opIdx = 0, parent = #mma2, kWidth = 2}>> loc(#loc)
      ttng.fence_async_shared {bCluster = false} loc(#loc)
      %228 = ttng.warp_group_dot %227, %221, %cst_2 {inputPrecision = 0 : i32, isAsync = true} : tensor<128x64xf16, #ttg.dot_op<{opIdx = 0, parent = #mma2, kWidth = 2}>> * !ttg.memdesc<64x32xf16, #shared7, #smem> -> tensor<128x32xf32, #mma2> loc(#loc)
      %229:2 = ttng.warp_group_dot_wait %228, %221 {pendings = 0 : i32} : tensor<128x32xf32, #mma2>, !ttg.memdesc<64x32xf16, #shared7, #smem> loc(#loc)
      %230 = arith.subf %229#0, %197 : tensor<128x32xf32, #mma2> loc(#loc)
      %231 = arith.mulf %226, %230 : tensor<128x32xf32, #mma2> loc(#loc)
      %232 = arith.truncf %231 : tensor<128x32xf32, #mma2> to tensor<128x32xf16, #mma2> loc(#loc)
      %233 = ttg.convert_layout %232 : tensor<128x32xf16, #mma2> -> tensor<128x32xf16, #ttg.dot_op<{opIdx = 0, parent = #mma, kWidth = 2}>> loc(#loc)
      ttng.fence_async_shared {bCluster = false} loc(#loc)
      %234 = ttng.warp_group_dot %233, %214, %curr_n_6 {inputPrecision = 0 : i32, isAsync = true} : tensor<128x32xf16, #ttg.dot_op<{opIdx = 0, parent = #mma, kWidth = 2}>> * !ttg.memdesc<32x64xf16, #shared8, #smem> -> tensor<128x64xf32, #mma> loc(#loc)
      %235:2 = ttng.warp_group_dot_wait %234, %214 {pendings = 0 : i32} : tensor<128x64xf32, #mma>, !ttg.memdesc<32x64xf16, #shared8, #smem> loc(#loc)
      %236 = tt.addptr %arg17, %115 : tensor<64x32x!tt.ptr<f16>, #blocked2>, tensor<64x32xi32, #blocked2> loc(#loc)
      %237 = tt.addptr %arg18, %115 : tensor<64x32x!tt.ptr<f16>, #blocked2>, tensor<64x32xi32, #blocked2> loc(#loc)
      scf.yield %235#0, %236, %237 : tensor<128x64xf32, #mma>, tensor<64x32x!tt.ptr<f16>, #blocked2>, tensor<64x32x!tt.ptr<f16>, #blocked2> loc(#loc)
    } loc(#loc26)
    %198 = tt.splat %14 : !tt.ptr<f16> -> tensor<128x1x!tt.ptr<f16>, #blocked> loc(#loc)
    %199 = tt.addptr %198, %32 : tensor<128x1x!tt.ptr<f16>, #blocked>, tensor<128x1xi32, #blocked> loc(#loc)
    %200 = tt.broadcast %199 : tensor<128x1x!tt.ptr<f16>, #blocked> -> tensor<128x64x!tt.ptr<f16>, #blocked> loc(#loc)
    %201 = tt.addptr %200, %38 : tensor<128x64x!tt.ptr<f16>, #blocked>, tensor<128x64xi32, #blocked> loc(#loc)
    %202 = arith.mulf %curr_n_4#0, %cst : tensor<128x64xf32, #mma> loc(#loc)
    %203 = arith.truncf %202 : tensor<128x64xf32, #mma> to tensor<128x64xf16, #mma> loc(#loc)
    %204 = ttg.convert_layout %203 : tensor<128x64xf16, #mma> -> tensor<128x64xf16, #blocked> loc(#loc)
    tt.store %201, %204 : tensor<128x64x!tt.ptr<f16>, #blocked> loc(#loc)
    tt.return loc(#loc)
  } loc(#loc)
} loc(#loc)
#loc = loc(unknown)
#loc16 = loc("dk")
#loc17 = loc("dq")
#loc18 = loc("dv"(#loc16))
#loc19 = loc("offs_n"(#loc17))
#loc20 = loc("kT_ptrs"(#loc17))
#loc21 = loc("offs_m"(#loc18))
#loc22 = loc("kT_ptrs"(#loc19))
#loc23 = loc("vT_ptrs"(#loc20))
#loc24 = loc("qT_ptrs"(#loc21))
#loc25 = loc("vT_ptrs"(#loc22))
#loc26 = loc("curr_n"(#loc23))
#loc27 = loc("do_ptrs"(#loc24))
#loc28 = loc("curr_n"(#loc25))
#loc29 = loc("curr_m"(#loc27))
