//
// Generated by LLVM NVPTX Back-End
//

.version 8.7
.target sm_90a
.address_size 64

	// .globl	_attn_bwd               // -- Begin function _attn_bwd
.extern .shared .align 16 .b8 global_smem[];
.global .align 1 .b8 _$_str[11] = {95, 95, 67, 85, 68, 65, 95, 70, 84, 90};
                                        // @_attn_bwd
.visible .entry _attn_bwd(
	.param .u64 .ptr .global .align 1 _attn_bwd_param_0,
	.param .u64 .ptr .global .align 1 _attn_bwd_param_1,
	.param .u64 .ptr .global .align 1 _attn_bwd_param_2,
	.param .f32 _attn_bwd_param_3,
	.param .u64 .ptr .global .align 1 _attn_bwd_param_4,
	.param .u64 .ptr .global .align 1 _attn_bwd_param_5,
	.param .u64 .ptr .global .align 1 _attn_bwd_param_6,
	.param .u64 .ptr .global .align 1 _attn_bwd_param_7,
	.param .u64 .ptr .global .align 1 _attn_bwd_param_8,
	.param .u64 .ptr .global .align 1 _attn_bwd_param_9,
	.param .u32 _attn_bwd_param_10,
	.param .u32 _attn_bwd_param_11,
	.param .u32 _attn_bwd_param_12,
	.param .u32 _attn_bwd_param_13,
	.param .u32 _attn_bwd_param_14,
	.param .u64 .ptr .global .align 1 _attn_bwd_param_15,
	.param .u64 .ptr .global .align 1 _attn_bwd_param_16
)
.reqntid 128
{
	.reg .pred 	%p<189>;
	.reg .b32 	%r<7215>;
	.reg .b64 	%rd<662>;

// %bb.0:
	ld.param.b32 	%r1265, [_attn_bwd_param_14];
	ld.param.b32 	%r1264, [_attn_bwd_param_12];
	ld.param.b64 	%rd91, [_attn_bwd_param_7];
	ld.param.b64 	%rd90, [_attn_bwd_param_6];
	ld.param.b64 	%rd89, [_attn_bwd_param_5];
	ld.param.b64 	%rd88, [_attn_bwd_param_4];
	ld.param.b32 	%r1263, [_attn_bwd_param_3];
	ld.param.b64 	%rd87, [_attn_bwd_param_2];
	ld.param.b64 	%rd86, [_attn_bwd_param_1];
	ld.param.b64 	%rd85, [_attn_bwd_param_0];
	mov.u32 	%r1382, %ctaid.z;
	mul.lo.s32 	%r1383, %r1265, %r1382;
	ld.param.b32 	%r1384, [_attn_bwd_param_13];
	div.s32 	%r1386, %r1382, %r1384;
	mul.lo.s32 	%r1387, %r1386, %r1384;
	sub.s32 	%r1388, %r1382, %r1387;
	ld.param.b32 	%r1389, [_attn_bwd_param_11];
	mul.lo.s32 	%r1390, %r1388, %r1389;
	ld.param.b64 	%rd132, [_attn_bwd_param_8];
	ld.param.b64 	%rd133, [_attn_bwd_param_9];
	ld.param.b32 	%r1391, [_attn_bwd_param_10];
	mad.lo.s32 	%r1392, %r1386, %r1391, %r1390;
	cvt.s64.s32 	%rd1, %r1392;
	mov.u32 	%r1393, %ctaid.x;
	mul.wide.s32 	%rd134, %r1392, 2;
	add.s64 	%rd2, %rd85, %rd134;
	add.s64 	%rd3, %rd86, %rd134;
	add.s64 	%rd4, %rd87, %rd134;
	add.s64 	%rd5, %rd88, %rd134;
	mul.wide.s32 	%rd135, %r1383, 4;
	add.s64 	%rd6, %rd132, %rd135;
	add.s64 	%rd7, %rd133, %rd135;
	shl.b32 	%r1, %r1393, 7;
	mov.u32 	%r2, %tid.x;
	shr.u32 	%r3, %r2, 5;
	shr.u32 	%r1394, %r2, 3;
	bfe.u32 	%r4, %r2, 3, 4;
	or.b32 	%r5, %r4, 16;
	and.b32 	%r6, %r2, 96;
	shr.u32 	%r1395, %r6, 1;
	bfe.u32 	%r1396, %r2, 2, 3;
	or.b32 	%r1397, %r1395, %r1396;
	and.b32 	%r7, %r2, 127;
	or.b32 	%r28, %r4, %r1;
	or.b32 	%r1398, %r5, %r1;
	or.b32 	%r1399, %r1394, %r1;
	or.b32 	%r1400, %r1399, 112;
	or.b32 	%r8, %r1397, %r1;
	or.b32 	%r9, %r8, 8;
	or.b32 	%r10, %r8, 64;
	or.b32 	%r11, %r8, 72;
	mul.lo.s32 	%r1401, %r1264, %r28;
	mul.lo.s32 	%r1402, %r1264, %r1398;
	shl.b32 	%r12, %r1264, 5;
	add.s32 	%r1403, %r1401, %r12;
	shl.b32 	%r1404, %r1264, 4;
	add.s32 	%r1405, %r1403, %r1404;
	add.s32 	%r1406, %r1405, %r1404;
	add.s32 	%r1407, %r1406, %r1404;
	add.s32 	%r1408, %r1407, %r1404;
	mul.lo.s32 	%r1409, %r1264, %r1400;
	cvt.s64.s32 	%rd8, %r1401;
	mul.wide.s32 	%rd136, %r1401, 2;
	add.s64 	%rd137, %rd3, %rd136;
	cvt.s64.s32 	%rd9, %r1402;
	mul.wide.s32 	%rd138, %r1402, 2;
	add.s64 	%rd139, %rd3, %rd138;
	cvt.s64.s32 	%rd10, %r1403;
	mul.wide.s32 	%rd140, %r1403, 2;
	add.s64 	%rd141, %rd3, %rd140;
	cvt.s64.s32 	%rd11, %r1405;
	mul.wide.s32 	%rd142, %r1405, 2;
	add.s64 	%rd143, %rd3, %rd142;
	cvt.s64.s32 	%rd12, %r1406;
	mul.wide.s32 	%rd144, %r1406, 2;
	add.s64 	%rd145, %rd3, %rd144;
	cvt.s64.s32 	%rd13, %r1407;
	mul.wide.s32 	%rd146, %r1407, 2;
	add.s64 	%rd147, %rd3, %rd146;
	cvt.s64.s32 	%rd14, %r1408;
	mul.wide.s32 	%rd148, %r1408, 2;
	add.s64 	%rd149, %rd3, %rd148;
	cvt.s64.s32 	%rd15, %r1409;
	mul.wide.s32 	%rd150, %r1409, 2;
	add.s64 	%rd151, %rd3, %rd150;
	shl.b32 	%r1410, %r2, 3;
	and.b32 	%r1411, %r1410, 56;
	cvt.u64.u32 	%rd16, %r1411;
	mul.wide.u32 	%rd152, %r1411, 2;
	add.s64 	%rd451, %rd137, %rd152;
	add.s64 	%rd93, %rd139, %rd152;
	add.s64 	%rd94, %rd141, %rd152;
	add.s64 	%rd95, %rd143, %rd152;
	add.s64 	%rd96, %rd145, %rd152;
	add.s64 	%rd97, %rd147, %rd152;
	add.s64 	%rd98, %rd149, %rd152;
	add.s64 	%rd99, %rd151, %rd152;
	// begin inline asm
	mov.u32 %r1266, 0x0;
	mov.u32 %r1267, 0x0;
	mov.u32 %r1268, 0x0;
	mov.u32 %r1269, 0x0;
	ld.global.v4.b32 { %r1266, %r1267, %r1268, %r1269 }, [ %rd451 + 0 ];
	// end inline asm
	// begin inline asm
	mov.u32 %r1270, 0x0;
	mov.u32 %r1271, 0x0;
	mov.u32 %r1272, 0x0;
	mov.u32 %r1273, 0x0;
	ld.global.v4.b32 { %r1270, %r1271, %r1272, %r1273 }, [ %rd93 + 0 ];
	// end inline asm
	// begin inline asm
	mov.u32 %r1274, 0x0;
	mov.u32 %r1275, 0x0;
	mov.u32 %r1276, 0x0;
	mov.u32 %r1277, 0x0;
	ld.global.v4.b32 { %r1274, %r1275, %r1276, %r1277 }, [ %rd94 + 0 ];
	// end inline asm
	// begin inline asm
	mov.u32 %r1278, 0x0;
	mov.u32 %r1279, 0x0;
	mov.u32 %r1280, 0x0;
	mov.u32 %r1281, 0x0;
	ld.global.v4.b32 { %r1278, %r1279, %r1280, %r1281 }, [ %rd95 + 0 ];
	// end inline asm
	// begin inline asm
	mov.u32 %r1282, 0x0;
	mov.u32 %r1283, 0x0;
	mov.u32 %r1284, 0x0;
	mov.u32 %r1285, 0x0;
	ld.global.v4.b32 { %r1282, %r1283, %r1284, %r1285 }, [ %rd96 + 0 ];
	// end inline asm
	// begin inline asm
	mov.u32 %r1286, 0x0;
	mov.u32 %r1287, 0x0;
	mov.u32 %r1288, 0x0;
	mov.u32 %r1289, 0x0;
	ld.global.v4.b32 { %r1286, %r1287, %r1288, %r1289 }, [ %rd97 + 0 ];
	// end inline asm
	// begin inline asm
	mov.u32 %r1290, 0x0;
	mov.u32 %r1291, 0x0;
	mov.u32 %r1292, 0x0;
	mov.u32 %r1293, 0x0;
	ld.global.v4.b32 { %r1290, %r1291, %r1292, %r1293 }, [ %rd98 + 0 ];
	// end inline asm
	// begin inline asm
	mov.u32 %r1294, 0x0;
	mov.u32 %r1295, 0x0;
	mov.u32 %r1296, 0x0;
	mov.u32 %r1297, 0x0;
	ld.global.v4.b32 { %r1294, %r1295, %r1296, %r1297 }, [ %rd99 + 0 ];
	// end inline asm
	shl.b32 	%r1412, %r7, 4;
	shl.b32 	%r13, %r2, 1;
	and.b32 	%r1413, %r13, 112;
	xor.b32 	%r14, %r1412, %r1413;
	mov.b32 	%r1414, global_smem;
	add.s32 	%r1415, %r1414, 40960;
	add.s32 	%r15, %r1415, %r14;
	st.shared.v4.b32 	[%r15], {%r1266, %r1267, %r1268, %r1269};
	st.shared.v4.b32 	[%r15+2048], {%r1270, %r1271, %r1272, %r1273};
	st.shared.v4.b32 	[%r15+4096], {%r1274, %r1275, %r1276, %r1277};
	st.shared.v4.b32 	[%r15+6144], {%r1278, %r1279, %r1280, %r1281};
	st.shared.v4.b32 	[%r15+8192], {%r1282, %r1283, %r1284, %r1285};
	st.shared.v4.b32 	[%r15+10240], {%r1286, %r1287, %r1288, %r1289};
	st.shared.v4.b32 	[%r15+12288], {%r1290, %r1291, %r1292, %r1293};
	st.shared.v4.b32 	[%r15+14336], {%r1294, %r1295, %r1296, %r1297};
	add.s64 	%rd153, %rd4, %rd136;
	add.s64 	%rd154, %rd4, %rd138;
	add.s64 	%rd155, %rd4, %rd140;
	add.s64 	%rd156, %rd4, %rd142;
	add.s64 	%rd157, %rd4, %rd144;
	add.s64 	%rd158, %rd4, %rd146;
	add.s64 	%rd159, %rd4, %rd148;
	add.s64 	%rd160, %rd4, %rd150;
	add.s64 	%rd452, %rd153, %rd152;
	add.s64 	%rd101, %rd154, %rd152;
	add.s64 	%rd102, %rd155, %rd152;
	add.s64 	%rd103, %rd156, %rd152;
	add.s64 	%rd104, %rd157, %rd152;
	add.s64 	%rd105, %rd158, %rd152;
	add.s64 	%rd106, %rd159, %rd152;
	add.s64 	%rd107, %rd160, %rd152;
	// begin inline asm
	mov.u32 %r1298, 0x0;
	mov.u32 %r1299, 0x0;
	mov.u32 %r1300, 0x0;
	mov.u32 %r1301, 0x0;
	ld.global.v4.b32 { %r1298, %r1299, %r1300, %r1301 }, [ %rd452 + 0 ];
	// end inline asm
	// begin inline asm
	mov.u32 %r1302, 0x0;
	mov.u32 %r1303, 0x0;
	mov.u32 %r1304, 0x0;
	mov.u32 %r1305, 0x0;
	ld.global.v4.b32 { %r1302, %r1303, %r1304, %r1305 }, [ %rd101 + 0 ];
	// end inline asm
	// begin inline asm
	mov.u32 %r1306, 0x0;
	mov.u32 %r1307, 0x0;
	mov.u32 %r1308, 0x0;
	mov.u32 %r1309, 0x0;
	ld.global.v4.b32 { %r1306, %r1307, %r1308, %r1309 }, [ %rd102 + 0 ];
	// end inline asm
	// begin inline asm
	mov.u32 %r1310, 0x0;
	mov.u32 %r1311, 0x0;
	mov.u32 %r1312, 0x0;
	mov.u32 %r1313, 0x0;
	ld.global.v4.b32 { %r1310, %r1311, %r1312, %r1313 }, [ %rd103 + 0 ];
	// end inline asm
	// begin inline asm
	mov.u32 %r1314, 0x0;
	mov.u32 %r1315, 0x0;
	mov.u32 %r1316, 0x0;
	mov.u32 %r1317, 0x0;
	ld.global.v4.b32 { %r1314, %r1315, %r1316, %r1317 }, [ %rd104 + 0 ];
	// end inline asm
	// begin inline asm
	mov.u32 %r1318, 0x0;
	mov.u32 %r1319, 0x0;
	mov.u32 %r1320, 0x0;
	mov.u32 %r1321, 0x0;
	ld.global.v4.b32 { %r1318, %r1319, %r1320, %r1321 }, [ %rd105 + 0 ];
	// end inline asm
	// begin inline asm
	mov.u32 %r1322, 0x0;
	mov.u32 %r1323, 0x0;
	mov.u32 %r1324, 0x0;
	mov.u32 %r1325, 0x0;
	ld.global.v4.b32 { %r1322, %r1323, %r1324, %r1325 }, [ %rd106 + 0 ];
	// end inline asm
	// begin inline asm
	mov.u32 %r1326, 0x0;
	mov.u32 %r1327, 0x0;
	mov.u32 %r1328, 0x0;
	mov.u32 %r1329, 0x0;
	ld.global.v4.b32 { %r1326, %r1327, %r1328, %r1329 }, [ %rd107 + 0 ];
	// end inline asm
	add.s32 	%r1416, %r1414, 57344;
	add.s32 	%r16, %r1416, %r14;
	st.shared.v4.b32 	[%r16], {%r1298, %r1299, %r1300, %r1301};
	st.shared.v4.b32 	[%r16+2048], {%r1302, %r1303, %r1304, %r1305};
	st.shared.v4.b32 	[%r16+4096], {%r1306, %r1307, %r1308, %r1309};
	st.shared.v4.b32 	[%r16+6144], {%r1310, %r1311, %r1312, %r1313};
	st.shared.v4.b32 	[%r16+8192], {%r1314, %r1315, %r1316, %r1317};
	st.shared.v4.b32 	[%r16+10240], {%r1318, %r1319, %r1320, %r1321};
	st.shared.v4.b32 	[%r16+12288], {%r1322, %r1323, %r1324, %r1325};
	st.shared.v4.b32 	[%r16+14336], {%r1326, %r1327, %r1328, %r1329};
	and.b32 	%r1417, %r2, 3;
	shl.b32 	%r1418, %r1417, 1;
	add.s64 	%rd161, %rd2, %rd136;
	add.s64 	%rd433, %rd161, %rd152;
	add.s64 	%rd162, %rd5, %rd136;
	add.s64 	%rd441, %rd162, %rd152;
	add.s32 	%r5245, %r1414, %r14;
	mov.b32 	%r1331, 16;
	// begin inline asm
	cp.async.cg.shared.global [ %r5245 + 0 ], [ %rd433 + 0 ], 0x10, %r1331;
	// end inline asm
	cp.async.commit_group;
	or.b32 	%r27, %r1418, %r1;
	mul.wide.s32 	%rd163, %r27, 4;
	add.s64 	%rd109, %rd6, %rd163;
	cvt.s64.s32 	%rd164, %r1;
	cvt.u64.u32 	%rd21, %r1418;
	or.b64 	%rd165, %rd21, %rd164;
	shl.b64 	%rd166, %rd165, 2;
	add.s64 	%rd167, %rd6, %rd166;
	add.s64 	%rd110, %rd167, 32;
	and.b32 	%r18, %r2, 124;
	setp.eq.b32 	%p1, %r18, 0;
	shl.b32 	%r19, %r1417, 3;
	add.s32 	%r1419, %r1414, %r19;
	add.s32 	%r1332, %r1419, 20480;
	mov.b32 	%r1333, 8;
	// begin inline asm
	@%p1 cp.async.ca.shared.global [ %r1332 + 0 ], [ %rd109 + 0 ], 0x8, %r1333;
	// end inline asm
	add.s32 	%r1334, %r1419, 20512;
	// begin inline asm
	@%p1 cp.async.ca.shared.global [ %r1334 + 0 ], [ %rd110 + 0 ], 0x8, %r1333;
	// end inline asm
	cp.async.commit_group;
	add.s32 	%r1336, %r5245, 10240;
	// begin inline asm
	cp.async.cg.shared.global [ %r1336 + 0 ], [ %rd441 + 0 ], 0x10, %r1331;
	// end inline asm
	cp.async.commit_group;
	add.s64 	%rd112, %rd7, %rd163;
	add.s64 	%rd168, %rd7, %rd166;
	add.s64 	%rd113, %rd168, 32;
	add.s32 	%r1338, %r1419, 20736;
	// begin inline asm
	@%p1 cp.async.ca.shared.global [ %r1338 + 0 ], [ %rd112 + 0 ], 0x8, %r1333;
	// end inline asm
	add.s32 	%r1340, %r1419, 20768;
	// begin inline asm
	@%p1 cp.async.ca.shared.global [ %r1340 + 0 ], [ %rd113 + 0 ], 0x8, %r1333;
	// end inline asm
	cp.async.commit_group;
	or.b32 	%r1420, %r1, 16;
	cvt.s64.s32 	%rd22, %r1404;
	mul.wide.s32 	%rd41, %r1404, 2;
	add.s64 	%rd114, %rd433, %rd41;
	add.s64 	%rd117, %rd441, %rd41;
	bar.sync 	0;
	add.s32 	%r1342, %r5245, 2048;
	// begin inline asm
	cp.async.cg.shared.global [ %r1342 + 0 ], [ %rd114 + 0 ], 0x10, %r1331;
	// end inline asm
	cp.async.commit_group;
	add.s64 	%rd115, %rd167, 64;
	cvt.s64.s32 	%rd169, %r1420;
	or.b64 	%rd170, %rd21, %rd169;
	shl.b64 	%rd171, %rd170, 2;
	add.s64 	%rd172, %rd6, %rd171;
	add.s64 	%rd116, %rd172, 32;
	add.s32 	%r1344, %r1419, 20544;
	// begin inline asm
	@%p1 cp.async.ca.shared.global [ %r1344 + 0 ], [ %rd115 + 0 ], 0x8, %r1333;
	// end inline asm
	add.s32 	%r1346, %r1419, 20576;
	// begin inline asm
	@%p1 cp.async.ca.shared.global [ %r1346 + 0 ], [ %rd116 + 0 ], 0x8, %r1333;
	// end inline asm
	cp.async.commit_group;
	add.s32 	%r1348, %r5245, 12288;
	// begin inline asm
	cp.async.cg.shared.global [ %r1348 + 0 ], [ %rd117 + 0 ], 0x10, %r1331;
	// end inline asm
	cp.async.commit_group;
	add.s64 	%rd118, %rd168, 64;
	add.s64 	%rd173, %rd7, %rd171;
	add.s64 	%rd119, %rd173, 32;
	add.s32 	%r1350, %r1419, 20800;
	// begin inline asm
	@%p1 cp.async.ca.shared.global [ %r1350 + 0 ], [ %rd118 + 0 ], 0x8, %r1333;
	// end inline asm
	add.s32 	%r1352, %r1419, 20832;
	// begin inline asm
	@%p1 cp.async.ca.shared.global [ %r1352 + 0 ], [ %rd119 + 0 ], 0x8, %r1333;
	// end inline asm
	cp.async.commit_group;
	or.b32 	%r1421, %r1, 32;
	add.s64 	%rd120, %rd114, %rd41;
	add.s64 	%rd123, %rd117, %rd41;
	bar.sync 	0;
	add.s32 	%r1354, %r5245, 4096;
	// begin inline asm
	cp.async.cg.shared.global [ %r1354 + 0 ], [ %rd120 + 0 ], 0x10, %r1331;
	// end inline asm
	cp.async.commit_group;
	add.s64 	%rd121, %rd167, 128;
	cvt.s64.s32 	%rd174, %r1421;
	or.b64 	%rd175, %rd21, %rd174;
	shl.b64 	%rd176, %rd175, 2;
	add.s64 	%rd177, %rd6, %rd176;
	add.s64 	%rd122, %rd177, 32;
	add.s32 	%r1356, %r1419, 20608;
	// begin inline asm
	@%p1 cp.async.ca.shared.global [ %r1356 + 0 ], [ %rd121 + 0 ], 0x8, %r1333;
	// end inline asm
	add.s32 	%r1358, %r1419, 20640;
	// begin inline asm
	@%p1 cp.async.ca.shared.global [ %r1358 + 0 ], [ %rd122 + 0 ], 0x8, %r1333;
	// end inline asm
	cp.async.commit_group;
	add.s32 	%r1360, %r5245, 14336;
	// begin inline asm
	cp.async.cg.shared.global [ %r1360 + 0 ], [ %rd123 + 0 ], 0x10, %r1331;
	// end inline asm
	cp.async.commit_group;
	add.s64 	%rd124, %rd168, 128;
	add.s64 	%rd178, %rd7, %rd176;
	add.s64 	%rd125, %rd178, 32;
	add.s32 	%r1362, %r1419, 20864;
	// begin inline asm
	@%p1 cp.async.ca.shared.global [ %r1362 + 0 ], [ %rd124 + 0 ], 0x8, %r1333;
	// end inline asm
	add.s32 	%r1364, %r1419, 20896;
	// begin inline asm
	@%p1 cp.async.ca.shared.global [ %r1364 + 0 ], [ %rd125 + 0 ], 0x8, %r1333;
	// end inline asm
	cp.async.commit_group;
	or.b32 	%r1422, %r1, 48;
	add.s64 	%rd126, %rd120, %rd41;
	add.s64 	%rd129, %rd123, %rd41;
	bar.sync 	0;
	add.s32 	%r1366, %r5245, 6144;
	// begin inline asm
	cp.async.cg.shared.global [ %r1366 + 0 ], [ %rd126 + 0 ], 0x10, %r1331;
	// end inline asm
	cp.async.commit_group;
	add.s64 	%rd127, %rd167, 192;
	cvt.s64.s32 	%rd179, %r1422;
	or.b64 	%rd180, %rd21, %rd179;
	shl.b64 	%rd181, %rd180, 2;
	add.s64 	%rd182, %rd6, %rd181;
	add.s64 	%rd128, %rd182, 32;
	add.s32 	%r1368, %r1419, 20672;
	// begin inline asm
	@%p1 cp.async.ca.shared.global [ %r1368 + 0 ], [ %rd127 + 0 ], 0x8, %r1333;
	// end inline asm
	add.s32 	%r1370, %r1419, 20704;
	// begin inline asm
	@%p1 cp.async.ca.shared.global [ %r1370 + 0 ], [ %rd128 + 0 ], 0x8, %r1333;
	// end inline asm
	cp.async.commit_group;
	add.s32 	%r1372, %r5245, 16384;
	// begin inline asm
	cp.async.cg.shared.global [ %r1372 + 0 ], [ %rd129 + 0 ], 0x10, %r1331;
	// end inline asm
	cp.async.commit_group;
	add.s64 	%rd130, %rd168, 192;
	add.s64 	%rd183, %rd7, %rd181;
	add.s64 	%rd131, %rd183, 32;
	add.s32 	%r1374, %r1419, 20928;
	// begin inline asm
	@%p1 cp.async.ca.shared.global [ %r1374 + 0 ], [ %rd130 + 0 ], 0x8, %r1333;
	// end inline asm
	add.s32 	%r1376, %r1419, 20960;
	// begin inline asm
	@%p1 cp.async.ca.shared.global [ %r1376 + 0 ], [ %rd131 + 0 ], 0x8, %r1333;
	// end inline asm
	cp.async.commit_group;
	// begin inline asm
	fence.proxy.async.shared::cta;
	// end inline asm
	bfe.u32 	%r1423, %r1415, 4, 14;
	cvt.u64.u32 	%rd184, %r1423;
	or.b64 	%rd574, %rd184, 4611686293372403712;
	add.s32 	%r1424, %r1414, 40992;
	bfe.u32 	%r1425, %r1424, 4, 14;
	cvt.u64.u32 	%rd185, %r1425;
	or.b64 	%rd576, %rd185, 4611686293372403712;
	add.s32 	%r1426, %r1414, 41024;
	bfe.u32 	%r1427, %r1426, 4, 14;
	cvt.u64.u32 	%rd186, %r1427;
	or.b64 	%rd578, %rd186, 4611686293372403712;
	add.s32 	%r1428, %r1414, 41056;
	bfe.u32 	%r1429, %r1428, 4, 14;
	cvt.u64.u32 	%rd187, %r1429;
	or.b64 	%rd580, %rd187, 4611686293372403712;
	add.s32 	%r1430, %r1414, 49152;
	bfe.u32 	%r1431, %r1430, 4, 14;
	cvt.u64.u32 	%rd188, %r1431;
	or.b64 	%rd582, %rd188, 4611686293372403712;
	add.s32 	%r1432, %r1414, 49184;
	bfe.u32 	%r1433, %r1432, 4, 14;
	cvt.u64.u32 	%rd189, %r1433;
	or.b64 	%rd584, %rd189, 4611686293372403712;
	add.s32 	%r1434, %r1414, 49216;
	bfe.u32 	%r1435, %r1434, 4, 14;
	cvt.u64.u32 	%rd190, %r1435;
	or.b64 	%rd586, %rd190, 4611686293372403712;
	add.s32 	%r1436, %r1414, 49248;
	bfe.u32 	%r1437, %r1436, 4, 14;
	cvt.u64.u32 	%rd191, %r1437;
	or.b64 	%rd588, %rd191, 4611686293372403712;
	bfe.u32 	%r1438, %r1416, 4, 14;
	cvt.u64.u32 	%rd192, %r1438;
	or.b64 	%rd590, %rd192, 4611686293372403712;
	add.s32 	%r1439, %r1414, 57376;
	bfe.u32 	%r1440, %r1439, 4, 14;
	cvt.u64.u32 	%rd193, %r1440;
	or.b64 	%rd592, %rd193, 4611686293372403712;
	add.s32 	%r1441, %r1414, 57408;
	bfe.u32 	%r1442, %r1441, 4, 14;
	cvt.u64.u32 	%rd194, %r1442;
	or.b64 	%rd594, %rd194, 4611686293372403712;
	add.s32 	%r1443, %r1414, 57440;
	bfe.u32 	%r1444, %r1443, 4, 14;
	cvt.u64.u32 	%rd195, %r1444;
	or.b64 	%rd596, %rd195, 4611686293372403712;
	add.s32 	%r1445, %r1414, 65536;
	bfe.u32 	%r1446, %r1445, 4, 14;
	cvt.u64.u32 	%rd196, %r1446;
	or.b64 	%rd598, %rd196, 4611686293372403712;
	add.s32 	%r1447, %r1414, 65568;
	bfe.u32 	%r1448, %r1447, 4, 14;
	cvt.u64.u32 	%rd197, %r1448;
	or.b64 	%rd600, %rd197, 4611686293372403712;
	add.s32 	%r1449, %r1414, 65600;
	bfe.u32 	%r1450, %r1449, 4, 14;
	cvt.u64.u32 	%rd198, %r1450;
	or.b64 	%rd602, %rd198, 4611686293372403712;
	add.s32 	%r1451, %r1414, 65632;
	bfe.u32 	%r1452, %r1451, 4, 14;
	cvt.u64.u32 	%rd199, %r1452;
	or.b64 	%rd604, %rd199, 4611686293372403712;
	and.b32 	%r1453, %r2, 7;
	mul.wide.u32 	%rd39, %r1453, 16;
	mul.wide.s32 	%rd200, %r1404, 8;
	or.b64 	%rd201, %rd39, %rd200;
	add.s64 	%rd202, %rd8, %rd1;
	shl.b64 	%rd203, %rd202, 1;
	add.s64 	%rd657, %rd201, %rd203;
	mov.b32 	%r6822, 0f00000000;
	mov.b32 	%r6622, 3;
	mov.b32 	%r6621, -1;
	mov.b32 	%r1605, 0;
	mov.b64 	%rd652, %rd657;
	mov.b32 	%r6620, %r1605;
	mov.b32 	%r6623, %r6621;
	mov.b32 	%r6624, %r6622;
	mov.b32 	%r6821, %r6822;
	mov.b32 	%r6820, %r6822;
	mov.b32 	%r6819, %r6822;
	mov.b32 	%r6818, %r6822;
	mov.b32 	%r6817, %r6822;
	mov.b32 	%r6816, %r6822;
	mov.b32 	%r6815, %r6822;
	mov.b32 	%r6814, %r6822;
	mov.b32 	%r6813, %r6822;
	mov.b32 	%r6812, %r6822;
	mov.b32 	%r6811, %r6822;
	mov.b32 	%r6810, %r6822;
	mov.b32 	%r6809, %r6822;
	mov.b32 	%r6808, %r6822;
	mov.b32 	%r6807, %r6822;
	mov.b32 	%r6806, %r6822;
	mov.b32 	%r6805, %r6822;
	mov.b32 	%r6804, %r6822;
	mov.b32 	%r6803, %r6822;
	mov.b32 	%r6802, %r6822;
	mov.b32 	%r6801, %r6822;
	mov.b32 	%r6800, %r6822;
	mov.b32 	%r6799, %r6822;
	mov.b32 	%r6798, %r6822;
	mov.b32 	%r6797, %r6822;
	mov.b32 	%r6796, %r6822;
	mov.b32 	%r6795, %r6822;
	mov.b32 	%r6794, %r6822;
	mov.b32 	%r6793, %r6822;
	mov.b32 	%r6792, %r6822;
	mov.b32 	%r6791, %r6822;
	mov.b32 	%r6790, %r6822;
	mov.b32 	%r6789, %r6822;
	mov.b32 	%r6788, %r6822;
	mov.b32 	%r6787, %r6822;
	mov.b32 	%r6786, %r6822;
	mov.b32 	%r6785, %r6822;
	mov.b32 	%r6784, %r6822;
	mov.b32 	%r6783, %r6822;
	mov.b32 	%r6782, %r6822;
	mov.b32 	%r6781, %r6822;
	mov.b32 	%r6780, %r6822;
	mov.b32 	%r6779, %r6822;
	mov.b32 	%r6778, %r6822;
	mov.b32 	%r6777, %r6822;
	mov.b32 	%r6776, %r6822;
	mov.b32 	%r6775, %r6822;
	mov.b32 	%r6774, %r6822;
	mov.b32 	%r6773, %r6822;
	mov.b32 	%r6772, %r6822;
	mov.b32 	%r6771, %r6822;
	mov.b32 	%r6770, %r6822;
	mov.b32 	%r6769, %r6822;
	mov.b32 	%r6768, %r6822;
	mov.b32 	%r6767, %r6822;
	mov.b32 	%r6766, %r6822;
	mov.b32 	%r6765, %r6822;
	mov.b32 	%r6764, %r6822;
	mov.b32 	%r6763, %r6822;
	mov.b32 	%r6762, %r6822;
	mov.b32 	%r6761, %r6822;
	mov.b32 	%r6760, %r6822;
	mov.b32 	%r6759, %r6822;
	mov.b32 	%r6886, %r6822;
	mov.b32 	%r6885, %r6822;
	mov.b32 	%r6884, %r6822;
	mov.b32 	%r6883, %r6822;
	mov.b32 	%r6882, %r6822;
	mov.b32 	%r6881, %r6822;
	mov.b32 	%r6880, %r6822;
	mov.b32 	%r6879, %r6822;
	mov.b32 	%r6878, %r6822;
	mov.b32 	%r6877, %r6822;
	mov.b32 	%r6876, %r6822;
	mov.b32 	%r6875, %r6822;
	mov.b32 	%r6874, %r6822;
	mov.b32 	%r6873, %r6822;
	mov.b32 	%r6872, %r6822;
	mov.b32 	%r6871, %r6822;
	mov.b32 	%r6870, %r6822;
	mov.b32 	%r6869, %r6822;
	mov.b32 	%r6868, %r6822;
	mov.b32 	%r6867, %r6822;
	mov.b32 	%r6866, %r6822;
	mov.b32 	%r6865, %r6822;
	mov.b32 	%r6864, %r6822;
	mov.b32 	%r6863, %r6822;
	mov.b32 	%r6862, %r6822;
	mov.b32 	%r6861, %r6822;
	mov.b32 	%r6860, %r6822;
	mov.b32 	%r6859, %r6822;
	mov.b32 	%r6858, %r6822;
	mov.b32 	%r6857, %r6822;
	mov.b32 	%r6856, %r6822;
	mov.b32 	%r6855, %r6822;
	mov.b32 	%r6854, %r6822;
	mov.b32 	%r6853, %r6822;
	mov.b32 	%r6852, %r6822;
	mov.b32 	%r6851, %r6822;
	mov.b32 	%r6850, %r6822;
	mov.b32 	%r6849, %r6822;
	mov.b32 	%r6848, %r6822;
	mov.b32 	%r6847, %r6822;
	mov.b32 	%r6846, %r6822;
	mov.b32 	%r6845, %r6822;
	mov.b32 	%r6844, %r6822;
	mov.b32 	%r6843, %r6822;
	mov.b32 	%r6842, %r6822;
	mov.b32 	%r6841, %r6822;
	mov.b32 	%r6840, %r6822;
	mov.b32 	%r6839, %r6822;
	mov.b32 	%r6838, %r6822;
	mov.b32 	%r6837, %r6822;
	mov.b32 	%r6836, %r6822;
	mov.b32 	%r6835, %r6822;
	mov.b32 	%r6834, %r6822;
	mov.b32 	%r6833, %r6822;
	mov.b32 	%r6832, %r6822;
	mov.b32 	%r6831, %r6822;
	mov.b32 	%r6830, %r6822;
	mov.b32 	%r6829, %r6822;
	mov.b32 	%r6828, %r6822;
	mov.b32 	%r6827, %r6822;
	mov.b32 	%r6826, %r6822;
	mov.b32 	%r6825, %r6822;
	mov.b32 	%r6824, %r6822;
	mov.b32 	%r6823, %r6822;
	mov.b32 	%r6753, %r1605;
$L__BB0_1:                              // %__nv_exp2f.exit873
                                        // =>This Inner Loop Header: Depth=1
	add.s32 	%r2050, %r27, %r6620;
	add.s32 	%r2051, %r2050, 8;
	add.s32 	%r2052, %r2050, 9;
	add.s32 	%r2053, %r2050, 1;
	setp.lt.u32 	%p37, %r6753, 4;
	add.s32 	%r2054, %r6621, 1;
	setp.gt.s32 	%p38, %r2054, 3;
	selp.b32 	%r6621, 0, %r2054, %p38;
	add.s32 	%r2055, %r6623, 1;
	setp.gt.s32 	%p39, %r2055, 4;
	selp.b32 	%r6623, 0, %r2055, %p39;
	cp.async.wait_group 	12;
	bar.sync 	0;
	shl.b32 	%r2056, %r6623, 11;
	add.s32 	%r1607, %r1414, %r2056;
	shl.b32 	%r2058, %r6621, 6;
	add.s32 	%r2059, %r1414, 20480;
	add.s32 	%r2060, %r2059, %r2058;
	add.s32 	%r2061, %r2060, %r19;
	ld.shared.v2.b32 	{%r2062, %r2063}, [%r2061];
	ld.shared.v2.b32 	{%r2064, %r2065}, [%r2061+32];
	shfl.sync.idx.b32 	%r2066, %r3, 0, 31, -1;
	wgmma.fence.sync.aligned;
	bfe.u32 	%r2067, %r1607, 4, 14;
	cvt.u64.u32 	%rd246, %r2067;
	or.b64 	%rd205, %rd246, 4611686293313683456;
	// begin inline asm
	wgmma.mma_async.sync.aligned.m64n16k16.f32.f16.f16 {%r1478,%r1479,%r1480,%r1481,%r1482,%r1483,%r1484,%r1485}, %rd574, %rd205, 0, 1, 1, 0, 0;
	// end inline asm
	add.s32 	%r2068, %r1607, 32;
	bfe.u32 	%r2069, %r2068, 4, 14;
	cvt.u64.u32 	%rd247, %r2069;
	or.b64 	%rd207, %rd247, 4611686293313683456;
	mov.pred 	%p17, -1;
	// begin inline asm
	wgmma.mma_async.sync.aligned.m64n16k16.f32.f16.f16 {%r1478,%r1479,%r1480,%r1481,%r1482,%r1483,%r1484,%r1485}, %rd576, %rd207, %p17, 1, 1, 0, 0;
	// end inline asm
	add.s32 	%r2070, %r1607, 64;
	bfe.u32 	%r2071, %r2070, 4, 14;
	cvt.u64.u32 	%rd248, %r2071;
	or.b64 	%rd209, %rd248, 4611686293313683456;
	// begin inline asm
	wgmma.mma_async.sync.aligned.m64n16k16.f32.f16.f16 {%r1478,%r1479,%r1480,%r1481,%r1482,%r1483,%r1484,%r1485}, %rd578, %rd209, %p17, 1, 1, 0, 0;
	// end inline asm
	add.s32 	%r2072, %r1607, 96;
	bfe.u32 	%r2073, %r2072, 4, 14;
	cvt.u64.u32 	%rd249, %r2073;
	or.b64 	%rd211, %rd249, 4611686293313683456;
	// begin inline asm
	wgmma.mma_async.sync.aligned.m64n16k16.f32.f16.f16 {%r1478,%r1479,%r1480,%r1481,%r1482,%r1483,%r1484,%r1485}, %rd580, %rd211, %p17, 1, 1, 0, 0;
	// end inline asm
	// begin inline asm
	wgmma.mma_async.sync.aligned.m64n16k16.f32.f16.f16 {%r1534,%r1535,%r1536,%r1537,%r1538,%r1539,%r1540,%r1541}, %rd582, %rd205, 0, 1, 1, 0, 0;
	// end inline asm
	// begin inline asm
	wgmma.mma_async.sync.aligned.m64n16k16.f32.f16.f16 {%r1534,%r1535,%r1536,%r1537,%r1538,%r1539,%r1540,%r1541}, %rd584, %rd207, %p17, 1, 1, 0, 0;
	// end inline asm
	// begin inline asm
	wgmma.mma_async.sync.aligned.m64n16k16.f32.f16.f16 {%r1534,%r1535,%r1536,%r1537,%r1538,%r1539,%r1540,%r1541}, %rd586, %rd209, %p17, 1, 1, 0, 0;
	// end inline asm
	// begin inline asm
	wgmma.mma_async.sync.aligned.m64n16k16.f32.f16.f16 {%r1534,%r1535,%r1536,%r1537,%r1538,%r1539,%r1540,%r1541}, %rd588, %rd211, %p17, 1, 1, 0, 0;
	// end inline asm
	wgmma.commit_group.sync.aligned;
	mov.b32 	%r1582, %r1415;
	mov.b32 	%r1583, %r1605;
	mov.b32 	%r1584, %r1605;
	mov.b32 	%r1586, %r1605;
	mov.b32 	%r1587, %r1605;
	mov.b32 	%r1585, %r1607;
	// begin inline asm
	// wait for regs: %r1478,%r1479,%r1480,%r1481,%r1482,%r1483,%r1484,%r1485,%r1534,%r1535,%r1536,%r1537,%r1538,%r1539,%r1540,%r1541,%r1582,%r1583,%r1584,%r1585,%r1586,%r1587
	wgmma.wait_group.sync.aligned 0;
	// end inline asm
	sub.f32 	%r2074, %r1478, %r2062;
	sub.f32 	%r2075, %r1479, %r2063;
	sub.f32 	%r2076, %r1480, %r2062;
	sub.f32 	%r2077, %r1481, %r2063;
	sub.f32 	%r2078, %r1482, %r2064;
	sub.f32 	%r2079, %r1483, %r2065;
	sub.f32 	%r2080, %r1484, %r2064;
	sub.f32 	%r2081, %r1485, %r2065;
	sub.f32 	%r2082, %r1534, %r2062;
	sub.f32 	%r2083, %r1535, %r2063;
	sub.f32 	%r2084, %r1536, %r2062;
	sub.f32 	%r2085, %r1537, %r2063;
	sub.f32 	%r2086, %r1538, %r2064;
	sub.f32 	%r2087, %r1539, %r2065;
	sub.f32 	%r2088, %r1540, %r2064;
	sub.f32 	%r2089, %r1541, %r2065;
	ex2.approx.ftz.f32 	%r2090, %r2074;
	ex2.approx.ftz.f32 	%r2091, %r2075;
	ex2.approx.ftz.f32 	%r2092, %r2076;
	ex2.approx.ftz.f32 	%r2093, %r2077;
	ex2.approx.ftz.f32 	%r2094, %r2078;
	ex2.approx.ftz.f32 	%r2095, %r2079;
	ex2.approx.ftz.f32 	%r2096, %r2080;
	ex2.approx.ftz.f32 	%r2097, %r2081;
	ex2.approx.ftz.f32 	%r2098, %r2082;
	ex2.approx.ftz.f32 	%r2099, %r2083;
	ex2.approx.ftz.f32 	%r2100, %r2084;
	ex2.approx.ftz.f32 	%r2101, %r2085;
	ex2.approx.ftz.f32 	%r2102, %r2086;
	ex2.approx.ftz.f32 	%r2103, %r2087;
	ex2.approx.ftz.f32 	%r2104, %r2088;
	ex2.approx.ftz.f32 	%r2105, %r2089;
	setp.lt.s32 	%p40, %r2050, %r8;
	setp.lt.s32 	%p41, %r2053, %r8;
	setp.lt.s32 	%p42, %r2050, %r9;
	setp.lt.s32 	%p43, %r2053, %r9;
	setp.lt.s32 	%p44, %r2051, %r8;
	setp.lt.s32 	%p45, %r2052, %r8;
	setp.lt.s32 	%p46, %r2051, %r9;
	setp.lt.s32 	%p47, %r2052, %r9;
	setp.lt.s32 	%p48, %r2050, %r10;
	setp.lt.s32 	%p49, %r2053, %r10;
	setp.lt.s32 	%p50, %r2050, %r11;
	setp.lt.s32 	%p51, %r2053, %r11;
	setp.lt.s32 	%p52, %r2051, %r10;
	setp.lt.s32 	%p53, %r2052, %r10;
	setp.lt.s32 	%p54, %r2051, %r11;
	setp.lt.s32 	%p55, %r2052, %r11;
	selp.f32 	%r2106, 0f00000000, %r2090, %p40;
	selp.f32 	%r2107, 0f00000000, %r2091, %p41;
	selp.f32 	%r2108, 0f00000000, %r2092, %p42;
	selp.f32 	%r2109, 0f00000000, %r2093, %p43;
	selp.f32 	%r2110, 0f00000000, %r2094, %p44;
	selp.f32 	%r2111, 0f00000000, %r2095, %p45;
	selp.f32 	%r2112, 0f00000000, %r2096, %p46;
	selp.f32 	%r2113, 0f00000000, %r2097, %p47;
	selp.f32 	%r2114, 0f00000000, %r2098, %p48;
	selp.f32 	%r2115, 0f00000000, %r2099, %p49;
	selp.f32 	%r2116, 0f00000000, %r2100, %p50;
	selp.f32 	%r2117, 0f00000000, %r2101, %p51;
	selp.f32 	%r2118, 0f00000000, %r2102, %p52;
	selp.f32 	%r2119, 0f00000000, %r2103, %p53;
	selp.f32 	%r2120, 0f00000000, %r2104, %p54;
	selp.f32 	%r2121, 0f00000000, %r2105, %p55;
	add.s32 	%r1877, %r1607, 10240;
	cvt.rn.f16x2.f32 	%r1674, %r2107, %r2106;
	cvt.rn.f16x2.f32 	%r1675, %r2109, %r2108;
	cvt.rn.f16x2.f32 	%r1676, %r2111, %r2110;
	cvt.rn.f16x2.f32 	%r1677, %r2113, %r2112;
	cvt.rn.f16x2.f32 	%r1742, %r2115, %r2114;
	cvt.rn.f16x2.f32 	%r1743, %r2117, %r2116;
	cvt.rn.f16x2.f32 	%r1744, %r2119, %r2118;
	cvt.rn.f16x2.f32 	%r1745, %r2121, %r2120;
	wgmma.fence.sync.aligned;
	bfe.u32 	%r2122, %r1877, 4, 14;
	cvt.u64.u32 	%rd250, %r2122;
	or.b64 	%rd220, %rd250, 4611686293313683456;
	// begin inline asm
	wgmma.mma_async.sync.aligned.m64n64k16.f32.f16.f16 {%r6822,%r6821,%r6820,%r6819,%r6818,%r6817,%r6816,%r6815,%r6814,%r6813,%r6812,%r6811,%r6810,%r6809,%r6808,%r6807,%r6806,%r6805,%r6804,%r6803,%r6802,%r6801,%r6800,%r6799,%r6798,%r6797,%r6796,%r6795,%r6794,%r6793,%r6792,%r6791}, {%r1674,%r1675,%r1676,%r1677}, %rd220, %p17, 1, 1, 1;
	// end inline asm
	// begin inline asm
	wgmma.mma_async.sync.aligned.m64n64k16.f32.f16.f16 {%r6790,%r6789,%r6788,%r6787,%r6786,%r6785,%r6784,%r6783,%r6782,%r6781,%r6780,%r6779,%r6778,%r6777,%r6776,%r6775,%r6774,%r6773,%r6772,%r6771,%r6770,%r6769,%r6768,%r6767,%r6766,%r6765,%r6764,%r6763,%r6762,%r6761,%r6760,%r6759}, {%r1742,%r1743,%r1744,%r1745}, %rd220, %p17, 1, 1, 1;
	// end inline asm
	wgmma.commit_group.sync.aligned;
	add.s32 	%r2123, %r1414, 20736;
	add.s32 	%r2124, %r2123, %r2058;
	add.s32 	%r2125, %r2124, %r19;
	ld.shared.v2.b32 	{%r2126, %r2127}, [%r2125];
	ld.shared.v2.b32 	{%r2128, %r2129}, [%r2125+32];
	wgmma.fence.sync.aligned;
	// begin inline asm
	wgmma.mma_async.sync.aligned.m64n16k16.f32.f16.f16 {%r1770,%r1771,%r1772,%r1773,%r1774,%r1775,%r1776,%r1777}, %rd590, %rd220, 0, 1, 1, 0, 0;
	// end inline asm
	add.s32 	%r2130, %r1607, 10272;
	bfe.u32 	%r2131, %r2130, 4, 14;
	cvt.u64.u32 	%rd251, %r2131;
	or.b64 	%rd225, %rd251, 4611686293313683456;
	// begin inline asm
	wgmma.mma_async.sync.aligned.m64n16k16.f32.f16.f16 {%r1770,%r1771,%r1772,%r1773,%r1774,%r1775,%r1776,%r1777}, %rd592, %rd225, %p17, 1, 1, 0, 0;
	// end inline asm
	add.s32 	%r2132, %r1607, 10304;
	bfe.u32 	%r2133, %r2132, 4, 14;
	cvt.u64.u32 	%rd252, %r2133;
	or.b64 	%rd227, %rd252, 4611686293313683456;
	// begin inline asm
	wgmma.mma_async.sync.aligned.m64n16k16.f32.f16.f16 {%r1770,%r1771,%r1772,%r1773,%r1774,%r1775,%r1776,%r1777}, %rd594, %rd227, %p17, 1, 1, 0, 0;
	// end inline asm
	add.s32 	%r2134, %r1607, 10336;
	bfe.u32 	%r2135, %r2134, 4, 14;
	cvt.u64.u32 	%rd253, %r2135;
	or.b64 	%rd229, %rd253, 4611686293313683456;
	// begin inline asm
	wgmma.mma_async.sync.aligned.m64n16k16.f32.f16.f16 {%r1770,%r1771,%r1772,%r1773,%r1774,%r1775,%r1776,%r1777}, %rd596, %rd229, %p17, 1, 1, 0, 0;
	// end inline asm
	// begin inline asm
	wgmma.mma_async.sync.aligned.m64n16k16.f32.f16.f16 {%r1826,%r1827,%r1828,%r1829,%r1830,%r1831,%r1832,%r1833}, %rd598, %rd220, 0, 1, 1, 0, 0;
	// end inline asm
	// begin inline asm
	wgmma.mma_async.sync.aligned.m64n16k16.f32.f16.f16 {%r1826,%r1827,%r1828,%r1829,%r1830,%r1831,%r1832,%r1833}, %rd600, %rd225, %p17, 1, 1, 0, 0;
	// end inline asm
	// begin inline asm
	wgmma.mma_async.sync.aligned.m64n16k16.f32.f16.f16 {%r1826,%r1827,%r1828,%r1829,%r1830,%r1831,%r1832,%r1833}, %rd602, %rd227, %p17, 1, 1, 0, 0;
	// end inline asm
	// begin inline asm
	wgmma.mma_async.sync.aligned.m64n16k16.f32.f16.f16 {%r1826,%r1827,%r1828,%r1829,%r1830,%r1831,%r1832,%r1833}, %rd604, %rd229, %p17, 1, 1, 0, 0;
	// end inline asm
	wgmma.commit_group.sync.aligned;
	mov.b32 	%r1879, %r1605;
	mov.b32 	%r1875, %r1605;
	mov.b32 	%r1876, %r1605;
	mov.b32 	%r1878, %r1605;
	mov.b32 	%r1874, %r1416;
	// begin inline asm
	// wait for regs: %r1770,%r1771,%r1772,%r1773,%r1774,%r1775,%r1776,%r1777,%r1826,%r1827,%r1828,%r1829,%r1830,%r1831,%r1832,%r1833,%r1874,%r1875,%r1876,%r1877,%r1878,%r1879
	wgmma.wait_group.sync.aligned 0;
	// end inline asm
	sub.f32 	%r2136, %r1770, %r2126;
	sub.f32 	%r2137, %r1771, %r2127;
	sub.f32 	%r2138, %r1772, %r2126;
	sub.f32 	%r2139, %r1773, %r2127;
	sub.f32 	%r2140, %r1774, %r2128;
	sub.f32 	%r2141, %r1775, %r2129;
	sub.f32 	%r2142, %r1776, %r2128;
	sub.f32 	%r2143, %r1777, %r2129;
	sub.f32 	%r2144, %r1826, %r2126;
	sub.f32 	%r2145, %r1827, %r2127;
	sub.f32 	%r2146, %r1828, %r2126;
	sub.f32 	%r2147, %r1829, %r2127;
	sub.f32 	%r2148, %r1830, %r2128;
	sub.f32 	%r2149, %r1831, %r2129;
	sub.f32 	%r2150, %r1832, %r2128;
	sub.f32 	%r2151, %r1833, %r2129;
	mul.f32 	%r2152, %r2106, %r2136;
	mul.f32 	%r2153, %r2107, %r2137;
	mul.f32 	%r2154, %r2108, %r2138;
	mul.f32 	%r2155, %r2109, %r2139;
	mul.f32 	%r2156, %r2110, %r2140;
	mul.f32 	%r2157, %r2111, %r2141;
	mul.f32 	%r2158, %r2112, %r2142;
	mul.f32 	%r2159, %r2113, %r2143;
	mul.f32 	%r2160, %r2114, %r2144;
	mul.f32 	%r2161, %r2115, %r2145;
	mul.f32 	%r2162, %r2116, %r2146;
	mul.f32 	%r2163, %r2117, %r2147;
	mul.f32 	%r2164, %r2118, %r2148;
	mul.f32 	%r2165, %r2119, %r2149;
	mul.f32 	%r2166, %r2120, %r2150;
	mul.f32 	%r2167, %r2121, %r2151;
	cvt.rn.f16x2.f32 	%r1966, %r2153, %r2152;
	cvt.rn.f16x2.f32 	%r1967, %r2155, %r2154;
	cvt.rn.f16x2.f32 	%r1968, %r2157, %r2156;
	cvt.rn.f16x2.f32 	%r1969, %r2159, %r2158;
	cvt.rn.f16x2.f32 	%r2034, %r2161, %r2160;
	cvt.rn.f16x2.f32 	%r2035, %r2163, %r2162;
	cvt.rn.f16x2.f32 	%r2036, %r2165, %r2164;
	cvt.rn.f16x2.f32 	%r2037, %r2167, %r2166;
	wgmma.fence.sync.aligned;
	// begin inline asm
	wgmma.mma_async.sync.aligned.m64n64k16.f32.f16.f16 {%r6886,%r6885,%r6884,%r6883,%r6882,%r6881,%r6880,%r6879,%r6878,%r6877,%r6876,%r6875,%r6874,%r6873,%r6872,%r6871,%r6870,%r6869,%r6868,%r6867,%r6866,%r6865,%r6864,%r6863,%r6862,%r6861,%r6860,%r6859,%r6858,%r6857,%r6856,%r6855}, {%r1966,%r1967,%r1968,%r1969}, %rd205, %p17, 1, 1, 1;
	// end inline asm
	// begin inline asm
	wgmma.mma_async.sync.aligned.m64n64k16.f32.f16.f16 {%r6854,%r6853,%r6852,%r6851,%r6850,%r6849,%r6848,%r6847,%r6846,%r6845,%r6844,%r6843,%r6842,%r6841,%r6840,%r6839,%r6838,%r6837,%r6836,%r6835,%r6834,%r6833,%r6832,%r6831,%r6830,%r6829,%r6828,%r6827,%r6826,%r6825,%r6824,%r6823}, {%r2034,%r2035,%r2036,%r2037}, %rd205, %p17, 1, 1, 1;
	// end inline asm
	wgmma.commit_group.sync.aligned;
	add.s32 	%r2168, %r1, %r6620;
	add.s32 	%r2169, %r2168, 64;
	add.s64 	%rd240, %rd85, %rd652;
	add.s64 	%rd243, %rd88, %rd652;
	add.s32 	%r2170, %r6622, 1;
	setp.gt.s32 	%p56, %r2170, 3;
	selp.b32 	%r6622, 0, %r2170, %p56;
	add.s32 	%r2171, %r6624, 1;
	setp.gt.s32 	%p57, %r2171, 4;
	selp.b32 	%r6624, 0, %r2171, %p57;
	bar.sync 	0;
	shl.b32 	%r2172, %r6624, 11;
	add.s32 	%r2038, %r5245, %r2172;
	selp.b32 	%r2039, 16, 0, %p37;
	// begin inline asm
	cp.async.cg.shared.global [ %r2038 + 0 ], [ %rd240 + 0 ], 0x10, %r2039;
	// end inline asm
	cp.async.commit_group;
	add.s32 	%r2173, %r2050, 64;
	mul.wide.s32 	%rd254, %r2173, 4;
	add.s64 	%rd241, %rd6, %rd254;
	cvt.s64.s32 	%rd255, %r2169;
	add.s64 	%rd256, %rd255, %rd21;
	shl.b64 	%rd257, %rd256, 2;
	add.s64 	%rd258, %rd6, %rd257;
	add.s64 	%rd242, %rd258, 32;
	shl.b32 	%r2174, %r6622, 6;
	add.s32 	%r2175, %r2059, %r2174;
	add.s32 	%r2040, %r2175, %r19;
	selp.b32 	%r2041, 8, 0, %p37;
	// begin inline asm
	@%p1 cp.async.ca.shared.global [ %r2040 + 0 ], [ %rd241 + 0 ], 0x8, %r2041;
	// end inline asm
	add.s32 	%r2042, %r2040, 32;
	// begin inline asm
	@%p1 cp.async.ca.shared.global [ %r2042 + 0 ], [ %rd242 + 0 ], 0x8, %r2041;
	// end inline asm
	cp.async.commit_group;
	add.s32 	%r2044, %r1336, %r2172;
	// begin inline asm
	cp.async.cg.shared.global [ %r2044 + 0 ], [ %rd243 + 0 ], 0x10, %r2039;
	// end inline asm
	cp.async.commit_group;
	add.s64 	%rd244, %rd7, %rd254;
	add.s64 	%rd259, %rd7, %rd257;
	add.s64 	%rd245, %rd259, 32;
	add.s32 	%r2176, %r2123, %r2174;
	add.s32 	%r2046, %r2176, %r19;
	// begin inline asm
	@%p1 cp.async.ca.shared.global [ %r2046 + 0 ], [ %rd244 + 0 ], 0x8, %r2041;
	// end inline asm
	add.s32 	%r2048, %r2046, 32;
	// begin inline asm
	@%p1 cp.async.ca.shared.global [ %r2048 + 0 ], [ %rd245 + 0 ], 0x8, %r2041;
	// end inline asm
	cp.async.commit_group;
	add.s32 	%r6753, %r6753, 1;
	add.s32 	%r6620, %r6620, 16;
	add.s64 	%rd652, %rd652, %rd41;
	setp.ne.b32 	%p58, %r6620, 128;
	@%p58 bra 	$L__BB0_1;
// %bb.2:
	cvt.u32.u64 	%r2529, %rd21;
	shl.b64 	%rd308, %rd1, 1;
	add.s64 	%rd44, %rd90, %rd308;
	add.s64 	%rd45, %rd91, %rd308;
	or.b32 	%r297, %r1, %r7;
	// begin inline asm
	// wait for regs: %r6822,%r6821,%r6820,%r6819,%r6818,%r6817,%r6816,%r6815,%r6814,%r6813,%r6812,%r6811,%r6810,%r6809,%r6808,%r6807,%r6806,%r6805,%r6804,%r6803,%r6802,%r6801,%r6800,%r6799,%r6798,%r6797,%r6796,%r6795,%r6794,%r6793,%r6792,%r6791,%r6790,%r6789,%r6788,%r6787,%r6786,%r6785,%r6784,%r6783,%r6782,%r6781,%r6780,%r6779,%r6778,%r6777,%r6776,%r6775,%r6774,%r6773,%r6772,%r6771,%r6770,%r6769,%r6768,%r6767,%r6766,%r6765,%r6764,%r6763,%r6762,%r6761,%r6760,%r6759,%r6886,%r6885,%r6884,%r6883,%r6882,%r6881,%r6880,%r6879,%r6878,%r6877,%r6876,%r6875,%r6874,%r6873,%r6872,%r6871,%r6870,%r6869,%r6868,%r6867,%r6866,%r6865,%r6864,%r6863,%r6862,%r6861,%r6860,%r6859,%r6858,%r6857,%r6856,%r6855,%r6854,%r6853,%r6852,%r6851,%r6850,%r6849,%r6848,%r6847,%r6846,%r6845,%r6844,%r6843,%r6842,%r6841,%r6840,%r6839,%r6838,%r6837,%r6836,%r6835,%r6834,%r6833,%r6832,%r6831,%r6830,%r6829,%r6828,%r6827,%r6826,%r6825,%r6824,%r6823
	wgmma.wait_group.sync.aligned 0;
	// end inline asm
	cp.async.wait_group 	0;
	bar.sync 	0;
	add.s32 	%r2530, %r1, 128;
	sub.s32 	%r2531, %r1265, %r2530;
	or.b32 	%r2535, %r2530, %r4;
	or.b32 	%r2536, %r5, %r2530;
	mul.lo.s32 	%r2537, %r1264, %r2535;
	mul.lo.s32 	%r2538, %r1264, %r2536;
	mul.wide.s32 	%rd309, %r2537, 2;
	add.s64 	%rd310, %rd2, %rd309;
	mul.wide.s32 	%rd311, %r2538, 2;
	add.s64 	%rd312, %rd2, %rd311;
	shl.b64 	%rd313, %rd16, 1;
	add.s64 	%rd260, %rd310, %rd313;
	add.s64 	%rd261, %rd312, %rd313;
	add.s64 	%rd314, %rd5, %rd309;
	add.s64 	%rd315, %rd5, %rd311;
	add.s64 	%rd266, %rd314, %rd313;
	add.s64 	%rd267, %rd315, %rd313;
	setp.lt.s32 	%p91, %r2531, 32;
	setp.gt.s32 	%p92, %r2531, 31;
	selp.b32 	%r2434, 16, 0, %p92;
	// begin inline asm
	cp.async.cg.shared.global [ %r5245 + 0 ], [ %rd260 + 0 ], 0x10, %r2434;
	// end inline asm
	// begin inline asm
	cp.async.cg.shared.global [ %r1342 + 0 ], [ %rd261 + 0 ], 0x10, %r2434;
	// end inline asm
	cp.async.commit_group;
	or.b32 	%r2539, %r2530, %r2529;
	mul.wide.s32 	%rd316, %r2539, 4;
	add.s64 	%rd262, %rd6, %rd316;
	cvt.s64.s32 	%rd317, %r2530;
	add.s64 	%rd318, %rd21, %rd317;
	shl.b64 	%rd319, %rd318, 2;
	add.s64 	%rd320, %rd6, %rd319;
	add.s64 	%rd263, %rd320, 32;
	add.s64 	%rd264, %rd320, 64;
	add.s64 	%rd265, %rd320, 96;
	add.s32 	%r2437, %r1419, 73728;
	selp.b32 	%r2438, 8, 0, %p92;
	// begin inline asm
	@%p1 cp.async.ca.shared.global [ %r2437 + 0 ], [ %rd262 + 0 ], 0x8, %r2438;
	// end inline asm
	add.s32 	%r2439, %r1419, 73760;
	// begin inline asm
	@%p1 cp.async.ca.shared.global [ %r2439 + 0 ], [ %rd263 + 0 ], 0x8, %r2438;
	// end inline asm
	add.s32 	%r2441, %r1419, 73792;
	// begin inline asm
	@%p1 cp.async.ca.shared.global [ %r2441 + 0 ], [ %rd264 + 0 ], 0x8, %r2438;
	// end inline asm
	add.s32 	%r2443, %r1419, 73824;
	// begin inline asm
	@%p1 cp.async.ca.shared.global [ %r2443 + 0 ], [ %rd265 + 0 ], 0x8, %r2438;
	// end inline asm
	cp.async.commit_group;
	add.s32 	%r5249, %r5245, 20480;
	// begin inline asm
	cp.async.cg.shared.global [ %r5249 + 0 ], [ %rd266 + 0 ], 0x10, %r2434;
	// end inline asm
	add.s32 	%r5251, %r5245, 22528;
	// begin inline asm
	cp.async.cg.shared.global [ %r5251 + 0 ], [ %rd267 + 0 ], 0x10, %r2434;
	// end inline asm
	cp.async.commit_group;
	add.s64 	%rd268, %rd7, %rd316;
	add.s64 	%rd321, %rd7, %rd319;
	add.s64 	%rd269, %rd321, 32;
	add.s64 	%rd270, %rd321, 64;
	add.s64 	%rd271, %rd321, 96;
	add.s32 	%r2449, %r1419, 74240;
	// begin inline asm
	@%p1 cp.async.ca.shared.global [ %r2449 + 0 ], [ %rd268 + 0 ], 0x8, %r2438;
	// end inline asm
	add.s32 	%r2451, %r1419, 74272;
	// begin inline asm
	@%p1 cp.async.ca.shared.global [ %r2451 + 0 ], [ %rd269 + 0 ], 0x8, %r2438;
	// end inline asm
	add.s32 	%r2453, %r1419, 74304;
	// begin inline asm
	@%p1 cp.async.ca.shared.global [ %r2453 + 0 ], [ %rd270 + 0 ], 0x8, %r2438;
	// end inline asm
	add.s32 	%r2455, %r1419, 74336;
	// begin inline asm
	@%p1 cp.async.ca.shared.global [ %r2455 + 0 ], [ %rd271 + 0 ], 0x8, %r2438;
	// end inline asm
	cp.async.commit_group;
	setp.gt.s32 	%p93, %r2531, 63;
	add.s32 	%r2543, %r1, 160;
	cvt.s64.s32 	%rd46, %r12;
	mul.wide.s32 	%rd322, %r12, 2;
	add.s64 	%rd272, %rd260, %rd322;
	add.s64 	%rd273, %rd261, %rd322;
	add.s64 	%rd278, %rd266, %rd322;
	add.s64 	%rd279, %rd267, %rd322;
	bar.sync 	0;
	selp.b32 	%r2458, 16, 0, %p93;
	// begin inline asm
	cp.async.cg.shared.global [ %r1354 + 0 ], [ %rd272 + 0 ], 0x10, %r2458;
	// end inline asm
	add.s32 	%r5255, %r1354, 2048;
	// begin inline asm
	cp.async.cg.shared.global [ %r5255 + 0 ], [ %rd273 + 0 ], 0x10, %r2458;
	// end inline asm
	cp.async.commit_group;
	or.b32 	%r2544, %r2543, %r2529;
	mul.wide.s32 	%rd323, %r2544, 4;
	add.s64 	%rd274, %rd6, %rd323;
	cvt.s64.s32 	%rd324, %r2543;
	add.s64 	%rd325, %rd21, %rd324;
	shl.b64 	%rd326, %rd325, 2;
	add.s64 	%rd327, %rd6, %rd326;
	add.s64 	%rd275, %rd327, 32;
	add.s64 	%rd276, %rd327, 64;
	add.s64 	%rd277, %rd327, 96;
	add.s32 	%r2461, %r1419, 73856;
	selp.b32 	%r2462, 8, 0, %p93;
	// begin inline asm
	@%p1 cp.async.ca.shared.global [ %r2461 + 0 ], [ %rd274 + 0 ], 0x8, %r2462;
	// end inline asm
	add.s32 	%r2463, %r1419, 73888;
	// begin inline asm
	@%p1 cp.async.ca.shared.global [ %r2463 + 0 ], [ %rd275 + 0 ], 0x8, %r2462;
	// end inline asm
	add.s32 	%r2465, %r1419, 73920;
	// begin inline asm
	@%p1 cp.async.ca.shared.global [ %r2465 + 0 ], [ %rd276 + 0 ], 0x8, %r2462;
	// end inline asm
	add.s32 	%r2467, %r1419, 73952;
	// begin inline asm
	@%p1 cp.async.ca.shared.global [ %r2467 + 0 ], [ %rd277 + 0 ], 0x8, %r2462;
	// end inline asm
	cp.async.commit_group;
	add.s32 	%r5257, %r5245, 24576;
	// begin inline asm
	cp.async.cg.shared.global [ %r5257 + 0 ], [ %rd278 + 0 ], 0x10, %r2458;
	// end inline asm
	add.s32 	%r5259, %r5245, 26624;
	// begin inline asm
	cp.async.cg.shared.global [ %r5259 + 0 ], [ %rd279 + 0 ], 0x10, %r2458;
	// end inline asm
	cp.async.commit_group;
	add.s64 	%rd280, %rd7, %rd323;
	add.s64 	%rd328, %rd7, %rd326;
	add.s64 	%rd281, %rd328, 32;
	add.s64 	%rd282, %rd328, 64;
	add.s64 	%rd283, %rd328, 96;
	add.s32 	%r2473, %r1419, 74368;
	// begin inline asm
	@%p1 cp.async.ca.shared.global [ %r2473 + 0 ], [ %rd280 + 0 ], 0x8, %r2462;
	// end inline asm
	add.s32 	%r2475, %r1419, 74400;
	// begin inline asm
	@%p1 cp.async.ca.shared.global [ %r2475 + 0 ], [ %rd281 + 0 ], 0x8, %r2462;
	// end inline asm
	add.s32 	%r2477, %r1419, 74432;
	// begin inline asm
	@%p1 cp.async.ca.shared.global [ %r2477 + 0 ], [ %rd282 + 0 ], 0x8, %r2462;
	// end inline asm
	add.s32 	%r2479, %r1419, 74464;
	// begin inline asm
	@%p1 cp.async.ca.shared.global [ %r2479 + 0 ], [ %rd283 + 0 ], 0x8, %r2462;
	// end inline asm
	cp.async.commit_group;
	setp.gt.s32 	%p94, %r2531, 95;
	add.s32 	%r2545, %r1, 192;
	add.s64 	%rd284, %rd272, %rd322;
	add.s64 	%rd285, %rd273, %rd322;
	add.s64 	%rd290, %rd278, %rd322;
	add.s64 	%rd291, %rd279, %rd322;
	bar.sync 	0;
	add.s32 	%r5261, %r5245, 8192;
	selp.b32 	%r2482, 16, 0, %p94;
	// begin inline asm
	cp.async.cg.shared.global [ %r5261 + 0 ], [ %rd284 + 0 ], 0x10, %r2482;
	// end inline asm
	// begin inline asm
	cp.async.cg.shared.global [ %r1336 + 0 ], [ %rd285 + 0 ], 0x10, %r2482;
	// end inline asm
	cp.async.commit_group;
	or.b32 	%r2546, %r2545, %r2529;
	mul.wide.s32 	%rd329, %r2546, 4;
	add.s64 	%rd286, %rd6, %rd329;
	cvt.s64.s32 	%rd330, %r2545;
	add.s64 	%rd331, %rd21, %rd330;
	shl.b64 	%rd332, %rd331, 2;
	add.s64 	%rd333, %rd6, %rd332;
	add.s64 	%rd287, %rd333, 32;
	add.s64 	%rd288, %rd333, 64;
	add.s64 	%rd289, %rd333, 96;
	add.s32 	%r2485, %r1419, 73984;
	selp.b32 	%r2486, 8, 0, %p94;
	// begin inline asm
	@%p1 cp.async.ca.shared.global [ %r2485 + 0 ], [ %rd286 + 0 ], 0x8, %r2486;
	// end inline asm
	add.s32 	%r2487, %r1419, 74016;
	// begin inline asm
	@%p1 cp.async.ca.shared.global [ %r2487 + 0 ], [ %rd287 + 0 ], 0x8, %r2486;
	// end inline asm
	add.s32 	%r2489, %r1419, 74048;
	// begin inline asm
	@%p1 cp.async.ca.shared.global [ %r2489 + 0 ], [ %rd288 + 0 ], 0x8, %r2486;
	// end inline asm
	add.s32 	%r2491, %r1419, 74080;
	// begin inline asm
	@%p1 cp.async.ca.shared.global [ %r2491 + 0 ], [ %rd289 + 0 ], 0x8, %r2486;
	// end inline asm
	cp.async.commit_group;
	add.s32 	%r5265, %r5245, 28672;
	// begin inline asm
	cp.async.cg.shared.global [ %r5265 + 0 ], [ %rd290 + 0 ], 0x10, %r2482;
	// end inline asm
	add.s32 	%r5267, %r5245, 30720;
	// begin inline asm
	cp.async.cg.shared.global [ %r5267 + 0 ], [ %rd291 + 0 ], 0x10, %r2482;
	// end inline asm
	cp.async.commit_group;
	add.s64 	%rd292, %rd7, %rd329;
	add.s64 	%rd334, %rd7, %rd332;
	add.s64 	%rd293, %rd334, 32;
	add.s64 	%rd294, %rd334, 64;
	add.s64 	%rd295, %rd334, 96;
	add.s32 	%r2497, %r1419, 74496;
	// begin inline asm
	@%p1 cp.async.ca.shared.global [ %r2497 + 0 ], [ %rd292 + 0 ], 0x8, %r2486;
	// end inline asm
	add.s32 	%r2499, %r1419, 74528;
	// begin inline asm
	@%p1 cp.async.ca.shared.global [ %r2499 + 0 ], [ %rd293 + 0 ], 0x8, %r2486;
	// end inline asm
	add.s32 	%r2501, %r1419, 74560;
	// begin inline asm
	@%p1 cp.async.ca.shared.global [ %r2501 + 0 ], [ %rd294 + 0 ], 0x8, %r2486;
	// end inline asm
	add.s32 	%r2503, %r1419, 74592;
	// begin inline asm
	@%p1 cp.async.ca.shared.global [ %r2503 + 0 ], [ %rd295 + 0 ], 0x8, %r2486;
	// end inline asm
	cp.async.commit_group;
	setp.gt.s32 	%p95, %r2531, 127;
	add.s32 	%r2547, %r1, 224;
	add.s64 	%rd296, %rd284, %rd322;
	add.s64 	%rd297, %rd285, %rd322;
	add.s64 	%rd302, %rd290, %rd322;
	add.s64 	%rd303, %rd291, %rd322;
	bar.sync 	0;
	selp.b32 	%r2506, 16, 0, %p95;
	// begin inline asm
	cp.async.cg.shared.global [ %r1348 + 0 ], [ %rd296 + 0 ], 0x10, %r2506;
	// end inline asm
	add.s32 	%r5271, %r1348, 2048;
	// begin inline asm
	cp.async.cg.shared.global [ %r5271 + 0 ], [ %rd297 + 0 ], 0x10, %r2506;
	// end inline asm
	cp.async.commit_group;
	or.b32 	%r2548, %r2547, %r2529;
	mul.wide.s32 	%rd335, %r2548, 4;
	add.s64 	%rd298, %rd6, %rd335;
	cvt.s64.s32 	%rd336, %r2547;
	add.s64 	%rd337, %rd21, %rd336;
	shl.b64 	%rd338, %rd337, 2;
	add.s64 	%rd339, %rd6, %rd338;
	add.s64 	%rd299, %rd339, 32;
	add.s64 	%rd300, %rd339, 64;
	add.s64 	%rd301, %rd339, 96;
	add.s32 	%r2509, %r1419, 74112;
	selp.b32 	%r2510, 8, 0, %p95;
	// begin inline asm
	@%p1 cp.async.ca.shared.global [ %r2509 + 0 ], [ %rd298 + 0 ], 0x8, %r2510;
	// end inline asm
	add.s32 	%r2511, %r1419, 74144;
	// begin inline asm
	@%p1 cp.async.ca.shared.global [ %r2511 + 0 ], [ %rd299 + 0 ], 0x8, %r2510;
	// end inline asm
	add.s32 	%r2513, %r1419, 74176;
	// begin inline asm
	@%p1 cp.async.ca.shared.global [ %r2513 + 0 ], [ %rd300 + 0 ], 0x8, %r2510;
	// end inline asm
	add.s32 	%r2515, %r1419, 74208;
	// begin inline asm
	@%p1 cp.async.ca.shared.global [ %r2515 + 0 ], [ %rd301 + 0 ], 0x8, %r2510;
	// end inline asm
	cp.async.commit_group;
	add.s32 	%r5273, %r5245, 32768;
	// begin inline asm
	cp.async.cg.shared.global [ %r5273 + 0 ], [ %rd302 + 0 ], 0x10, %r2506;
	// end inline asm
	add.s32 	%r5275, %r5245, 34816;
	// begin inline asm
	cp.async.cg.shared.global [ %r5275 + 0 ], [ %rd303 + 0 ], 0x10, %r2506;
	// end inline asm
	cp.async.commit_group;
	add.s64 	%rd304, %rd7, %rd335;
	add.s64 	%rd340, %rd7, %rd338;
	add.s64 	%rd305, %rd340, 32;
	add.s64 	%rd306, %rd340, 64;
	add.s64 	%rd307, %rd340, 96;
	add.s32 	%r2521, %r1419, 74624;
	// begin inline asm
	@%p1 cp.async.ca.shared.global [ %r2521 + 0 ], [ %rd304 + 0 ], 0x8, %r2510;
	// end inline asm
	add.s32 	%r2523, %r1419, 74656;
	// begin inline asm
	@%p1 cp.async.ca.shared.global [ %r2523 + 0 ], [ %rd305 + 0 ], 0x8, %r2510;
	// end inline asm
	add.s32 	%r2525, %r1419, 74688;
	// begin inline asm
	@%p1 cp.async.ca.shared.global [ %r2525 + 0 ], [ %rd306 + 0 ], 0x8, %r2510;
	// end inline asm
	add.s32 	%r2527, %r1419, 74720;
	// begin inline asm
	@%p1 cp.async.ca.shared.global [ %r2527 + 0 ], [ %rd307 + 0 ], 0x8, %r2510;
	// end inline asm
	cp.async.commit_group;
	// begin inline asm
	fence.proxy.async.shared::cta;
	// end inline asm
	shl.b64 	%rd650, %rd46, 3;
	shl.b64 	%rd651, %rd46, 1;
	@%p91 bra 	$L__BB0_5;
// %bb.3:                               // %.lr.ph
	shr.s32 	%r2532, %r2531, 31;
	shr.u32 	%r2533, %r2532, 27;
	add.s32 	%r2534, %r2531, %r2533;
	shr.s32 	%r426, %r2534, 5;
	add.s32 	%r440, %r426, -4;
	add.s32 	%r2552, %r28, 144;
	mul.lo.s32 	%r2553, %r1264, %r2552;
	cvt.s64.s32 	%rd341, %r2553;
	add.s64 	%rd342, %rd341, %rd1;
	shl.b64 	%rd343, %rd342, 1;
	add.s64 	%rd344, %rd39, %rd343;
	add.s64 	%rd656, %rd88, %rd344;
	add.s32 	%r2554, %r28, 128;
	mul.lo.s32 	%r2555, %r1264, %r2554;
	cvt.s64.s32 	%rd345, %r2555;
	add.s64 	%rd346, %rd345, %rd1;
	shl.b64 	%rd347, %rd346, 1;
	add.s64 	%rd348, %rd39, %rd347;
	add.s64 	%rd655, %rd88, %rd348;
	add.s64 	%rd654, %rd85, %rd344;
	add.s64 	%rd653, %rd85, %rd348;
	add.s32 	%r6754, %r1, 256;
	mov.b32 	%r2851, 0;
	mov.b32 	%r6756, 3;
	mov.b32 	%r6755, -1;
	mov.b32 	%r6757, %r6755;
	mov.b32 	%r6758, %r6756;
	mov.b32 	%r6887, %r2851;
$L__BB0_4:                              // %__nv_exp2f.exit777
                                        // =>This Inner Loop Header: Depth=1
	setp.lt.s32 	%p124, %r6887, %r440;
	add.s32 	%r3725, %r6755, 1;
	setp.gt.s32 	%p125, %r3725, 3;
	selp.b32 	%r6755, 0, %r3725, %p125;
	add.s32 	%r3726, %r6757, 1;
	setp.gt.s32 	%p126, %r3726, 4;
	selp.b32 	%r6757, 0, %r3726, %p126;
	cp.async.wait_group 	12;
	bar.sync 	0;
	shl.b32 	%r3727, %r6757, 12;
	add.s32 	%r2853, %r1414, %r3727;
	shl.b32 	%r3729, %r6755, 7;
	add.s32 	%r3730, %r1414, 73728;
	add.s32 	%r3731, %r3730, %r3729;
	add.s32 	%r3732, %r3731, %r19;
	ld.shared.v2.b32 	{%r3733, %r3734}, [%r3732];
	ld.shared.v2.b32 	{%r3735, %r3736}, [%r3732+32];
	ld.shared.v2.b32 	{%r3737, %r3738}, [%r3732+64];
	ld.shared.v2.b32 	{%r3739, %r3740}, [%r3732+96];
	shfl.sync.idx.b32 	%r3741, %r3, 0, 31, -1;
	wgmma.fence.sync.aligned;
	bfe.u32 	%r3742, %r2853, 4, 14;
	cvt.u64.u32 	%rd401, %r3742;
	or.b64 	%rd350, %rd401, 4611686293322072064;
	// begin inline asm
	wgmma.mma_async.sync.aligned.m64n32k16.f32.f16.f16 {%r2604,%r2605,%r2606,%r2607,%r2608,%r2609,%r2610,%r2611,%r2612,%r2613,%r2614,%r2615,%r2616,%r2617,%r2618,%r2619}, %rd574, %rd350, 0, 1, 1, 0, 0;
	// end inline asm
	add.s32 	%r3743, %r2853, 32;
	bfe.u32 	%r3744, %r3743, 4, 14;
	cvt.u64.u32 	%rd402, %r3744;
	or.b64 	%rd352, %rd402, 4611686293322072064;
	mov.pred 	%p96, -1;
	// begin inline asm
	wgmma.mma_async.sync.aligned.m64n32k16.f32.f16.f16 {%r2604,%r2605,%r2606,%r2607,%r2608,%r2609,%r2610,%r2611,%r2612,%r2613,%r2614,%r2615,%r2616,%r2617,%r2618,%r2619}, %rd576, %rd352, %p96, 1, 1, 0, 0;
	// end inline asm
	add.s32 	%r3745, %r2853, 64;
	bfe.u32 	%r3746, %r3745, 4, 14;
	cvt.u64.u32 	%rd403, %r3746;
	or.b64 	%rd354, %rd403, 4611686293322072064;
	// begin inline asm
	wgmma.mma_async.sync.aligned.m64n32k16.f32.f16.f16 {%r2604,%r2605,%r2606,%r2607,%r2608,%r2609,%r2610,%r2611,%r2612,%r2613,%r2614,%r2615,%r2616,%r2617,%r2618,%r2619}, %rd578, %rd354, %p96, 1, 1, 0, 0;
	// end inline asm
	add.s32 	%r3747, %r2853, 96;
	bfe.u32 	%r3748, %r3747, 4, 14;
	cvt.u64.u32 	%rd404, %r3748;
	or.b64 	%rd356, %rd404, 4611686293322072064;
	// begin inline asm
	wgmma.mma_async.sync.aligned.m64n32k16.f32.f16.f16 {%r2604,%r2605,%r2606,%r2607,%r2608,%r2609,%r2610,%r2611,%r2612,%r2613,%r2614,%r2615,%r2616,%r2617,%r2618,%r2619}, %rd580, %rd356, %p96, 1, 1, 0, 0;
	// end inline asm
	// begin inline asm
	wgmma.mma_async.sync.aligned.m64n32k16.f32.f16.f16 {%r2716,%r2717,%r2718,%r2719,%r2720,%r2721,%r2722,%r2723,%r2724,%r2725,%r2726,%r2727,%r2728,%r2729,%r2730,%r2731}, %rd582, %rd350, 0, 1, 1, 0, 0;
	// end inline asm
	// begin inline asm
	wgmma.mma_async.sync.aligned.m64n32k16.f32.f16.f16 {%r2716,%r2717,%r2718,%r2719,%r2720,%r2721,%r2722,%r2723,%r2724,%r2725,%r2726,%r2727,%r2728,%r2729,%r2730,%r2731}, %rd584, %rd352, %p96, 1, 1, 0, 0;
	// end inline asm
	// begin inline asm
	wgmma.mma_async.sync.aligned.m64n32k16.f32.f16.f16 {%r2716,%r2717,%r2718,%r2719,%r2720,%r2721,%r2722,%r2723,%r2724,%r2725,%r2726,%r2727,%r2728,%r2729,%r2730,%r2731}, %rd586, %rd354, %p96, 1, 1, 0, 0;
	// end inline asm
	// begin inline asm
	wgmma.mma_async.sync.aligned.m64n32k16.f32.f16.f16 {%r2716,%r2717,%r2718,%r2719,%r2720,%r2721,%r2722,%r2723,%r2724,%r2725,%r2726,%r2727,%r2728,%r2729,%r2730,%r2731}, %rd588, %rd356, %p96, 1, 1, 0, 0;
	// end inline asm
	wgmma.commit_group.sync.aligned;
	mov.b32 	%r2813, %r2851;
	mov.b32 	%r2814, %r2851;
	mov.b32 	%r2816, %r2851;
	mov.b32 	%r2817, %r2851;
	mov.b32 	%r2812, %r1415;
	mov.b32 	%r2815, %r2853;
	// begin inline asm
	// wait for regs: %r2604,%r2605,%r2606,%r2607,%r2608,%r2609,%r2610,%r2611,%r2612,%r2613,%r2614,%r2615,%r2616,%r2617,%r2618,%r2619,%r2716,%r2717,%r2718,%r2719,%r2720,%r2721,%r2722,%r2723,%r2724,%r2725,%r2726,%r2727,%r2728,%r2729,%r2730,%r2731,%r2812,%r2813,%r2814,%r2815,%r2816,%r2817
	wgmma.wait_group.sync.aligned 0;
	// end inline asm
	sub.f32 	%r3749, %r2604, %r3733;
	sub.f32 	%r3750, %r2605, %r3734;
	sub.f32 	%r3751, %r2606, %r3733;
	sub.f32 	%r3752, %r2607, %r3734;
	sub.f32 	%r3753, %r2608, %r3735;
	sub.f32 	%r3754, %r2609, %r3736;
	sub.f32 	%r3755, %r2610, %r3735;
	sub.f32 	%r3756, %r2611, %r3736;
	sub.f32 	%r3757, %r2612, %r3737;
	sub.f32 	%r3758, %r2613, %r3738;
	sub.f32 	%r3759, %r2614, %r3737;
	sub.f32 	%r3760, %r2615, %r3738;
	sub.f32 	%r3761, %r2616, %r3739;
	sub.f32 	%r3762, %r2617, %r3740;
	sub.f32 	%r3763, %r2618, %r3739;
	sub.f32 	%r3764, %r2619, %r3740;
	sub.f32 	%r3765, %r2716, %r3733;
	sub.f32 	%r3766, %r2717, %r3734;
	sub.f32 	%r3767, %r2718, %r3733;
	sub.f32 	%r3768, %r2719, %r3734;
	sub.f32 	%r3769, %r2720, %r3735;
	sub.f32 	%r3770, %r2721, %r3736;
	sub.f32 	%r3771, %r2722, %r3735;
	sub.f32 	%r3772, %r2723, %r3736;
	sub.f32 	%r3773, %r2724, %r3737;
	sub.f32 	%r3774, %r2725, %r3738;
	sub.f32 	%r3775, %r2726, %r3737;
	sub.f32 	%r3776, %r2727, %r3738;
	sub.f32 	%r3777, %r2728, %r3739;
	sub.f32 	%r3778, %r2729, %r3740;
	sub.f32 	%r3779, %r2730, %r3739;
	sub.f32 	%r3780, %r2731, %r3740;
	ex2.approx.ftz.f32 	%r3781, %r3749;
	ex2.approx.ftz.f32 	%r3782, %r3750;
	ex2.approx.ftz.f32 	%r3783, %r3751;
	ex2.approx.ftz.f32 	%r3784, %r3752;
	ex2.approx.ftz.f32 	%r3785, %r3753;
	ex2.approx.ftz.f32 	%r3786, %r3754;
	ex2.approx.ftz.f32 	%r3787, %r3755;
	ex2.approx.ftz.f32 	%r3788, %r3756;
	ex2.approx.ftz.f32 	%r3789, %r3757;
	ex2.approx.ftz.f32 	%r3790, %r3758;
	ex2.approx.ftz.f32 	%r3791, %r3759;
	ex2.approx.ftz.f32 	%r3792, %r3760;
	ex2.approx.ftz.f32 	%r3793, %r3761;
	ex2.approx.ftz.f32 	%r3794, %r3762;
	ex2.approx.ftz.f32 	%r3795, %r3763;
	ex2.approx.ftz.f32 	%r3796, %r3764;
	ex2.approx.ftz.f32 	%r3797, %r3765;
	ex2.approx.ftz.f32 	%r3798, %r3766;
	ex2.approx.ftz.f32 	%r3799, %r3767;
	ex2.approx.ftz.f32 	%r3800, %r3768;
	ex2.approx.ftz.f32 	%r3801, %r3769;
	ex2.approx.ftz.f32 	%r3802, %r3770;
	ex2.approx.ftz.f32 	%r3803, %r3771;
	ex2.approx.ftz.f32 	%r3804, %r3772;
	ex2.approx.ftz.f32 	%r3805, %r3773;
	ex2.approx.ftz.f32 	%r3806, %r3774;
	ex2.approx.ftz.f32 	%r3807, %r3775;
	ex2.approx.ftz.f32 	%r3808, %r3776;
	ex2.approx.ftz.f32 	%r3809, %r3777;
	ex2.approx.ftz.f32 	%r3810, %r3778;
	ex2.approx.ftz.f32 	%r3811, %r3779;
	ex2.approx.ftz.f32 	%r3812, %r3780;
	add.s32 	%r3387, %r2059, %r3727;
	cvt.rn.f16x2.f32 	%r2920, %r3782, %r3781;
	cvt.rn.f16x2.f32 	%r2921, %r3784, %r3783;
	cvt.rn.f16x2.f32 	%r2922, %r3786, %r3785;
	cvt.rn.f16x2.f32 	%r2923, %r3788, %r3787;
	cvt.rn.f16x2.f32 	%r2988, %r3790, %r3789;
	cvt.rn.f16x2.f32 	%r2989, %r3792, %r3791;
	cvt.rn.f16x2.f32 	%r2990, %r3794, %r3793;
	cvt.rn.f16x2.f32 	%r2991, %r3796, %r3795;
	cvt.rn.f16x2.f32 	%r3056, %r3798, %r3797;
	cvt.rn.f16x2.f32 	%r3057, %r3800, %r3799;
	cvt.rn.f16x2.f32 	%r3058, %r3802, %r3801;
	cvt.rn.f16x2.f32 	%r3059, %r3804, %r3803;
	cvt.rn.f16x2.f32 	%r3124, %r3806, %r3805;
	cvt.rn.f16x2.f32 	%r3125, %r3808, %r3807;
	cvt.rn.f16x2.f32 	%r3126, %r3810, %r3809;
	cvt.rn.f16x2.f32 	%r3127, %r3812, %r3811;
	wgmma.fence.sync.aligned;
	bfe.u32 	%r3814, %r3387, 4, 14;
	cvt.u64.u32 	%rd405, %r3814;
	or.b64 	%rd365, %rd405, 4611686293322072064;
	// begin inline asm
	wgmma.mma_async.sync.aligned.m64n64k16.f32.f16.f16 {%r6822,%r6821,%r6820,%r6819,%r6818,%r6817,%r6816,%r6815,%r6814,%r6813,%r6812,%r6811,%r6810,%r6809,%r6808,%r6807,%r6806,%r6805,%r6804,%r6803,%r6802,%r6801,%r6800,%r6799,%r6798,%r6797,%r6796,%r6795,%r6794,%r6793,%r6792,%r6791}, {%r2920,%r2921,%r2922,%r2923}, %rd365, %p96, 1, 1, 1;
	// end inline asm
	add.s32 	%r3815, %r3387, 2048;
	bfe.u32 	%r3816, %r3815, 4, 14;
	cvt.u64.u32 	%rd406, %r3816;
	or.b64 	%rd366, %rd406, 4611686293322072064;
	// begin inline asm
	wgmma.mma_async.sync.aligned.m64n64k16.f32.f16.f16 {%r6822,%r6821,%r6820,%r6819,%r6818,%r6817,%r6816,%r6815,%r6814,%r6813,%r6812,%r6811,%r6810,%r6809,%r6808,%r6807,%r6806,%r6805,%r6804,%r6803,%r6802,%r6801,%r6800,%r6799,%r6798,%r6797,%r6796,%r6795,%r6794,%r6793,%r6792,%r6791}, {%r2988,%r2989,%r2990,%r2991}, %rd366, %p96, 1, 1, 1;
	// end inline asm
	// begin inline asm
	wgmma.mma_async.sync.aligned.m64n64k16.f32.f16.f16 {%r6790,%r6789,%r6788,%r6787,%r6786,%r6785,%r6784,%r6783,%r6782,%r6781,%r6780,%r6779,%r6778,%r6777,%r6776,%r6775,%r6774,%r6773,%r6772,%r6771,%r6770,%r6769,%r6768,%r6767,%r6766,%r6765,%r6764,%r6763,%r6762,%r6761,%r6760,%r6759}, {%r3056,%r3057,%r3058,%r3059}, %rd365, %p96, 1, 1, 1;
	// end inline asm
	// begin inline asm
	wgmma.mma_async.sync.aligned.m64n64k16.f32.f16.f16 {%r6790,%r6789,%r6788,%r6787,%r6786,%r6785,%r6784,%r6783,%r6782,%r6781,%r6780,%r6779,%r6778,%r6777,%r6776,%r6775,%r6774,%r6773,%r6772,%r6771,%r6770,%r6769,%r6768,%r6767,%r6766,%r6765,%r6764,%r6763,%r6762,%r6761,%r6760,%r6759}, {%r3124,%r3125,%r3126,%r3127}, %rd366, %p96, 1, 1, 1;
	// end inline asm
	wgmma.commit_group.sync.aligned;
	add.s32 	%r3817, %r1414, 74240;
	add.s32 	%r3818, %r3817, %r3729;
	add.s32 	%r3819, %r3818, %r19;
	add.s32 	%r3820, %r3387, 32;
	bfe.u32 	%r3821, %r3820, 4, 14;
	cvt.u64.u32 	%rd407, %r3821;
	or.b64 	%rd372, %rd407, 4611686293322072064;
	add.s32 	%r3822, %r3387, 64;
	bfe.u32 	%r3823, %r3822, 4, 14;
	cvt.u64.u32 	%rd408, %r3823;
	or.b64 	%rd374, %rd408, 4611686293322072064;
	add.s32 	%r3824, %r3387, 96;
	bfe.u32 	%r3825, %r3824, 4, 14;
	cvt.u64.u32 	%rd409, %r3825;
	or.b64 	%rd376, %rd409, 4611686293322072064;
	ld.shared.v2.b32 	{%r3826, %r3827}, [%r3819+96];
	ld.shared.v2.b32 	{%r3828, %r3829}, [%r3819+64];
	ld.shared.v2.b32 	{%r3830, %r3831}, [%r3819+32];
	ld.shared.v2.b32 	{%r3832, %r3833}, [%r3819];
	wgmma.fence.sync.aligned;
	// begin inline asm
	wgmma.mma_async.sync.aligned.m64n32k16.f32.f16.f16 {%r3176,%r3177,%r3178,%r3179,%r3180,%r3181,%r3182,%r3183,%r3184,%r3185,%r3186,%r3187,%r3188,%r3189,%r3190,%r3191}, %rd590, %rd365, 0, 1, 1, 0, 0;
	// end inline asm
	// begin inline asm
	wgmma.mma_async.sync.aligned.m64n32k16.f32.f16.f16 {%r3176,%r3177,%r3178,%r3179,%r3180,%r3181,%r3182,%r3183,%r3184,%r3185,%r3186,%r3187,%r3188,%r3189,%r3190,%r3191}, %rd592, %rd372, %p96, 1, 1, 0, 0;
	// end inline asm
	// begin inline asm
	wgmma.mma_async.sync.aligned.m64n32k16.f32.f16.f16 {%r3176,%r3177,%r3178,%r3179,%r3180,%r3181,%r3182,%r3183,%r3184,%r3185,%r3186,%r3187,%r3188,%r3189,%r3190,%r3191}, %rd594, %rd374, %p96, 1, 1, 0, 0;
	// end inline asm
	// begin inline asm
	wgmma.mma_async.sync.aligned.m64n32k16.f32.f16.f16 {%r3176,%r3177,%r3178,%r3179,%r3180,%r3181,%r3182,%r3183,%r3184,%r3185,%r3186,%r3187,%r3188,%r3189,%r3190,%r3191}, %rd596, %rd376, %p96, 1, 1, 0, 0;
	// end inline asm
	// begin inline asm
	wgmma.mma_async.sync.aligned.m64n32k16.f32.f16.f16 {%r3288,%r3289,%r3290,%r3291,%r3292,%r3293,%r3294,%r3295,%r3296,%r3297,%r3298,%r3299,%r3300,%r3301,%r3302,%r3303}, %rd598, %rd365, 0, 1, 1, 0, 0;
	// end inline asm
	// begin inline asm
	wgmma.mma_async.sync.aligned.m64n32k16.f32.f16.f16 {%r3288,%r3289,%r3290,%r3291,%r3292,%r3293,%r3294,%r3295,%r3296,%r3297,%r3298,%r3299,%r3300,%r3301,%r3302,%r3303}, %rd600, %rd372, %p96, 1, 1, 0, 0;
	// end inline asm
	// begin inline asm
	wgmma.mma_async.sync.aligned.m64n32k16.f32.f16.f16 {%r3288,%r3289,%r3290,%r3291,%r3292,%r3293,%r3294,%r3295,%r3296,%r3297,%r3298,%r3299,%r3300,%r3301,%r3302,%r3303}, %rd602, %rd374, %p96, 1, 1, 0, 0;
	// end inline asm
	// begin inline asm
	wgmma.mma_async.sync.aligned.m64n32k16.f32.f16.f16 {%r3288,%r3289,%r3290,%r3291,%r3292,%r3293,%r3294,%r3295,%r3296,%r3297,%r3298,%r3299,%r3300,%r3301,%r3302,%r3303}, %rd604, %rd376, %p96, 1, 1, 0, 0;
	// end inline asm
	wgmma.commit_group.sync.aligned;
	mov.b32 	%r3385, %r2851;
	mov.b32 	%r3386, %r2851;
	mov.b32 	%r3388, %r2851;
	mov.b32 	%r3384, %r1416;
	mov.b32 	%r3389, %r2851;
	// begin inline asm
	// wait for regs: %r3176,%r3177,%r3178,%r3179,%r3180,%r3181,%r3182,%r3183,%r3184,%r3185,%r3186,%r3187,%r3188,%r3189,%r3190,%r3191,%r3288,%r3289,%r3290,%r3291,%r3292,%r3293,%r3294,%r3295,%r3296,%r3297,%r3298,%r3299,%r3300,%r3301,%r3302,%r3303,%r3384,%r3385,%r3386,%r3387,%r3388,%r3389
	wgmma.wait_group.sync.aligned 0;
	// end inline asm
	sub.f32 	%r3834, %r3291, %r3833;
	sub.f32 	%r3835, %r3290, %r3832;
	sub.f32 	%r3836, %r3295, %r3831;
	sub.f32 	%r3837, %r3294, %r3830;
	sub.f32 	%r3838, %r3299, %r3829;
	sub.f32 	%r3839, %r3298, %r3828;
	sub.f32 	%r3840, %r3303, %r3827;
	sub.f32 	%r3841, %r3302, %r3826;
	mul.f32 	%r3842, %r3799, %r3835;
	mul.f32 	%r3843, %r3800, %r3834;
	mul.f32 	%r3844, %r3803, %r3837;
	mul.f32 	%r3845, %r3804, %r3836;
	mul.f32 	%r3846, %r3807, %r3839;
	mul.f32 	%r3847, %r3808, %r3838;
	mul.f32 	%r3848, %r3811, %r3841;
	mul.f32 	%r3849, %r3812, %r3840;
	sub.f32 	%r3850, %r3177, %r3833;
	sub.f32 	%r3851, %r3176, %r3832;
	mul.f32 	%r3852, %r3781, %r3851;
	mul.f32 	%r3853, %r3782, %r3850;
	cvt.rn.f16x2.f32 	%r3492, %r3853, %r3852;
	sub.f32 	%r3854, %r3179, %r3833;
	sub.f32 	%r3855, %r3178, %r3832;
	mul.f32 	%r3856, %r3783, %r3855;
	mul.f32 	%r3857, %r3784, %r3854;
	cvt.rn.f16x2.f32 	%r3493, %r3857, %r3856;
	sub.f32 	%r3858, %r3181, %r3831;
	sub.f32 	%r3859, %r3180, %r3830;
	mul.f32 	%r3860, %r3785, %r3859;
	mul.f32 	%r3861, %r3786, %r3858;
	cvt.rn.f16x2.f32 	%r3494, %r3861, %r3860;
	sub.f32 	%r3862, %r3183, %r3831;
	sub.f32 	%r3863, %r3182, %r3830;
	mul.f32 	%r3864, %r3787, %r3863;
	mul.f32 	%r3865, %r3788, %r3862;
	cvt.rn.f16x2.f32 	%r3495, %r3865, %r3864;
	sub.f32 	%r3866, %r3185, %r3829;
	sub.f32 	%r3867, %r3184, %r3828;
	mul.f32 	%r3868, %r3789, %r3867;
	mul.f32 	%r3869, %r3790, %r3866;
	cvt.rn.f16x2.f32 	%r3560, %r3869, %r3868;
	sub.f32 	%r3870, %r3187, %r3829;
	sub.f32 	%r3871, %r3186, %r3828;
	mul.f32 	%r3872, %r3791, %r3871;
	mul.f32 	%r3873, %r3792, %r3870;
	cvt.rn.f16x2.f32 	%r3561, %r3873, %r3872;
	sub.f32 	%r3874, %r3189, %r3827;
	sub.f32 	%r3875, %r3188, %r3826;
	mul.f32 	%r3876, %r3793, %r3875;
	mul.f32 	%r3877, %r3794, %r3874;
	cvt.rn.f16x2.f32 	%r3562, %r3877, %r3876;
	sub.f32 	%r3878, %r3191, %r3827;
	sub.f32 	%r3879, %r3190, %r3826;
	mul.f32 	%r3880, %r3795, %r3879;
	mul.f32 	%r3881, %r3796, %r3878;
	cvt.rn.f16x2.f32 	%r3563, %r3881, %r3880;
	sub.f32 	%r3882, %r3289, %r3833;
	sub.f32 	%r3883, %r3288, %r3832;
	mul.f32 	%r3884, %r3797, %r3883;
	mul.f32 	%r3885, %r3798, %r3882;
	cvt.rn.f16x2.f32 	%r3628, %r3885, %r3884;
	cvt.rn.f16x2.f32 	%r3629, %r3843, %r3842;
	sub.f32 	%r3886, %r3293, %r3831;
	sub.f32 	%r3887, %r3292, %r3830;
	mul.f32 	%r3888, %r3801, %r3887;
	mul.f32 	%r3889, %r3802, %r3886;
	cvt.rn.f16x2.f32 	%r3630, %r3889, %r3888;
	cvt.rn.f16x2.f32 	%r3631, %r3845, %r3844;
	sub.f32 	%r3890, %r3297, %r3829;
	sub.f32 	%r3891, %r3296, %r3828;
	mul.f32 	%r3892, %r3805, %r3891;
	mul.f32 	%r3893, %r3806, %r3890;
	cvt.rn.f16x2.f32 	%r3696, %r3893, %r3892;
	cvt.rn.f16x2.f32 	%r3697, %r3847, %r3846;
	sub.f32 	%r3894, %r3301, %r3827;
	sub.f32 	%r3895, %r3300, %r3826;
	mul.f32 	%r3896, %r3809, %r3895;
	mul.f32 	%r3897, %r3810, %r3894;
	cvt.rn.f16x2.f32 	%r3698, %r3897, %r3896;
	cvt.rn.f16x2.f32 	%r3699, %r3849, %r3848;
	wgmma.fence.sync.aligned;
	// begin inline asm
	wgmma.mma_async.sync.aligned.m64n64k16.f32.f16.f16 {%r6886,%r6885,%r6884,%r6883,%r6882,%r6881,%r6880,%r6879,%r6878,%r6877,%r6876,%r6875,%r6874,%r6873,%r6872,%r6871,%r6870,%r6869,%r6868,%r6867,%r6866,%r6865,%r6864,%r6863,%r6862,%r6861,%r6860,%r6859,%r6858,%r6857,%r6856,%r6855}, {%r3492,%r3493,%r3494,%r3495}, %rd350, %p96, 1, 1, 1;
	// end inline asm
	add.s32 	%r3898, %r2853, 2048;
	bfe.u32 	%r3899, %r3898, 4, 14;
	cvt.u64.u32 	%rd410, %r3899;
	or.b64 	%rd386, %rd410, 4611686293322072064;
	// begin inline asm
	wgmma.mma_async.sync.aligned.m64n64k16.f32.f16.f16 {%r6886,%r6885,%r6884,%r6883,%r6882,%r6881,%r6880,%r6879,%r6878,%r6877,%r6876,%r6875,%r6874,%r6873,%r6872,%r6871,%r6870,%r6869,%r6868,%r6867,%r6866,%r6865,%r6864,%r6863,%r6862,%r6861,%r6860,%r6859,%r6858,%r6857,%r6856,%r6855}, {%r3560,%r3561,%r3562,%r3563}, %rd386, %p96, 1, 1, 1;
	// end inline asm
	// begin inline asm
	wgmma.mma_async.sync.aligned.m64n64k16.f32.f16.f16 {%r6854,%r6853,%r6852,%r6851,%r6850,%r6849,%r6848,%r6847,%r6846,%r6845,%r6844,%r6843,%r6842,%r6841,%r6840,%r6839,%r6838,%r6837,%r6836,%r6835,%r6834,%r6833,%r6832,%r6831,%r6830,%r6829,%r6828,%r6827,%r6826,%r6825,%r6824,%r6823}, {%r3628,%r3629,%r3630,%r3631}, %rd350, %p96, 1, 1, 1;
	// end inline asm
	// begin inline asm
	wgmma.mma_async.sync.aligned.m64n64k16.f32.f16.f16 {%r6854,%r6853,%r6852,%r6851,%r6850,%r6849,%r6848,%r6847,%r6846,%r6845,%r6844,%r6843,%r6842,%r6841,%r6840,%r6839,%r6838,%r6837,%r6836,%r6835,%r6834,%r6833,%r6832,%r6831,%r6830,%r6829,%r6828,%r6827,%r6826,%r6825,%r6824,%r6823}, {%r3696,%r3697,%r3698,%r3699}, %rd386, %p96, 1, 1, 1;
	// end inline asm
	wgmma.commit_group.sync.aligned;
	add.s64 	%rd389, %rd653, %rd650;
	add.s64 	%rd390, %rd654, %rd650;
	add.s64 	%rd395, %rd655, %rd650;
	add.s64 	%rd396, %rd656, %rd650;
	add.s32 	%r3900, %r6756, 1;
	setp.gt.s32 	%p127, %r3900, 3;
	selp.b32 	%r6756, 0, %r3900, %p127;
	add.s32 	%r3901, %r6758, 1;
	setp.gt.s32 	%p128, %r3901, 4;
	selp.b32 	%r6758, 0, %r3901, %p128;
	shl.b32 	%r3902, %r6758, 12;
	add.s32 	%r3903, %r1414, %r3902;
	bar.sync 	0;
	add.s32 	%r3700, %r3903, %r14;
	selp.b32 	%r3701, 16, 0, %p124;
	// begin inline asm
	cp.async.cg.shared.global [ %r3700 + 0 ], [ %rd389 + 0 ], 0x10, %r3701;
	// end inline asm
	add.s32 	%r3702, %r3700, 2048;
	// begin inline asm
	cp.async.cg.shared.global [ %r3702 + 0 ], [ %rd390 + 0 ], 0x10, %r3701;
	// end inline asm
	cp.async.commit_group;
	add.s32 	%r3904, %r2529, %r6754;
	mul.wide.s32 	%rd411, %r3904, 4;
	add.s64 	%rd391, %rd6, %rd411;
	cvt.s64.s32 	%rd412, %r6754;
	add.s64 	%rd413, %rd412, %rd21;
	shl.b64 	%rd414, %rd413, 2;
	add.s64 	%rd415, %rd6, %rd414;
	add.s64 	%rd392, %rd415, 32;
	add.s64 	%rd393, %rd415, 64;
	add.s64 	%rd394, %rd415, 96;
	shl.b32 	%r3905, %r6756, 7;
	add.s32 	%r3906, %r3730, %r3905;
	add.s32 	%r3704, %r3906, %r19;
	selp.b32 	%r3705, 8, 0, %p124;
	// begin inline asm
	@%p1 cp.async.ca.shared.global [ %r3704 + 0 ], [ %rd391 + 0 ], 0x8, %r3705;
	// end inline asm
	add.s32 	%r3706, %r3704, 32;
	// begin inline asm
	@%p1 cp.async.ca.shared.global [ %r3706 + 0 ], [ %rd392 + 0 ], 0x8, %r3705;
	// end inline asm
	add.s32 	%r3708, %r3704, 64;
	// begin inline asm
	@%p1 cp.async.ca.shared.global [ %r3708 + 0 ], [ %rd393 + 0 ], 0x8, %r3705;
	// end inline asm
	add.s32 	%r3710, %r3704, 96;
	// begin inline asm
	@%p1 cp.async.ca.shared.global [ %r3710 + 0 ], [ %rd394 + 0 ], 0x8, %r3705;
	// end inline asm
	cp.async.commit_group;
	add.s32 	%r3907, %r2059, %r3902;
	add.s32 	%r3712, %r3907, %r14;
	// begin inline asm
	cp.async.cg.shared.global [ %r3712 + 0 ], [ %rd395 + 0 ], 0x10, %r3701;
	// end inline asm
	add.s32 	%r3714, %r3712, 2048;
	// begin inline asm
	cp.async.cg.shared.global [ %r3714 + 0 ], [ %rd396 + 0 ], 0x10, %r3701;
	// end inline asm
	cp.async.commit_group;
	add.s64 	%rd397, %rd7, %rd411;
	add.s64 	%rd416, %rd7, %rd414;
	add.s64 	%rd398, %rd416, 32;
	add.s64 	%rd399, %rd416, 64;
	add.s64 	%rd400, %rd416, 96;
	add.s32 	%r3908, %r3817, %r3905;
	add.s32 	%r3716, %r3908, %r19;
	// begin inline asm
	@%p1 cp.async.ca.shared.global [ %r3716 + 0 ], [ %rd397 + 0 ], 0x8, %r3705;
	// end inline asm
	add.s32 	%r3718, %r3716, 32;
	// begin inline asm
	@%p1 cp.async.ca.shared.global [ %r3718 + 0 ], [ %rd398 + 0 ], 0x8, %r3705;
	// end inline asm
	add.s32 	%r3720, %r3716, 64;
	// begin inline asm
	@%p1 cp.async.ca.shared.global [ %r3720 + 0 ], [ %rd399 + 0 ], 0x8, %r3705;
	// end inline asm
	add.s32 	%r3722, %r3716, 96;
	// begin inline asm
	@%p1 cp.async.ca.shared.global [ %r3722 + 0 ], [ %rd400 + 0 ], 0x8, %r3705;
	// end inline asm
	cp.async.commit_group;
	add.s32 	%r6887, %r6887, 1;
	add.s64 	%rd656, %rd656, %rd651;
	add.s64 	%rd655, %rd655, %rd651;
	add.s64 	%rd654, %rd654, %rd651;
	add.s64 	%rd653, %rd653, %rd651;
	add.s32 	%r6754, %r6754, 32;
	setp.ne.b32 	%p129, %r426, %r6887;
	@%p129 bra 	$L__BB0_4;
$L__BB0_5:                              // %._crit_edge
	cvt.u32.u64 	%r4395, %rd16;
	// begin inline asm
	// wait for regs: %r6822,%r6821,%r6820,%r6819,%r6818,%r6817,%r6816,%r6815,%r6814,%r6813,%r6812,%r6811,%r6810,%r6809,%r6808,%r6807,%r6806,%r6805,%r6804,%r6803,%r6802,%r6801,%r6800,%r6799,%r6798,%r6797,%r6796,%r6795,%r6794,%r6793,%r6792,%r6791,%r6790,%r6789,%r6788,%r6787,%r6786,%r6785,%r6784,%r6783,%r6782,%r6781,%r6780,%r6779,%r6778,%r6777,%r6776,%r6775,%r6774,%r6773,%r6772,%r6771,%r6770,%r6769,%r6768,%r6767,%r6766,%r6765,%r6764,%r6763,%r6762,%r6761,%r6760,%r6759,%r6886,%r6885,%r6884,%r6883,%r6882,%r6881,%r6880,%r6879,%r6878,%r6877,%r6876,%r6875,%r6874,%r6873,%r6872,%r6871,%r6870,%r6869,%r6868,%r6867,%r6866,%r6865,%r6864,%r6863,%r6862,%r6861,%r6860,%r6859,%r6858,%r6857,%r6856,%r6855,%r6854,%r6853,%r6852,%r6851,%r6850,%r6849,%r6848,%r6847,%r6846,%r6845,%r6844,%r6843,%r6842,%r6841,%r6840,%r6839,%r6838,%r6837,%r6836,%r6835,%r6834,%r6833,%r6832,%r6831,%r6830,%r6829,%r6828,%r6827,%r6826,%r6825,%r6824,%r6823
	wgmma.wait_group.sync.aligned 0;
	// end inline asm
	cp.async.wait_group 	0;
	bar.sync 	0;
	shl.b64 	%rd459, %rd8, 1;
	add.s64 	%rd460, %rd45, %rd459;
	shl.b64 	%rd461, %rd9, 1;
	add.s64 	%rd462, %rd45, %rd461;
	shl.b64 	%rd463, %rd10, 1;
	add.s64 	%rd464, %rd45, %rd463;
	shl.b64 	%rd465, %rd11, 1;
	add.s64 	%rd466, %rd45, %rd465;
	shl.b64 	%rd467, %rd12, 1;
	add.s64 	%rd468, %rd45, %rd467;
	shl.b64 	%rd469, %rd13, 1;
	add.s64 	%rd470, %rd45, %rd469;
	shl.b64 	%rd471, %rd14, 1;
	add.s64 	%rd472, %rd45, %rd471;
	shl.b64 	%rd473, %rd15, 1;
	add.s64 	%rd474, %rd45, %rd473;
	add.s64 	%rd417, %rd460, %rd313;
	add.s64 	%rd418, %rd462, %rd313;
	add.s64 	%rd419, %rd464, %rd313;
	add.s64 	%rd420, %rd466, %rd313;
	add.s64 	%rd421, %rd468, %rd313;
	add.s64 	%rd422, %rd470, %rd313;
	add.s64 	%rd423, %rd472, %rd313;
	add.s64 	%rd424, %rd474, %rd313;
	cvt.rn.f16x2.f32 	%r4396, %r6821, %r6822;
	cvt.rn.f16x2.f32 	%r4397, %r6819, %r6820;
	cvt.rn.f16x2.f32 	%r4398, %r6817, %r6818;
	cvt.rn.f16x2.f32 	%r4399, %r6815, %r6816;
	cvt.rn.f16x2.f32 	%r4400, %r6813, %r6814;
	cvt.rn.f16x2.f32 	%r4401, %r6811, %r6812;
	cvt.rn.f16x2.f32 	%r4402, %r6809, %r6810;
	cvt.rn.f16x2.f32 	%r4403, %r6807, %r6808;
	cvt.rn.f16x2.f32 	%r4404, %r6805, %r6806;
	cvt.rn.f16x2.f32 	%r4405, %r6803, %r6804;
	cvt.rn.f16x2.f32 	%r4406, %r6801, %r6802;
	cvt.rn.f16x2.f32 	%r4407, %r6799, %r6800;
	cvt.rn.f16x2.f32 	%r4408, %r6797, %r6798;
	cvt.rn.f16x2.f32 	%r4409, %r6795, %r6796;
	cvt.rn.f16x2.f32 	%r4410, %r6793, %r6794;
	cvt.rn.f16x2.f32 	%r4411, %r6791, %r6792;
	cvt.rn.f16x2.f32 	%r4412, %r6789, %r6790;
	cvt.rn.f16x2.f32 	%r4413, %r6787, %r6788;
	cvt.rn.f16x2.f32 	%r4414, %r6785, %r6786;
	cvt.rn.f16x2.f32 	%r4415, %r6783, %r6784;
	cvt.rn.f16x2.f32 	%r4416, %r6781, %r6782;
	cvt.rn.f16x2.f32 	%r4417, %r6779, %r6780;
	cvt.rn.f16x2.f32 	%r4418, %r6777, %r6778;
	cvt.rn.f16x2.f32 	%r4419, %r6775, %r6776;
	cvt.rn.f16x2.f32 	%r4420, %r6773, %r6774;
	cvt.rn.f16x2.f32 	%r4421, %r6771, %r6772;
	cvt.rn.f16x2.f32 	%r4422, %r6769, %r6770;
	cvt.rn.f16x2.f32 	%r4423, %r6767, %r6768;
	cvt.rn.f16x2.f32 	%r4424, %r6765, %r6766;
	cvt.rn.f16x2.f32 	%r4425, %r6763, %r6764;
	cvt.rn.f16x2.f32 	%r4426, %r6761, %r6762;
	cvt.rn.f16x2.f32 	%r4427, %r6759, %r6760;
	shl.b32 	%r4428, %r6, 4;
	and.b32 	%r4429, %r2, 1;
	shl.b32 	%r4430, %r4429, 5;
	and.b32 	%r4431, %r2, 2;
	bfe.s32 	%r4432, %r2, 1, 1;
	and.b32 	%r4433, %r4432, 4160;
	bfe.s32 	%r4434, %r2, 2, 1;
	and.b32 	%r4435, %r2, 4;
	shl.b32 	%r4436, %r4435, 2;
	and.b32 	%r4437, %r2, 8;
	bfe.s32 	%r4438, %r2, 3, 1;
	and.b32 	%r4439, %r4438, 2080;
	bfe.s32 	%r4440, %r2, 4, 1;
	and.b32 	%r4441, %r2, 16;
	shl.b32 	%r4442, %r4441, 3;
	or.b32 	%r4443, %r4433, %r4436;
	or.b32 	%r4444, %r4443, %r4442;
	or.b32 	%r4445, %r4428, %r4430;
	xor.b32 	%r4446, %r4445, %r4439;
	or.b32 	%r4447, %r4444, %r4446;
	add.s32 	%r838, %r1414, %r4447;
	st.shared.v4.b32 	[%r838], {%r4396, %r4398, %r4400, %r4402};
	st.shared.v4.b32 	[%r838+256], {%r4397, %r4399, %r4401, %r4403};
	xor.b32 	%r4449, %r4447, 64;
	add.s32 	%r839, %r1414, %r4449;
	st.shared.v4.b32 	[%r839], {%r4404, %r4406, %r4408, %r4410};
	st.shared.v4.b32 	[%r839+256], {%r4405, %r4407, %r4409, %r4411};
	bar.sync 	0;
	shl.b32 	%r4450, %r2, 2;
	and.b32 	%r4451, %r4450, 416;
	shl.b32 	%r4452, %r4429, 6;
	shl.b32 	%r4453, %r4431, 3;
	and.b32 	%r4454, %r4434, 2080;
	and.b32 	%r4455, %r4440, 4160;
	or.b32 	%r4456, %r4451, %r4452;
	or.b32 	%r4457, %r4454, %r4455;
	xor.b32 	%r4458, %r4457, %r4456;
	add.s32 	%r4459, %r1414, %r4453;
	add.s32 	%r6476, %r4459, %r4458;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r4205, %r4206, %r4207, %r4208}, [%r6476];
	// end inline asm
	add.s32 	%r6481, %r6476, 512;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r4209, %r4210, %r4211, %r4212}, [%r6481];
	// end inline asm
	add.s32 	%r6486, %r6476, 1024;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r4213, %r4214, %r4215, %r4216}, [%r6486];
	// end inline asm
	add.s32 	%r6491, %r6476, 1536;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r4217, %r4218, %r4219, %r4220}, [%r6491];
	// end inline asm
	bar.sync 	0;
	st.shared.v4.b32 	[%r838], {%r4412, %r4414, %r4416, %r4418};
	st.shared.v4.b32 	[%r838+256], {%r4413, %r4415, %r4417, %r4419};
	st.shared.v4.b32 	[%r839], {%r4420, %r4422, %r4424, %r4426};
	st.shared.v4.b32 	[%r839+256], {%r4421, %r4423, %r4425, %r4427};
	bar.sync 	0;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r4221, %r4222, %r4223, %r4224}, [%r6476];
	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r4225, %r4226, %r4227, %r4228}, [%r6481];
	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r4229, %r4230, %r4231, %r4232}, [%r6486];
	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r4233, %r4234, %r4235, %r4236}, [%r6491];
	// end inline asm
	// begin inline asm
	st.global.v4.b32 [ %rd417 + 0 ], { %r4205, %r4206, %r4207, %r4208 };
	// end inline asm
	// begin inline asm
	st.global.v4.b32 [ %rd418 + 0 ], { %r4209, %r4210, %r4211, %r4212 };
	// end inline asm
	// begin inline asm
	st.global.v4.b32 [ %rd419 + 0 ], { %r4213, %r4214, %r4215, %r4216 };
	// end inline asm
	// begin inline asm
	st.global.v4.b32 [ %rd420 + 0 ], { %r4217, %r4218, %r4219, %r4220 };
	// end inline asm
	// begin inline asm
	st.global.v4.b32 [ %rd421 + 0 ], { %r4221, %r4222, %r4223, %r4224 };
	// end inline asm
	// begin inline asm
	st.global.v4.b32 [ %rd422 + 0 ], { %r4225, %r4226, %r4227, %r4228 };
	// end inline asm
	// begin inline asm
	st.global.v4.b32 [ %rd423 + 0 ], { %r4229, %r4230, %r4231, %r4232 };
	// end inline asm
	// begin inline asm
	st.global.v4.b32 [ %rd424 + 0 ], { %r4233, %r4234, %r4235, %r4236 };
	// end inline asm
	mul.f32 	%r4460, %r1263, %r6886;
	mul.f32 	%r4461, %r1263, %r6885;
	mul.f32 	%r4462, %r1263, %r6884;
	mul.f32 	%r4463, %r1263, %r6883;
	mul.f32 	%r4464, %r1263, %r6882;
	mul.f32 	%r4465, %r1263, %r6881;
	mul.f32 	%r4466, %r1263, %r6880;
	mul.f32 	%r4467, %r1263, %r6879;
	mul.f32 	%r4468, %r1263, %r6878;
	mul.f32 	%r4469, %r1263, %r6877;
	mul.f32 	%r4470, %r1263, %r6876;
	mul.f32 	%r4471, %r1263, %r6875;
	mul.f32 	%r4472, %r1263, %r6874;
	mul.f32 	%r4473, %r1263, %r6873;
	mul.f32 	%r4474, %r1263, %r6872;
	mul.f32 	%r4475, %r1263, %r6871;
	mul.f32 	%r4476, %r1263, %r6870;
	mul.f32 	%r4477, %r1263, %r6869;
	mul.f32 	%r4478, %r1263, %r6868;
	mul.f32 	%r4479, %r1263, %r6867;
	mul.f32 	%r4480, %r1263, %r6866;
	mul.f32 	%r4481, %r1263, %r6865;
	mul.f32 	%r4482, %r1263, %r6864;
	mul.f32 	%r4483, %r1263, %r6863;
	mul.f32 	%r4484, %r1263, %r6862;
	mul.f32 	%r4485, %r1263, %r6861;
	mul.f32 	%r4486, %r1263, %r6860;
	mul.f32 	%r4487, %r1263, %r6859;
	mul.f32 	%r4488, %r1263, %r6858;
	mul.f32 	%r4489, %r1263, %r6857;
	mul.f32 	%r4490, %r1263, %r6856;
	mul.f32 	%r4491, %r1263, %r6855;
	mul.f32 	%r4492, %r1263, %r6854;
	mul.f32 	%r4493, %r1263, %r6853;
	mul.f32 	%r4494, %r1263, %r6852;
	mul.f32 	%r4495, %r1263, %r6851;
	mul.f32 	%r4496, %r1263, %r6850;
	mul.f32 	%r4497, %r1263, %r6849;
	mul.f32 	%r4498, %r1263, %r6848;
	mul.f32 	%r4499, %r1263, %r6847;
	mul.f32 	%r4500, %r1263, %r6846;
	mul.f32 	%r4501, %r1263, %r6845;
	mul.f32 	%r4502, %r1263, %r6844;
	mul.f32 	%r4503, %r1263, %r6843;
	mul.f32 	%r4504, %r1263, %r6842;
	mul.f32 	%r4505, %r1263, %r6841;
	mul.f32 	%r4506, %r1263, %r6840;
	mul.f32 	%r4507, %r1263, %r6839;
	mul.f32 	%r4508, %r1263, %r6838;
	mul.f32 	%r4509, %r1263, %r6837;
	mul.f32 	%r4510, %r1263, %r6836;
	mul.f32 	%r4511, %r1263, %r6835;
	mul.f32 	%r4512, %r1263, %r6834;
	mul.f32 	%r4513, %r1263, %r6833;
	mul.f32 	%r4514, %r1263, %r6832;
	mul.f32 	%r4515, %r1263, %r6831;
	mul.f32 	%r4516, %r1263, %r6830;
	mul.f32 	%r4517, %r1263, %r6829;
	mul.f32 	%r4518, %r1263, %r6828;
	mul.f32 	%r4519, %r1263, %r6827;
	mul.f32 	%r4520, %r1263, %r6826;
	mul.f32 	%r4521, %r1263, %r6825;
	mul.f32 	%r4522, %r1263, %r6824;
	mul.f32 	%r4523, %r1263, %r6823;
	add.s64 	%rd476, %rd44, %rd459;
	add.s64 	%rd477, %rd44, %rd461;
	add.s64 	%rd478, %rd44, %rd463;
	add.s64 	%rd479, %rd44, %rd465;
	add.s64 	%rd480, %rd44, %rd467;
	add.s64 	%rd481, %rd44, %rd469;
	add.s64 	%rd482, %rd44, %rd471;
	add.s64 	%rd483, %rd44, %rd473;
	add.s64 	%rd425, %rd476, %rd313;
	add.s64 	%rd426, %rd477, %rd313;
	add.s64 	%rd427, %rd478, %rd313;
	add.s64 	%rd428, %rd479, %rd313;
	add.s64 	%rd429, %rd480, %rd313;
	add.s64 	%rd430, %rd481, %rd313;
	add.s64 	%rd431, %rd482, %rd313;
	add.s64 	%rd432, %rd483, %rd313;
	bar.sync 	0;
	cvt.rn.f16x2.f32 	%r4524, %r4473, %r4472;
	cvt.rn.f16x2.f32 	%r4525, %r4469, %r4468;
	cvt.rn.f16x2.f32 	%r4526, %r4465, %r4464;
	cvt.rn.f16x2.f32 	%r4527, %r4461, %r4460;
	st.shared.v4.b32 	[%r838], {%r4527, %r4526, %r4525, %r4524};
	cvt.rn.f16x2.f32 	%r4528, %r4475, %r4474;
	cvt.rn.f16x2.f32 	%r4529, %r4471, %r4470;
	cvt.rn.f16x2.f32 	%r4530, %r4467, %r4466;
	cvt.rn.f16x2.f32 	%r4531, %r4463, %r4462;
	st.shared.v4.b32 	[%r838+256], {%r4531, %r4530, %r4529, %r4528};
	cvt.rn.f16x2.f32 	%r4532, %r4489, %r4488;
	cvt.rn.f16x2.f32 	%r4533, %r4485, %r4484;
	cvt.rn.f16x2.f32 	%r4534, %r4481, %r4480;
	cvt.rn.f16x2.f32 	%r4535, %r4477, %r4476;
	st.shared.v4.b32 	[%r839], {%r4535, %r4534, %r4533, %r4532};
	cvt.rn.f16x2.f32 	%r4536, %r4491, %r4490;
	cvt.rn.f16x2.f32 	%r4537, %r4487, %r4486;
	cvt.rn.f16x2.f32 	%r4538, %r4483, %r4482;
	cvt.rn.f16x2.f32 	%r4539, %r4479, %r4478;
	st.shared.v4.b32 	[%r839+256], {%r4539, %r4538, %r4537, %r4536};
	bar.sync 	0;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r4277, %r4278, %r4279, %r4280}, [%r6476];
	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r4281, %r4282, %r4283, %r4284}, [%r6481];
	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r4285, %r4286, %r4287, %r4288}, [%r6486];
	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r4289, %r4290, %r4291, %r4292}, [%r6491];
	// end inline asm
	bar.sync 	0;
	cvt.rn.f16x2.f32 	%r4540, %r4505, %r4504;
	cvt.rn.f16x2.f32 	%r4541, %r4501, %r4500;
	cvt.rn.f16x2.f32 	%r4542, %r4497, %r4496;
	cvt.rn.f16x2.f32 	%r4543, %r4493, %r4492;
	st.shared.v4.b32 	[%r838], {%r4543, %r4542, %r4541, %r4540};
	cvt.rn.f16x2.f32 	%r4544, %r4507, %r4506;
	cvt.rn.f16x2.f32 	%r4545, %r4503, %r4502;
	cvt.rn.f16x2.f32 	%r4546, %r4499, %r4498;
	cvt.rn.f16x2.f32 	%r4547, %r4495, %r4494;
	st.shared.v4.b32 	[%r838+256], {%r4547, %r4546, %r4545, %r4544};
	cvt.rn.f16x2.f32 	%r4548, %r4521, %r4520;
	cvt.rn.f16x2.f32 	%r4549, %r4517, %r4516;
	cvt.rn.f16x2.f32 	%r4550, %r4513, %r4512;
	cvt.rn.f16x2.f32 	%r4551, %r4509, %r4508;
	st.shared.v4.b32 	[%r839], {%r4551, %r4550, %r4549, %r4548};
	cvt.rn.f16x2.f32 	%r4552, %r4523, %r4522;
	cvt.rn.f16x2.f32 	%r4553, %r4519, %r4518;
	cvt.rn.f16x2.f32 	%r4554, %r4515, %r4514;
	cvt.rn.f16x2.f32 	%r4555, %r4511, %r4510;
	st.shared.v4.b32 	[%r839+256], {%r4555, %r4554, %r4553, %r4552};
	bar.sync 	0;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r4293, %r4294, %r4295, %r4296}, [%r6476];
	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r4297, %r4298, %r4299, %r4300}, [%r6481];
	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r4301, %r4302, %r4303, %r4304}, [%r6486];
	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r4305, %r4306, %r4307, %r4308}, [%r6491];
	// end inline asm
	// begin inline asm
	st.global.v4.b32 [ %rd425 + 0 ], { %r4277, %r4278, %r4279, %r4280 };
	// end inline asm
	// begin inline asm
	st.global.v4.b32 [ %rd426 + 0 ], { %r4281, %r4282, %r4283, %r4284 };
	// end inline asm
	// begin inline asm
	st.global.v4.b32 [ %rd427 + 0 ], { %r4285, %r4286, %r4287, %r4288 };
	// end inline asm
	// begin inline asm
	st.global.v4.b32 [ %rd428 + 0 ], { %r4289, %r4290, %r4291, %r4292 };
	// end inline asm
	// begin inline asm
	st.global.v4.b32 [ %rd429 + 0 ], { %r4293, %r4294, %r4295, %r4296 };
	// end inline asm
	// begin inline asm
	st.global.v4.b32 [ %rd430 + 0 ], { %r4297, %r4298, %r4299, %r4300 };
	// end inline asm
	// begin inline asm
	st.global.v4.b32 [ %rd431 + 0 ], { %r4301, %r4302, %r4303, %r4304 };
	// end inline asm
	// begin inline asm
	st.global.v4.b32 [ %rd432 + 0 ], { %r4305, %r4306, %r4307, %r4308 };
	// end inline asm
	add.s64 	%rd484, %rd2, %rd461;
	add.s64 	%rd485, %rd2, %rd463;
	add.s64 	%rd486, %rd2, %rd465;
	add.s64 	%rd487, %rd2, %rd467;
	add.s64 	%rd488, %rd2, %rd469;
	add.s64 	%rd489, %rd2, %rd471;
	add.s64 	%rd490, %rd2, %rd473;
	add.s64 	%rd434, %rd484, %rd313;
	add.s64 	%rd435, %rd485, %rd313;
	add.s64 	%rd436, %rd486, %rd313;
	add.s64 	%rd437, %rd487, %rd313;
	add.s64 	%rd438, %rd488, %rd313;
	add.s64 	%rd439, %rd489, %rd313;
	add.s64 	%rd440, %rd490, %rd313;
	// begin inline asm
	mov.u32 %r4309, 0x0;
	mov.u32 %r4310, 0x0;
	mov.u32 %r4311, 0x0;
	mov.u32 %r4312, 0x0;
	ld.global.v4.b32 { %r4309, %r4310, %r4311, %r4312 }, [ %rd433 + 0 ];
	// end inline asm
	// begin inline asm
	mov.u32 %r4313, 0x0;
	mov.u32 %r4314, 0x0;
	mov.u32 %r4315, 0x0;
	mov.u32 %r4316, 0x0;
	ld.global.v4.b32 { %r4313, %r4314, %r4315, %r4316 }, [ %rd434 + 0 ];
	// end inline asm
	// begin inline asm
	mov.u32 %r4317, 0x0;
	mov.u32 %r4318, 0x0;
	mov.u32 %r4319, 0x0;
	mov.u32 %r4320, 0x0;
	ld.global.v4.b32 { %r4317, %r4318, %r4319, %r4320 }, [ %rd435 + 0 ];
	// end inline asm
	// begin inline asm
	mov.u32 %r4321, 0x0;
	mov.u32 %r4322, 0x0;
	mov.u32 %r4323, 0x0;
	mov.u32 %r4324, 0x0;
	ld.global.v4.b32 { %r4321, %r4322, %r4323, %r4324 }, [ %rd436 + 0 ];
	// end inline asm
	// begin inline asm
	mov.u32 %r4325, 0x0;
	mov.u32 %r4326, 0x0;
	mov.u32 %r4327, 0x0;
	mov.u32 %r4328, 0x0;
	ld.global.v4.b32 { %r4325, %r4326, %r4327, %r4328 }, [ %rd437 + 0 ];
	// end inline asm
	// begin inline asm
	mov.u32 %r4329, 0x0;
	mov.u32 %r4330, 0x0;
	mov.u32 %r4331, 0x0;
	mov.u32 %r4332, 0x0;
	ld.global.v4.b32 { %r4329, %r4330, %r4331, %r4332 }, [ %rd438 + 0 ];
	// end inline asm
	// begin inline asm
	mov.u32 %r4333, 0x0;
	mov.u32 %r4334, 0x0;
	mov.u32 %r4335, 0x0;
	mov.u32 %r4336, 0x0;
	ld.global.v4.b32 { %r4333, %r4334, %r4335, %r4336 }, [ %rd439 + 0 ];
	// end inline asm
	// begin inline asm
	mov.u32 %r4337, 0x0;
	mov.u32 %r4338, 0x0;
	mov.u32 %r4339, 0x0;
	mov.u32 %r4340, 0x0;
	ld.global.v4.b32 { %r4337, %r4338, %r4339, %r4340 }, [ %rd440 + 0 ];
	// end inline asm
	st.shared.v4.b32 	[%r15], {%r4309, %r4310, %r4311, %r4312};
	st.shared.v4.b32 	[%r15+2048], {%r4313, %r4314, %r4315, %r4316};
	st.shared.v4.b32 	[%r15+4096], {%r4317, %r4318, %r4319, %r4320};
	st.shared.v4.b32 	[%r15+6144], {%r4321, %r4322, %r4323, %r4324};
	st.shared.v4.b32 	[%r15+8192], {%r4325, %r4326, %r4327, %r4328};
	st.shared.v4.b32 	[%r15+10240], {%r4329, %r4330, %r4331, %r4332};
	st.shared.v4.b32 	[%r15+12288], {%r4333, %r4334, %r4335, %r4336};
	st.shared.v4.b32 	[%r15+14336], {%r4337, %r4338, %r4339, %r4340};
	add.s64 	%rd491, %rd5, %rd461;
	add.s64 	%rd492, %rd5, %rd463;
	add.s64 	%rd493, %rd5, %rd465;
	add.s64 	%rd494, %rd5, %rd467;
	add.s64 	%rd495, %rd5, %rd469;
	add.s64 	%rd496, %rd5, %rd471;
	add.s64 	%rd497, %rd5, %rd473;
	add.s64 	%rd442, %rd491, %rd313;
	add.s64 	%rd443, %rd492, %rd313;
	add.s64 	%rd444, %rd493, %rd313;
	add.s64 	%rd445, %rd494, %rd313;
	add.s64 	%rd446, %rd495, %rd313;
	add.s64 	%rd447, %rd496, %rd313;
	add.s64 	%rd448, %rd497, %rd313;
	// begin inline asm
	mov.u32 %r4341, 0x0;
	mov.u32 %r4342, 0x0;
	mov.u32 %r4343, 0x0;
	mov.u32 %r4344, 0x0;
	ld.global.v4.b32 { %r4341, %r4342, %r4343, %r4344 }, [ %rd441 + 0 ];
	// end inline asm
	// begin inline asm
	mov.u32 %r4345, 0x0;
	mov.u32 %r4346, 0x0;
	mov.u32 %r4347, 0x0;
	mov.u32 %r4348, 0x0;
	ld.global.v4.b32 { %r4345, %r4346, %r4347, %r4348 }, [ %rd442 + 0 ];
	// end inline asm
	// begin inline asm
	mov.u32 %r4349, 0x0;
	mov.u32 %r4350, 0x0;
	mov.u32 %r4351, 0x0;
	mov.u32 %r4352, 0x0;
	ld.global.v4.b32 { %r4349, %r4350, %r4351, %r4352 }, [ %rd443 + 0 ];
	// end inline asm
	// begin inline asm
	mov.u32 %r4353, 0x0;
	mov.u32 %r4354, 0x0;
	mov.u32 %r4355, 0x0;
	mov.u32 %r4356, 0x0;
	ld.global.v4.b32 { %r4353, %r4354, %r4355, %r4356 }, [ %rd444 + 0 ];
	// end inline asm
	// begin inline asm
	mov.u32 %r4357, 0x0;
	mov.u32 %r4358, 0x0;
	mov.u32 %r4359, 0x0;
	mov.u32 %r4360, 0x0;
	ld.global.v4.b32 { %r4357, %r4358, %r4359, %r4360 }, [ %rd445 + 0 ];
	// end inline asm
	// begin inline asm
	mov.u32 %r4361, 0x0;
	mov.u32 %r4362, 0x0;
	mov.u32 %r4363, 0x0;
	mov.u32 %r4364, 0x0;
	ld.global.v4.b32 { %r4361, %r4362, %r4363, %r4364 }, [ %rd446 + 0 ];
	// end inline asm
	// begin inline asm
	mov.u32 %r4365, 0x0;
	mov.u32 %r4366, 0x0;
	mov.u32 %r4367, 0x0;
	mov.u32 %r4368, 0x0;
	ld.global.v4.b32 { %r4365, %r4366, %r4367, %r4368 }, [ %rd447 + 0 ];
	// end inline asm
	// begin inline asm
	mov.u32 %r4369, 0x0;
	mov.u32 %r4370, 0x0;
	mov.u32 %r4371, 0x0;
	mov.u32 %r4372, 0x0;
	ld.global.v4.b32 { %r4369, %r4370, %r4371, %r4372 }, [ %rd448 + 0 ];
	// end inline asm
	st.shared.v4.b32 	[%r16], {%r4341, %r4342, %r4343, %r4344};
	st.shared.v4.b32 	[%r16+2048], {%r4345, %r4346, %r4347, %r4348};
	st.shared.v4.b32 	[%r16+4096], {%r4349, %r4350, %r4351, %r4352};
	st.shared.v4.b32 	[%r16+6144], {%r4353, %r4354, %r4355, %r4356};
	st.shared.v4.b32 	[%r16+8192], {%r4357, %r4358, %r4359, %r4360};
	st.shared.v4.b32 	[%r16+10240], {%r4361, %r4362, %r4363, %r4364};
	st.shared.v4.b32 	[%r16+12288], {%r4365, %r4366, %r4367, %r4368};
	st.shared.v4.b32 	[%r16+14336], {%r4369, %r4370, %r4371, %r4372};
	mul.wide.s32 	%rd498, %r297, 4;
	add.s64 	%rd449, %rd6, %rd498;
	// begin inline asm
	mov.u32 %r4373, 0x0;
	ld.global.b32 { %r4373 }, [ %rd449 + 0 ];
	// end inline asm
	bar.sync 	0;
	and.b32 	%r4556, %r4450, 448;
	shr.u32 	%r4557, %r4437, 1;
	add.s32 	%r4558, %r1414, %r4556;
	add.s32 	%r4559, %r4558, %r4557;
	add.s32 	%r844, %r4559, %r4395;
	st.shared.b32 	[%r844], %r4373;
	bar.sync 	0;
	and.b32 	%r4560, %r13, 248;
	add.s32 	%r845, %r1414, %r4560;
	ld.shared.v2.b32 	{%r846, %r847}, [%r845];
	ld.shared.v2.b32 	{%r848, %r849}, [%r845+256];
	bar.sync 	0;
	st.shared.b32 	[%r844], %r4373;
	bar.sync 	0;
	ld.shared.b64 	%rd61, [%r845];
	ld.shared.b64 	%rd62, [%r845+256];
	add.s64 	%rd450, %rd7, %rd498;
	// begin inline asm
	mov.u32 %r850, 0x0;
	ld.global.b32 { %r850 }, [ %rd450 + 0 ];
	// end inline asm
	bar.sync 	0;
	st.shared.b32 	[%r844], %r850;
	bar.sync 	0;
	ld.shared.v2.b32 	{%r851, %r852}, [%r845];
	ld.shared.v2.b32 	{%r853, %r854}, [%r845+256];
	bar.sync 	0;
	mov.b32 	%r4376, 16;
	// begin inline asm
	cp.async.cg.shared.global [ %r5245 + 0 ], [ %rd451 + 0 ], 0x10, %r4376;
	// end inline asm
	cp.async.commit_group;
	// begin inline asm
	cp.async.cg.shared.global [ %r1336 + 0 ], [ %rd452 + 0 ], 0x10, %r4376;
	// end inline asm
	cp.async.commit_group;
	shl.b64 	%rd499, %rd22, 1;
	add.s64 	%rd453, %rd451, %rd499;
	add.s64 	%rd454, %rd452, %rd499;
	bar.sync 	0;
	// begin inline asm
	cp.async.cg.shared.global [ %r1342 + 0 ], [ %rd453 + 0 ], 0x10, %r4376;
	// end inline asm
	cp.async.commit_group;
	// begin inline asm
	cp.async.cg.shared.global [ %r1348 + 0 ], [ %rd454 + 0 ], 0x10, %r4376;
	// end inline asm
	cp.async.commit_group;
	add.s64 	%rd455, %rd453, %rd499;
	add.s64 	%rd456, %rd454, %rd499;
	bar.sync 	0;
	// begin inline asm
	cp.async.cg.shared.global [ %r1354 + 0 ], [ %rd455 + 0 ], 0x10, %r4376;
	// end inline asm
	cp.async.commit_group;
	// begin inline asm
	cp.async.cg.shared.global [ %r1360 + 0 ], [ %rd456 + 0 ], 0x10, %r4376;
	// end inline asm
	cp.async.commit_group;
	add.s64 	%rd457, %rd455, %rd499;
	add.s64 	%rd458, %rd456, %rd499;
	bar.sync 	0;
	// begin inline asm
	cp.async.cg.shared.global [ %r1366 + 0 ], [ %rd457 + 0 ], 0x10, %r4376;
	// end inline asm
	cp.async.commit_group;
	// begin inline asm
	cp.async.cg.shared.global [ %r1372 + 0 ], [ %rd458 + 0 ], 0x10, %r4376;
	// end inline asm
	cp.async.commit_group;
	// begin inline asm
	fence.proxy.async.shared::cta;
	// end inline asm
	mov.b32 	%r7086, 0f00000000;
	mov.b32 	%r7018, 3;
	mov.b32 	%r7017, -1;
	mov.b32 	%r4712, 0;
	mov.b32 	%r7016, %r4712;
	mov.b32 	%r7087, %r7086;
	mov.b32 	%r7088, %r7086;
	mov.b32 	%r7089, %r7086;
	mov.b32 	%r7090, %r7086;
	mov.b32 	%r7091, %r7086;
	mov.b32 	%r7092, %r7086;
	mov.b32 	%r7093, %r7086;
	mov.b32 	%r7094, %r7086;
	mov.b32 	%r7095, %r7086;
	mov.b32 	%r7096, %r7086;
	mov.b32 	%r7097, %r7086;
	mov.b32 	%r7098, %r7086;
	mov.b32 	%r7099, %r7086;
	mov.b32 	%r7100, %r7086;
	mov.b32 	%r7101, %r7086;
	mov.b32 	%r7102, %r7086;
	mov.b32 	%r7103, %r7086;
	mov.b32 	%r7104, %r7086;
	mov.b32 	%r7105, %r7086;
	mov.b32 	%r7106, %r7086;
	mov.b32 	%r7107, %r7086;
	mov.b32 	%r7108, %r7086;
	mov.b32 	%r7109, %r7086;
	mov.b32 	%r7110, %r7086;
	mov.b32 	%r7111, %r7086;
	mov.b32 	%r7112, %r7086;
	mov.b32 	%r7113, %r7086;
	mov.b32 	%r7114, %r7086;
	mov.b32 	%r7115, %r7086;
	mov.b32 	%r7116, %r7086;
	mov.b32 	%r7117, %r7086;
	mov.b32 	%r7118, %r7086;
	mov.b32 	%r7119, %r7086;
	mov.b32 	%r7120, %r7086;
	mov.b32 	%r7121, %r7086;
	mov.b32 	%r7122, %r7086;
	mov.b32 	%r7123, %r7086;
	mov.b32 	%r7124, %r7086;
	mov.b32 	%r7125, %r7086;
	mov.b32 	%r7126, %r7086;
	mov.b32 	%r7127, %r7086;
	mov.b32 	%r7128, %r7086;
	mov.b32 	%r7129, %r7086;
	mov.b32 	%r7130, %r7086;
	mov.b32 	%r7131, %r7086;
	mov.b32 	%r7132, %r7086;
	mov.b32 	%r7133, %r7086;
	mov.b32 	%r7134, %r7086;
	mov.b32 	%r7135, %r7086;
	mov.b32 	%r7136, %r7086;
	mov.b32 	%r7137, %r7086;
	mov.b32 	%r7138, %r7086;
	mov.b32 	%r7139, %r7086;
	mov.b32 	%r7140, %r7086;
	mov.b32 	%r7141, %r7086;
	mov.b32 	%r7142, %r7086;
	mov.b32 	%r7143, %r7086;
	mov.b32 	%r7144, %r7086;
	mov.b32 	%r7145, %r7086;
	mov.b32 	%r7146, %r7086;
	mov.b32 	%r7147, %r7086;
	mov.b32 	%r7148, %r7086;
	mov.b32 	%r7149, %r7086;
	mov.b32 	%r7083, %r4712;
$L__BB0_6:                              // %__nv_exp2f.exit729
                                        // =>This Inner Loop Header: Depth=1
	setp.lt.u32 	%p144, %r7083, 4;
	add.s32 	%r5013, %r7017, 1;
	setp.gt.s32 	%p145, %r5013, 4;
	selp.b32 	%r7017, 0, %r5013, %p145;
	cp.async.wait_group 	6;
	bar.sync 	0;
	shl.b32 	%r5014, %r7017, 11;
	add.s32 	%r4714, %r1414, %r5014;
	add.s32 	%r4848, %r4714, 10240;
	shfl.sync.idx.b32 	%r5016, %r3, 0, 31, -1;
	wgmma.fence.sync.aligned;
	bfe.u32 	%r5017, %r4714, 4, 14;
	cvt.u64.u32 	%rd536, %r5017;
	or.b64 	%rd501, %rd536, 4611686293313683456;
	// begin inline asm
	wgmma.mma_async.sync.aligned.m64n16k16.f32.f16.f16 {%r4585,%r4586,%r4587,%r4588,%r4589,%r4590,%r4591,%r4592}, %rd574, %rd501, 0, 1, 1, 0, 0;
	// end inline asm
	add.s32 	%r5018, %r4714, 32;
	bfe.u32 	%r5019, %r5018, 4, 14;
	cvt.u64.u32 	%rd537, %r5019;
	or.b64 	%rd503, %rd537, 4611686293313683456;
	mov.pred 	%p130, -1;
	// begin inline asm
	wgmma.mma_async.sync.aligned.m64n16k16.f32.f16.f16 {%r4585,%r4586,%r4587,%r4588,%r4589,%r4590,%r4591,%r4592}, %rd576, %rd503, %p130, 1, 1, 0, 0;
	// end inline asm
	add.s32 	%r5020, %r4714, 64;
	bfe.u32 	%r5021, %r5020, 4, 14;
	cvt.u64.u32 	%rd538, %r5021;
	or.b64 	%rd505, %rd538, 4611686293313683456;
	// begin inline asm
	wgmma.mma_async.sync.aligned.m64n16k16.f32.f16.f16 {%r4585,%r4586,%r4587,%r4588,%r4589,%r4590,%r4591,%r4592}, %rd578, %rd505, %p130, 1, 1, 0, 0;
	// end inline asm
	add.s32 	%r5022, %r4714, 96;
	bfe.u32 	%r5023, %r5022, 4, 14;
	cvt.u64.u32 	%rd539, %r5023;
	or.b64 	%rd507, %rd539, 4611686293313683456;
	// begin inline asm
	wgmma.mma_async.sync.aligned.m64n16k16.f32.f16.f16 {%r4585,%r4586,%r4587,%r4588,%r4589,%r4590,%r4591,%r4592}, %rd580, %rd507, %p130, 1, 1, 0, 0;
	// end inline asm
	// begin inline asm
	wgmma.mma_async.sync.aligned.m64n16k16.f32.f16.f16 {%r4641,%r4642,%r4643,%r4644,%r4645,%r4646,%r4647,%r4648}, %rd582, %rd501, 0, 1, 1, 0, 0;
	// end inline asm
	// begin inline asm
	wgmma.mma_async.sync.aligned.m64n16k16.f32.f16.f16 {%r4641,%r4642,%r4643,%r4644,%r4645,%r4646,%r4647,%r4648}, %rd584, %rd503, %p130, 1, 1, 0, 0;
	// end inline asm
	// begin inline asm
	wgmma.mma_async.sync.aligned.m64n16k16.f32.f16.f16 {%r4641,%r4642,%r4643,%r4644,%r4645,%r4646,%r4647,%r4648}, %rd586, %rd505, %p130, 1, 1, 0, 0;
	// end inline asm
	// begin inline asm
	wgmma.mma_async.sync.aligned.m64n16k16.f32.f16.f16 {%r4641,%r4642,%r4643,%r4644,%r4645,%r4646,%r4647,%r4648}, %rd588, %rd507, %p130, 1, 1, 0, 0;
	// end inline asm
	wgmma.commit_group.sync.aligned;
	mov.b32 	%r4690, %r4712;
	mov.b32 	%r4691, %r4712;
	mov.b32 	%r4693, %r4712;
	mov.b32 	%r4694, %r4712;
	mov.b32 	%r4692, %r4714;
	mov.b32 	%r4689, %r1415;
	// begin inline asm
	// wait for regs: %r4585,%r4586,%r4587,%r4588,%r4589,%r4590,%r4591,%r4592,%r4641,%r4642,%r4643,%r4644,%r4645,%r4646,%r4647,%r4648,%r4689,%r4690,%r4691,%r4692,%r4693,%r4694
	wgmma.wait_group.sync.aligned 0;
	// end inline asm
	sub.f32 	%r5024, %r4585, %r846;
	sub.f32 	%r5025, %r4586, %r846;
	sub.f32 	%r5026, %r4587, %r847;
	sub.f32 	%r5027, %r4588, %r847;
	sub.f32 	%r5028, %r4589, %r846;
	sub.f32 	%r5029, %r4590, %r846;
	sub.f32 	%r5030, %r4591, %r847;
	sub.f32 	%r5031, %r4592, %r847;
	sub.f32 	%r5032, %r4641, %r848;
	sub.f32 	%r5033, %r4642, %r848;
	sub.f32 	%r5034, %r4643, %r849;
	sub.f32 	%r5035, %r4644, %r849;
	sub.f32 	%r5036, %r4645, %r848;
	sub.f32 	%r5037, %r4646, %r848;
	sub.f32 	%r5038, %r4647, %r849;
	sub.f32 	%r5039, %r4648, %r849;
	ex2.approx.ftz.f32 	%r5040, %r5024;
	ex2.approx.ftz.f32 	%r5041, %r5025;
	ex2.approx.ftz.f32 	%r5042, %r5026;
	ex2.approx.ftz.f32 	%r5043, %r5027;
	ex2.approx.ftz.f32 	%r5044, %r5028;
	ex2.approx.ftz.f32 	%r5045, %r5029;
	ex2.approx.ftz.f32 	%r5046, %r5030;
	ex2.approx.ftz.f32 	%r5047, %r5031;
	ex2.approx.ftz.f32 	%r5048, %r5032;
	ex2.approx.ftz.f32 	%r5049, %r5033;
	ex2.approx.ftz.f32 	%r5050, %r5034;
	ex2.approx.ftz.f32 	%r5051, %r5035;
	ex2.approx.ftz.f32 	%r5052, %r5036;
	ex2.approx.ftz.f32 	%r5053, %r5037;
	ex2.approx.ftz.f32 	%r5054, %r5038;
	ex2.approx.ftz.f32 	%r5055, %r5039;
	add.s32 	%r5056, %r27, %r7016;
	add.s32 	%r5057, %r5056, 1;
	add.s32 	%r5058, %r5056, 8;
	add.s32 	%r5059, %r5056, 9;
	setp.lt.s32 	%p146, %r8, %r5056;
	setp.lt.s32 	%p147, %r8, %r5057;
	setp.lt.s32 	%p148, %r9, %r5056;
	setp.lt.s32 	%p149, %r9, %r5057;
	setp.lt.s32 	%p150, %r8, %r5058;
	setp.lt.s32 	%p151, %r8, %r5059;
	setp.lt.s32 	%p152, %r9, %r5058;
	setp.lt.s32 	%p153, %r9, %r5059;
	setp.lt.s32 	%p154, %r10, %r5056;
	setp.lt.s32 	%p155, %r10, %r5057;
	setp.lt.s32 	%p156, %r11, %r5056;
	setp.lt.s32 	%p157, %r11, %r5057;
	setp.lt.s32 	%p158, %r10, %r5058;
	setp.lt.s32 	%p159, %r10, %r5059;
	setp.lt.s32 	%p160, %r11, %r5058;
	setp.lt.s32 	%p161, %r11, %r5059;
	selp.f32 	%r5060, 0f00000000, %r5040, %p146;
	selp.f32 	%r5061, 0f00000000, %r5041, %p147;
	selp.f32 	%r5062, 0f00000000, %r5042, %p148;
	selp.f32 	%r5063, 0f00000000, %r5043, %p149;
	selp.f32 	%r5064, 0f00000000, %r5044, %p150;
	selp.f32 	%r5065, 0f00000000, %r5045, %p151;
	selp.f32 	%r5066, 0f00000000, %r5046, %p152;
	selp.f32 	%r5067, 0f00000000, %r5047, %p153;
	selp.f32 	%r5068, 0f00000000, %r5048, %p154;
	selp.f32 	%r5069, 0f00000000, %r5049, %p155;
	selp.f32 	%r5070, 0f00000000, %r5050, %p156;
	selp.f32 	%r5071, 0f00000000, %r5051, %p157;
	selp.f32 	%r5072, 0f00000000, %r5052, %p158;
	selp.f32 	%r5073, 0f00000000, %r5053, %p159;
	selp.f32 	%r5074, 0f00000000, %r5054, %p160;
	selp.f32 	%r5075, 0f00000000, %r5055, %p161;
	wgmma.fence.sync.aligned;
	bfe.u32 	%r5076, %r4848, 4, 14;
	cvt.u64.u32 	%rd540, %r5076;
	or.b64 	%rd517, %rd540, 4611686293313683456;
	// begin inline asm
	wgmma.mma_async.sync.aligned.m64n16k16.f32.f16.f16 {%r4741,%r4742,%r4743,%r4744,%r4745,%r4746,%r4747,%r4748}, %rd590, %rd517, 0, 1, 1, 0, 0;
	// end inline asm
	add.s32 	%r5077, %r4714, 10272;
	bfe.u32 	%r5078, %r5077, 4, 14;
	cvt.u64.u32 	%rd541, %r5078;
	or.b64 	%rd519, %rd541, 4611686293313683456;
	// begin inline asm
	wgmma.mma_async.sync.aligned.m64n16k16.f32.f16.f16 {%r4741,%r4742,%r4743,%r4744,%r4745,%r4746,%r4747,%r4748}, %rd592, %rd519, %p130, 1, 1, 0, 0;
	// end inline asm
	add.s32 	%r5079, %r4714, 10304;
	bfe.u32 	%r5080, %r5079, 4, 14;
	cvt.u64.u32 	%rd542, %r5080;
	or.b64 	%rd521, %rd542, 4611686293313683456;
	// begin inline asm
	wgmma.mma_async.sync.aligned.m64n16k16.f32.f16.f16 {%r4741,%r4742,%r4743,%r4744,%r4745,%r4746,%r4747,%r4748}, %rd594, %rd521, %p130, 1, 1, 0, 0;
	// end inline asm
	add.s32 	%r5081, %r4714, 10336;
	bfe.u32 	%r5082, %r5081, 4, 14;
	cvt.u64.u32 	%rd543, %r5082;
	or.b64 	%rd523, %rd543, 4611686293313683456;
	// begin inline asm
	wgmma.mma_async.sync.aligned.m64n16k16.f32.f16.f16 {%r4741,%r4742,%r4743,%r4744,%r4745,%r4746,%r4747,%r4748}, %rd596, %rd523, %p130, 1, 1, 0, 0;
	// end inline asm
	// begin inline asm
	wgmma.mma_async.sync.aligned.m64n16k16.f32.f16.f16 {%r4797,%r4798,%r4799,%r4800,%r4801,%r4802,%r4803,%r4804}, %rd598, %rd517, 0, 1, 1, 0, 0;
	// end inline asm
	// begin inline asm
	wgmma.mma_async.sync.aligned.m64n16k16.f32.f16.f16 {%r4797,%r4798,%r4799,%r4800,%r4801,%r4802,%r4803,%r4804}, %rd600, %rd519, %p130, 1, 1, 0, 0;
	// end inline asm
	// begin inline asm
	wgmma.mma_async.sync.aligned.m64n16k16.f32.f16.f16 {%r4797,%r4798,%r4799,%r4800,%r4801,%r4802,%r4803,%r4804}, %rd602, %rd521, %p130, 1, 1, 0, 0;
	// end inline asm
	// begin inline asm
	wgmma.mma_async.sync.aligned.m64n16k16.f32.f16.f16 {%r4797,%r4798,%r4799,%r4800,%r4801,%r4802,%r4803,%r4804}, %rd604, %rd523, %p130, 1, 1, 0, 0;
	// end inline asm
	wgmma.commit_group.sync.aligned;
	mov.b32 	%r4846, %r4712;
	mov.b32 	%r4847, %r4712;
	mov.b32 	%r4849, %r4712;
	mov.b32 	%r4850, %r4712;
	mov.b32 	%r4845, %r1416;
	// begin inline asm
	// wait for regs: %r4741,%r4742,%r4743,%r4744,%r4745,%r4746,%r4747,%r4748,%r4797,%r4798,%r4799,%r4800,%r4801,%r4802,%r4803,%r4804,%r4845,%r4846,%r4847,%r4848,%r4849,%r4850
	wgmma.wait_group.sync.aligned 0;
	// end inline asm
	sub.f32 	%r5083, %r4741, %r851;
	sub.f32 	%r5084, %r4742, %r851;
	sub.f32 	%r5085, %r4743, %r852;
	sub.f32 	%r5086, %r4744, %r852;
	sub.f32 	%r5087, %r4745, %r851;
	sub.f32 	%r5088, %r4746, %r851;
	sub.f32 	%r5089, %r4747, %r852;
	sub.f32 	%r5090, %r4748, %r852;
	sub.f32 	%r5091, %r4797, %r853;
	sub.f32 	%r5092, %r4798, %r853;
	sub.f32 	%r5093, %r4799, %r854;
	sub.f32 	%r5094, %r4800, %r854;
	sub.f32 	%r5095, %r4801, %r853;
	sub.f32 	%r5096, %r4802, %r853;
	sub.f32 	%r5097, %r4803, %r854;
	sub.f32 	%r5098, %r4804, %r854;
	mul.f32 	%r5099, %r5060, %r5083;
	mul.f32 	%r5100, %r5061, %r5084;
	mul.f32 	%r5101, %r5062, %r5085;
	mul.f32 	%r5102, %r5063, %r5086;
	mul.f32 	%r5103, %r5064, %r5087;
	mul.f32 	%r5104, %r5065, %r5088;
	mul.f32 	%r5105, %r5066, %r5089;
	mul.f32 	%r5106, %r5067, %r5090;
	mul.f32 	%r5107, %r5068, %r5091;
	mul.f32 	%r5108, %r5069, %r5092;
	mul.f32 	%r5109, %r5070, %r5093;
	mul.f32 	%r5110, %r5071, %r5094;
	mul.f32 	%r5111, %r5072, %r5095;
	mul.f32 	%r5112, %r5073, %r5096;
	mul.f32 	%r5113, %r5074, %r5097;
	mul.f32 	%r5114, %r5075, %r5098;
	cvt.rn.f16x2.f32 	%r4937, %r5100, %r5099;
	cvt.rn.f16x2.f32 	%r4938, %r5102, %r5101;
	cvt.rn.f16x2.f32 	%r4939, %r5104, %r5103;
	cvt.rn.f16x2.f32 	%r4940, %r5106, %r5105;
	cvt.rn.f16x2.f32 	%r5005, %r5108, %r5107;
	cvt.rn.f16x2.f32 	%r5006, %r5110, %r5109;
	cvt.rn.f16x2.f32 	%r5007, %r5112, %r5111;
	cvt.rn.f16x2.f32 	%r5008, %r5114, %r5113;
	wgmma.fence.sync.aligned;
	// begin inline asm
	wgmma.mma_async.sync.aligned.m64n64k16.f32.f16.f16 {%r7086,%r7087,%r7088,%r7089,%r7090,%r7091,%r7092,%r7093,%r7094,%r7095,%r7096,%r7097,%r7098,%r7099,%r7100,%r7101,%r7102,%r7103,%r7104,%r7105,%r7106,%r7107,%r7108,%r7109,%r7110,%r7111,%r7112,%r7113,%r7114,%r7115,%r7116,%r7117}, {%r4937,%r4938,%r4939,%r4940}, %rd501, %p130, 1, 1, 1;
	// end inline asm
	// begin inline asm
	wgmma.mma_async.sync.aligned.m64n64k16.f32.f16.f16 {%r7118,%r7119,%r7120,%r7121,%r7122,%r7123,%r7124,%r7125,%r7126,%r7127,%r7128,%r7129,%r7130,%r7131,%r7132,%r7133,%r7134,%r7135,%r7136,%r7137,%r7138,%r7139,%r7140,%r7141,%r7142,%r7143,%r7144,%r7145,%r7146,%r7147,%r7148,%r7149}, {%r5005,%r5006,%r5007,%r5008}, %rd501, %p130, 1, 1, 1;
	// end inline asm
	wgmma.commit_group.sync.aligned;
	add.s64 	%rd534, %rd86, %rd657;
	add.s64 	%rd535, %rd87, %rd657;
	add.s32 	%r5115, %r7018, 1;
	setp.gt.s32 	%p162, %r5115, 4;
	selp.b32 	%r7018, 0, %r5115, %p162;
	bar.sync 	0;
	shl.b32 	%r5116, %r7018, 11;
	add.s32 	%r5009, %r5245, %r5116;
	selp.b32 	%r5010, 16, 0, %p144;
	// begin inline asm
	cp.async.cg.shared.global [ %r5009 + 0 ], [ %rd534 + 0 ], 0x10, %r5010;
	// end inline asm
	cp.async.commit_group;
	add.s32 	%r5011, %r1336, %r5116;
	// begin inline asm
	cp.async.cg.shared.global [ %r5011 + 0 ], [ %rd535 + 0 ], 0x10, %r5010;
	// end inline asm
	cp.async.commit_group;
	add.s32 	%r7083, %r7083, 1;
	add.s64 	%rd657, %rd657, %rd41;
	add.s32 	%r7016, %r7016, 16;
	setp.ne.b32 	%p163, %r7016, 128;
	@%p163 bra 	$L__BB0_6;
// %bb.7:
	cvt.u32.u64 	%r5277, %rd22;
	// begin inline asm
	// wait for regs: %r7086,%r7087,%r7088,%r7089,%r7090,%r7091,%r7092,%r7093,%r7094,%r7095,%r7096,%r7097,%r7098,%r7099,%r7100,%r7101,%r7102,%r7103,%r7104,%r7105,%r7106,%r7107,%r7108,%r7109,%r7110,%r7111,%r7112,%r7113,%r7114,%r7115,%r7116,%r7117,%r7118,%r7119,%r7120,%r7121,%r7122,%r7123,%r7124,%r7125,%r7126,%r7127,%r7128,%r7129,%r7130,%r7131,%r7132,%r7133,%r7134,%r7135,%r7136,%r7137,%r7138,%r7139,%r7140,%r7141,%r7142,%r7143,%r7144,%r7145,%r7146,%r7147,%r7148,%r7149
	wgmma.wait_group.sync.aligned 0;
	// end inline asm
	cp.async.wait_group 	0;
	bar.sync 	0;
	st.shared.b32 	[%r844], %r850;
	bar.sync 	0;
	ld.shared.v2.b32 	{%r995, %r996}, [%r845];
	ld.shared.v2.b32 	{%r997, %r998}, [%r845+256];
	shr.s32 	%r999, %r1, 5;
	mul.lo.s32 	%r5278, %r1264, %r4;
	add.s32 	%r5279, %r5278, %r5277;
	mul.wide.s32 	%rd560, %r5278, 2;
	add.s64 	%rd561, %rd3, %rd560;
	mul.wide.s32 	%rd562, %r5279, 2;
	add.s64 	%rd563, %rd3, %rd562;
	add.s64 	%rd544, %rd561, %rd313;
	add.s64 	%rd545, %rd563, %rd313;
	add.s64 	%rd565, %rd4, %rd560;
	add.s64 	%rd566, %rd4, %rd562;
	add.s64 	%rd546, %rd565, %rd313;
	add.s64 	%rd547, %rd566, %rd313;
	setp.lt.s32 	%p164, %r999, 1;
	setp.gt.s32 	%p165, %r999, 0;
	bar.sync 	0;
	selp.b32 	%r5246, 16, 0, %p165;
	// begin inline asm
	cp.async.cg.shared.global [ %r5245 + 0 ], [ %rd544 + 0 ], 0x10, %r5246;
	// end inline asm
	// begin inline asm
	cp.async.cg.shared.global [ %r1342 + 0 ], [ %rd545 + 0 ], 0x10, %r5246;
	// end inline asm
	cp.async.commit_group;
	// begin inline asm
	cp.async.cg.shared.global [ %r5249 + 0 ], [ %rd546 + 0 ], 0x10, %r5246;
	// end inline asm
	// begin inline asm
	cp.async.cg.shared.global [ %r5251 + 0 ], [ %rd547 + 0 ], 0x10, %r5246;
	// end inline asm
	cp.async.commit_group;
	setp.gt.s32 	%p166, %r999, 1;
	add.s64 	%rd548, %rd544, %rd651;
	add.s64 	%rd549, %rd545, %rd651;
	add.s64 	%rd550, %rd546, %rd651;
	add.s64 	%rd551, %rd547, %rd651;
	bar.sync 	0;
	selp.b32 	%r5254, 16, 0, %p166;
	// begin inline asm
	cp.async.cg.shared.global [ %r1354 + 0 ], [ %rd548 + 0 ], 0x10, %r5254;
	// end inline asm
	// begin inline asm
	cp.async.cg.shared.global [ %r5255 + 0 ], [ %rd549 + 0 ], 0x10, %r5254;
	// end inline asm
	cp.async.commit_group;
	// begin inline asm
	cp.async.cg.shared.global [ %r5257 + 0 ], [ %rd550 + 0 ], 0x10, %r5254;
	// end inline asm
	// begin inline asm
	cp.async.cg.shared.global [ %r5259 + 0 ], [ %rd551 + 0 ], 0x10, %r5254;
	// end inline asm
	cp.async.commit_group;
	setp.gt.s32 	%p167, %r999, 2;
	add.s64 	%rd552, %rd548, %rd651;
	add.s64 	%rd553, %rd549, %rd651;
	add.s64 	%rd554, %rd550, %rd651;
	add.s64 	%rd555, %rd551, %rd651;
	bar.sync 	0;
	selp.b32 	%r5262, 16, 0, %p167;
	// begin inline asm
	cp.async.cg.shared.global [ %r5261 + 0 ], [ %rd552 + 0 ], 0x10, %r5262;
	// end inline asm
	// begin inline asm
	cp.async.cg.shared.global [ %r1336 + 0 ], [ %rd553 + 0 ], 0x10, %r5262;
	// end inline asm
	cp.async.commit_group;
	// begin inline asm
	cp.async.cg.shared.global [ %r5265 + 0 ], [ %rd554 + 0 ], 0x10, %r5262;
	// end inline asm
	// begin inline asm
	cp.async.cg.shared.global [ %r5267 + 0 ], [ %rd555 + 0 ], 0x10, %r5262;
	// end inline asm
	cp.async.commit_group;
	setp.gt.s32 	%p168, %r999, 3;
	add.s64 	%rd556, %rd552, %rd651;
	add.s64 	%rd557, %rd553, %rd651;
	add.s64 	%rd558, %rd554, %rd651;
	add.s64 	%rd559, %rd555, %rd651;
	bar.sync 	0;
	selp.b32 	%r5270, 16, 0, %p168;
	// begin inline asm
	cp.async.cg.shared.global [ %r1348 + 0 ], [ %rd556 + 0 ], 0x10, %r5270;
	// end inline asm
	// begin inline asm
	cp.async.cg.shared.global [ %r5271 + 0 ], [ %rd557 + 0 ], 0x10, %r5270;
	// end inline asm
	cp.async.commit_group;
	// begin inline asm
	cp.async.cg.shared.global [ %r5273 + 0 ], [ %rd558 + 0 ], 0x10, %r5270;
	// end inline asm
	// begin inline asm
	cp.async.cg.shared.global [ %r5275 + 0 ], [ %rd559 + 0 ], 0x10, %r5270;
	// end inline asm
	cp.async.commit_group;
	// begin inline asm
	fence.proxy.async.shared::cta;
	// end inline asm
	@%p164 bra 	$L__BB0_10;
// %bb.8:                               // %.lr.ph1207
	mov.b64 	{%r991, %r992}, %rd61;
	mov.b64 	{%r993, %r994}, %rd62;
	cvt.s64.s32 	%rd65, %r5278;
	cvt.s64.s32 	%rd66, %r5279;
	add.s32 	%r1064, %r999, -4;
	mov.b64 	%rd67, {%r998, %r998};
	mov.b64 	%rd68, {%r997, %r997};
	mov.b64 	%rd69, {%r996, %r996};
	mov.b64 	%rd70, {%r995, %r995};
	add.s64 	%rd568, %rd66, %rd1;
	shl.b64 	%rd569, %rd568, 1;
	add.s64 	%rd570, %rd39, %rd569;
	add.s64 	%rd661, %rd87, %rd570;
	add.s64 	%rd571, %rd65, %rd1;
	shl.b64 	%rd572, %rd571, 1;
	add.s64 	%rd573, %rd39, %rd572;
	add.s64 	%rd660, %rd87, %rd573;
	add.s64 	%rd659, %rd86, %rd570;
	add.s64 	%rd658, %rd86, %rd573;
	mov.b32 	%r5578, 0;
	mov.b32 	%r7085, 3;
	mov.b32 	%r7084, -1;
	mov.b32 	%r7150, %r5578;
$L__BB0_9:                              // %__nv_exp2f.exit
                                        // =>This Inner Loop Header: Depth=1
	setp.lt.s32 	%p185, %r7150, %r1064;
	add.s32 	%r6163, %r7084, 1;
	setp.gt.s32 	%p186, %r6163, 4;
	selp.b32 	%r7084, 0, %r6163, %p186;
	cp.async.wait_group 	6;
	bar.sync 	0;
	shl.b32 	%r6164, %r7084, 12;
	add.s32 	%r5580, %r1414, %r6164;
	add.s32 	%r5842, %r2059, %r6164;
	shfl.sync.idx.b32 	%r6167, %r3, 0, 31, -1;
	wgmma.fence.sync.aligned;
	bfe.u32 	%r6168, %r5580, 4, 14;
	cvt.u64.u32 	%rd614, %r6168;
	or.b64 	%rd575, %rd614, 4611686293322072064;
	// begin inline asm
	wgmma.mma_async.sync.aligned.m64n32k16.f32.f16.f16 {%r5331,%r5332,%r5333,%r5334,%r5335,%r5336,%r5337,%r5338,%r5339,%r5340,%r5341,%r5342,%r5343,%r5344,%r5345,%r5346}, %rd574, %rd575, 0, 1, 1, 0, 0;
	// end inline asm
	add.s32 	%r6169, %r5580, 32;
	bfe.u32 	%r6170, %r6169, 4, 14;
	cvt.u64.u32 	%rd615, %r6170;
	or.b64 	%rd577, %rd615, 4611686293322072064;
	mov.pred 	%p169, -1;
	// begin inline asm
	wgmma.mma_async.sync.aligned.m64n32k16.f32.f16.f16 {%r5331,%r5332,%r5333,%r5334,%r5335,%r5336,%r5337,%r5338,%r5339,%r5340,%r5341,%r5342,%r5343,%r5344,%r5345,%r5346}, %rd576, %rd577, %p169, 1, 1, 0, 0;
	// end inline asm
	add.s32 	%r6171, %r5580, 64;
	bfe.u32 	%r6172, %r6171, 4, 14;
	cvt.u64.u32 	%rd616, %r6172;
	or.b64 	%rd579, %rd616, 4611686293322072064;
	// begin inline asm
	wgmma.mma_async.sync.aligned.m64n32k16.f32.f16.f16 {%r5331,%r5332,%r5333,%r5334,%r5335,%r5336,%r5337,%r5338,%r5339,%r5340,%r5341,%r5342,%r5343,%r5344,%r5345,%r5346}, %rd578, %rd579, %p169, 1, 1, 0, 0;
	// end inline asm
	add.s32 	%r6173, %r5580, 96;
	bfe.u32 	%r6174, %r6173, 4, 14;
	cvt.u64.u32 	%rd617, %r6174;
	or.b64 	%rd581, %rd617, 4611686293322072064;
	// begin inline asm
	wgmma.mma_async.sync.aligned.m64n32k16.f32.f16.f16 {%r5331,%r5332,%r5333,%r5334,%r5335,%r5336,%r5337,%r5338,%r5339,%r5340,%r5341,%r5342,%r5343,%r5344,%r5345,%r5346}, %rd580, %rd581, %p169, 1, 1, 0, 0;
	// end inline asm
	// begin inline asm
	wgmma.mma_async.sync.aligned.m64n32k16.f32.f16.f16 {%r5443,%r5444,%r5445,%r5446,%r5447,%r5448,%r5449,%r5450,%r5451,%r5452,%r5453,%r5454,%r5455,%r5456,%r5457,%r5458}, %rd582, %rd575, 0, 1, 1, 0, 0;
	// end inline asm
	// begin inline asm
	wgmma.mma_async.sync.aligned.m64n32k16.f32.f16.f16 {%r5443,%r5444,%r5445,%r5446,%r5447,%r5448,%r5449,%r5450,%r5451,%r5452,%r5453,%r5454,%r5455,%r5456,%r5457,%r5458}, %rd584, %rd577, %p169, 1, 1, 0, 0;
	// end inline asm
	// begin inline asm
	wgmma.mma_async.sync.aligned.m64n32k16.f32.f16.f16 {%r5443,%r5444,%r5445,%r5446,%r5447,%r5448,%r5449,%r5450,%r5451,%r5452,%r5453,%r5454,%r5455,%r5456,%r5457,%r5458}, %rd586, %rd579, %p169, 1, 1, 0, 0;
	// end inline asm
	// begin inline asm
	wgmma.mma_async.sync.aligned.m64n32k16.f32.f16.f16 {%r5443,%r5444,%r5445,%r5446,%r5447,%r5448,%r5449,%r5450,%r5451,%r5452,%r5453,%r5454,%r5455,%r5456,%r5457,%r5458}, %rd588, %rd581, %p169, 1, 1, 0, 0;
	// end inline asm
	wgmma.commit_group.sync.aligned;
	mov.b32 	%r5540, %r5578;
	mov.b32 	%r5541, %r5578;
	mov.b32 	%r5543, %r5578;
	mov.b32 	%r5544, %r5578;
	mov.b32 	%r5542, %r5580;
	mov.b32 	%r5539, %r1415;
	// begin inline asm
	// wait for regs: %r5331,%r5332,%r5333,%r5334,%r5335,%r5336,%r5337,%r5338,%r5339,%r5340,%r5341,%r5342,%r5343,%r5344,%r5345,%r5346,%r5443,%r5444,%r5445,%r5446,%r5447,%r5448,%r5449,%r5450,%r5451,%r5452,%r5453,%r5454,%r5455,%r5456,%r5457,%r5458,%r5539,%r5540,%r5541,%r5542,%r5543,%r5544
	wgmma.wait_group.sync.aligned 0;
	// end inline asm
	sub.f32 	%r6175, %r5331, %r991;
	sub.f32 	%r6176, %r5332, %r991;
	sub.f32 	%r6177, %r5333, %r992;
	sub.f32 	%r6178, %r5334, %r992;
	sub.f32 	%r6179, %r5335, %r991;
	sub.f32 	%r6180, %r5336, %r991;
	sub.f32 	%r6181, %r5337, %r992;
	sub.f32 	%r6182, %r5338, %r992;
	sub.f32 	%r6183, %r5339, %r991;
	sub.f32 	%r6184, %r5340, %r991;
	sub.f32 	%r6185, %r5341, %r992;
	sub.f32 	%r6186, %r5342, %r992;
	sub.f32 	%r6187, %r5343, %r991;
	sub.f32 	%r6188, %r5344, %r991;
	sub.f32 	%r6189, %r5345, %r992;
	sub.f32 	%r6190, %r5346, %r992;
	sub.f32 	%r6191, %r5443, %r993;
	sub.f32 	%r6192, %r5444, %r993;
	sub.f32 	%r6193, %r5445, %r994;
	sub.f32 	%r6194, %r5446, %r994;
	sub.f32 	%r6195, %r5447, %r993;
	sub.f32 	%r6196, %r5448, %r993;
	sub.f32 	%r6197, %r5449, %r994;
	sub.f32 	%r6198, %r5450, %r994;
	sub.f32 	%r6199, %r5451, %r993;
	sub.f32 	%r6200, %r5452, %r993;
	sub.f32 	%r6201, %r5453, %r994;
	sub.f32 	%r6202, %r5454, %r994;
	sub.f32 	%r6203, %r5455, %r993;
	sub.f32 	%r6204, %r5456, %r993;
	sub.f32 	%r6205, %r5457, %r994;
	sub.f32 	%r6206, %r5458, %r994;
	ex2.approx.ftz.f32 	%r6207, %r6175;
	ex2.approx.ftz.f32 	%r6208, %r6176;
	ex2.approx.ftz.f32 	%r6209, %r6177;
	ex2.approx.ftz.f32 	%r6210, %r6178;
	ex2.approx.ftz.f32 	%r6211, %r6179;
	ex2.approx.ftz.f32 	%r6212, %r6180;
	ex2.approx.ftz.f32 	%r6213, %r6181;
	ex2.approx.ftz.f32 	%r6214, %r6182;
	ex2.approx.ftz.f32 	%r6215, %r6183;
	ex2.approx.ftz.f32 	%r6216, %r6184;
	ex2.approx.ftz.f32 	%r6217, %r6185;
	ex2.approx.ftz.f32 	%r6218, %r6186;
	ex2.approx.ftz.f32 	%r6219, %r6187;
	ex2.approx.ftz.f32 	%r6220, %r6188;
	ex2.approx.ftz.f32 	%r6221, %r6189;
	ex2.approx.ftz.f32 	%r6222, %r6190;
	ex2.approx.ftz.f32 	%r6223, %r6191;
	ex2.approx.ftz.f32 	%r6224, %r6192;
	ex2.approx.ftz.f32 	%r6225, %r6193;
	ex2.approx.ftz.f32 	%r6226, %r6194;
	ex2.approx.ftz.f32 	%r6227, %r6195;
	ex2.approx.ftz.f32 	%r6228, %r6196;
	ex2.approx.ftz.f32 	%r6229, %r6197;
	ex2.approx.ftz.f32 	%r6230, %r6198;
	ex2.approx.ftz.f32 	%r6231, %r6199;
	ex2.approx.ftz.f32 	%r6232, %r6200;
	ex2.approx.ftz.f32 	%r6233, %r6201;
	ex2.approx.ftz.f32 	%r6234, %r6202;
	ex2.approx.ftz.f32 	%r6235, %r6203;
	ex2.approx.ftz.f32 	%r6236, %r6204;
	ex2.approx.ftz.f32 	%r6237, %r6205;
	ex2.approx.ftz.f32 	%r6238, %r6206;
	wgmma.fence.sync.aligned;
	bfe.u32 	%r6239, %r5842, 4, 14;
	cvt.u64.u32 	%rd618, %r6239;
	or.b64 	%rd591, %rd618, 4611686293322072064;
	// begin inline asm
	wgmma.mma_async.sync.aligned.m64n32k16.f32.f16.f16 {%r5631,%r5632,%r5633,%r5634,%r5635,%r5636,%r5637,%r5638,%r5639,%r5640,%r5641,%r5642,%r5643,%r5644,%r5645,%r5646}, %rd590, %rd591, 0, 1, 1, 0, 0;
	// end inline asm
	add.s32 	%r6240, %r5842, 32;
	bfe.u32 	%r6241, %r6240, 4, 14;
	cvt.u64.u32 	%rd619, %r6241;
	or.b64 	%rd593, %rd619, 4611686293322072064;
	// begin inline asm
	wgmma.mma_async.sync.aligned.m64n32k16.f32.f16.f16 {%r5631,%r5632,%r5633,%r5634,%r5635,%r5636,%r5637,%r5638,%r5639,%r5640,%r5641,%r5642,%r5643,%r5644,%r5645,%r5646}, %rd592, %rd593, %p169, 1, 1, 0, 0;
	// end inline asm
	add.s32 	%r6242, %r5842, 64;
	bfe.u32 	%r6243, %r6242, 4, 14;
	cvt.u64.u32 	%rd620, %r6243;
	or.b64 	%rd595, %rd620, 4611686293322072064;
	// begin inline asm
	wgmma.mma_async.sync.aligned.m64n32k16.f32.f16.f16 {%r5631,%r5632,%r5633,%r5634,%r5635,%r5636,%r5637,%r5638,%r5639,%r5640,%r5641,%r5642,%r5643,%r5644,%r5645,%r5646}, %rd594, %rd595, %p169, 1, 1, 0, 0;
	// end inline asm
	add.s32 	%r6244, %r5842, 96;
	bfe.u32 	%r6245, %r6244, 4, 14;
	cvt.u64.u32 	%rd621, %r6245;
	or.b64 	%rd597, %rd621, 4611686293322072064;
	// begin inline asm
	wgmma.mma_async.sync.aligned.m64n32k16.f32.f16.f16 {%r5631,%r5632,%r5633,%r5634,%r5635,%r5636,%r5637,%r5638,%r5639,%r5640,%r5641,%r5642,%r5643,%r5644,%r5645,%r5646}, %rd596, %rd597, %p169, 1, 1, 0, 0;
	// end inline asm
	// begin inline asm
	wgmma.mma_async.sync.aligned.m64n32k16.f32.f16.f16 {%r5743,%r5744,%r5745,%r5746,%r5747,%r5748,%r5749,%r5750,%r5751,%r5752,%r5753,%r5754,%r5755,%r5756,%r5757,%r5758}, %rd598, %rd591, 0, 1, 1, 0, 0;
	// end inline asm
	// begin inline asm
	wgmma.mma_async.sync.aligned.m64n32k16.f32.f16.f16 {%r5743,%r5744,%r5745,%r5746,%r5747,%r5748,%r5749,%r5750,%r5751,%r5752,%r5753,%r5754,%r5755,%r5756,%r5757,%r5758}, %rd600, %rd593, %p169, 1, 1, 0, 0;
	// end inline asm
	// begin inline asm
	wgmma.mma_async.sync.aligned.m64n32k16.f32.f16.f16 {%r5743,%r5744,%r5745,%r5746,%r5747,%r5748,%r5749,%r5750,%r5751,%r5752,%r5753,%r5754,%r5755,%r5756,%r5757,%r5758}, %rd602, %rd595, %p169, 1, 1, 0, 0;
	// end inline asm
	// begin inline asm
	wgmma.mma_async.sync.aligned.m64n32k16.f32.f16.f16 {%r5743,%r5744,%r5745,%r5746,%r5747,%r5748,%r5749,%r5750,%r5751,%r5752,%r5753,%r5754,%r5755,%r5756,%r5757,%r5758}, %rd604, %rd597, %p169, 1, 1, 0, 0;
	// end inline asm
	wgmma.commit_group.sync.aligned;
	mov.b32 	%r5844, %r5578;
	mov.b32 	%r5840, %r5578;
	mov.b32 	%r5841, %r5578;
	mov.b32 	%r5843, %r5578;
	mov.b32 	%r5839, %r1416;
	// begin inline asm
	// wait for regs: %r5631,%r5632,%r5633,%r5634,%r5635,%r5636,%r5637,%r5638,%r5639,%r5640,%r5641,%r5642,%r5643,%r5644,%r5645,%r5646,%r5743,%r5744,%r5745,%r5746,%r5747,%r5748,%r5749,%r5750,%r5751,%r5752,%r5753,%r5754,%r5755,%r5756,%r5757,%r5758,%r5839,%r5840,%r5841,%r5842,%r5843,%r5844
	wgmma.wait_group.sync.aligned 0;
	// end inline asm
	mov.b64 	{%r6246, %r6247}, %rd70;
	sub.f32 	%r6248, %r5632, %r6247;
	sub.f32 	%r6249, %r5631, %r6246;
	mul.f32 	%r6250, %r6207, %r6249;
	mul.f32 	%r6251, %r6208, %r6248;
	cvt.rn.f16x2.f32 	%r5947, %r6251, %r6250;
	mov.b64 	{%r6252, %r6253}, %rd69;
	sub.f32 	%r6254, %r5634, %r6253;
	sub.f32 	%r6255, %r5633, %r6252;
	mul.f32 	%r6256, %r6209, %r6255;
	mul.f32 	%r6257, %r6210, %r6254;
	cvt.rn.f16x2.f32 	%r5948, %r6257, %r6256;
	sub.f32 	%r6258, %r5636, %r6247;
	sub.f32 	%r6259, %r5635, %r6246;
	mul.f32 	%r6260, %r6211, %r6259;
	mul.f32 	%r6261, %r6212, %r6258;
	cvt.rn.f16x2.f32 	%r5949, %r6261, %r6260;
	sub.f32 	%r6262, %r5638, %r6253;
	sub.f32 	%r6263, %r5637, %r6252;
	mul.f32 	%r6264, %r6213, %r6263;
	mul.f32 	%r6265, %r6214, %r6262;
	cvt.rn.f16x2.f32 	%r5950, %r6265, %r6264;
	sub.f32 	%r6266, %r5640, %r6247;
	sub.f32 	%r6267, %r5639, %r6246;
	mul.f32 	%r6268, %r6215, %r6267;
	mul.f32 	%r6269, %r6216, %r6266;
	cvt.rn.f16x2.f32 	%r6015, %r6269, %r6268;
	sub.f32 	%r6270, %r5642, %r6253;
	sub.f32 	%r6271, %r5641, %r6252;
	mul.f32 	%r6272, %r6217, %r6271;
	mul.f32 	%r6273, %r6218, %r6270;
	cvt.rn.f16x2.f32 	%r6016, %r6273, %r6272;
	sub.f32 	%r6274, %r5644, %r6247;
	sub.f32 	%r6275, %r5643, %r6246;
	mul.f32 	%r6276, %r6219, %r6275;
	mul.f32 	%r6277, %r6220, %r6274;
	cvt.rn.f16x2.f32 	%r6017, %r6277, %r6276;
	sub.f32 	%r6278, %r5646, %r6253;
	sub.f32 	%r6279, %r5645, %r6252;
	mul.f32 	%r6280, %r6221, %r6279;
	mul.f32 	%r6281, %r6222, %r6278;
	cvt.rn.f16x2.f32 	%r6018, %r6281, %r6280;
	mov.b64 	{%r6282, %r6283}, %rd68;
	sub.f32 	%r6284, %r5744, %r6283;
	sub.f32 	%r6285, %r5743, %r6282;
	mul.f32 	%r6286, %r6223, %r6285;
	mul.f32 	%r6287, %r6224, %r6284;
	cvt.rn.f16x2.f32 	%r6083, %r6287, %r6286;
	mov.b64 	{%r6288, %r6289}, %rd67;
	sub.f32 	%r6290, %r5746, %r6289;
	sub.f32 	%r6291, %r5745, %r6288;
	mul.f32 	%r6292, %r6225, %r6291;
	mul.f32 	%r6293, %r6226, %r6290;
	cvt.rn.f16x2.f32 	%r6084, %r6293, %r6292;
	sub.f32 	%r6294, %r5748, %r6283;
	sub.f32 	%r6295, %r5747, %r6282;
	mul.f32 	%r6296, %r6227, %r6295;
	mul.f32 	%r6297, %r6228, %r6294;
	cvt.rn.f16x2.f32 	%r6085, %r6297, %r6296;
	sub.f32 	%r6298, %r5750, %r6289;
	sub.f32 	%r6299, %r5749, %r6288;
	mul.f32 	%r6300, %r6229, %r6299;
	mul.f32 	%r6301, %r6230, %r6298;
	cvt.rn.f16x2.f32 	%r6086, %r6301, %r6300;
	sub.f32 	%r6302, %r5752, %r6283;
	sub.f32 	%r6303, %r5751, %r6282;
	mul.f32 	%r6304, %r6231, %r6303;
	mul.f32 	%r6305, %r6232, %r6302;
	cvt.rn.f16x2.f32 	%r6151, %r6305, %r6304;
	sub.f32 	%r6306, %r5754, %r6289;
	sub.f32 	%r6307, %r5753, %r6288;
	mul.f32 	%r6308, %r6233, %r6307;
	mul.f32 	%r6309, %r6234, %r6306;
	cvt.rn.f16x2.f32 	%r6152, %r6309, %r6308;
	sub.f32 	%r6310, %r5756, %r6283;
	sub.f32 	%r6311, %r5755, %r6282;
	mul.f32 	%r6312, %r6235, %r6311;
	mul.f32 	%r6313, %r6236, %r6310;
	cvt.rn.f16x2.f32 	%r6153, %r6313, %r6312;
	sub.f32 	%r6314, %r5758, %r6289;
	sub.f32 	%r6315, %r5757, %r6288;
	mul.f32 	%r6316, %r6237, %r6315;
	mul.f32 	%r6317, %r6238, %r6314;
	cvt.rn.f16x2.f32 	%r6154, %r6317, %r6316;
	wgmma.fence.sync.aligned;
	// begin inline asm
	wgmma.mma_async.sync.aligned.m64n64k16.f32.f16.f16 {%r7086,%r7087,%r7088,%r7089,%r7090,%r7091,%r7092,%r7093,%r7094,%r7095,%r7096,%r7097,%r7098,%r7099,%r7100,%r7101,%r7102,%r7103,%r7104,%r7105,%r7106,%r7107,%r7108,%r7109,%r7110,%r7111,%r7112,%r7113,%r7114,%r7115,%r7116,%r7117}, {%r5947,%r5948,%r5949,%r5950}, %rd575, %p169, 1, 1, 1;
	// end inline asm
	add.s32 	%r6318, %r5580, 2048;
	bfe.u32 	%r6319, %r6318, 4, 14;
	cvt.u64.u32 	%rd622, %r6319;
	or.b64 	%rd607, %rd622, 4611686293322072064;
	// begin inline asm
	wgmma.mma_async.sync.aligned.m64n64k16.f32.f16.f16 {%r7086,%r7087,%r7088,%r7089,%r7090,%r7091,%r7092,%r7093,%r7094,%r7095,%r7096,%r7097,%r7098,%r7099,%r7100,%r7101,%r7102,%r7103,%r7104,%r7105,%r7106,%r7107,%r7108,%r7109,%r7110,%r7111,%r7112,%r7113,%r7114,%r7115,%r7116,%r7117}, {%r6015,%r6016,%r6017,%r6018}, %rd607, %p169, 1, 1, 1;
	// end inline asm
	// begin inline asm
	wgmma.mma_async.sync.aligned.m64n64k16.f32.f16.f16 {%r7118,%r7119,%r7120,%r7121,%r7122,%r7123,%r7124,%r7125,%r7126,%r7127,%r7128,%r7129,%r7130,%r7131,%r7132,%r7133,%r7134,%r7135,%r7136,%r7137,%r7138,%r7139,%r7140,%r7141,%r7142,%r7143,%r7144,%r7145,%r7146,%r7147,%r7148,%r7149}, {%r6083,%r6084,%r6085,%r6086}, %rd575, %p169, 1, 1, 1;
	// end inline asm
	// begin inline asm
	wgmma.mma_async.sync.aligned.m64n64k16.f32.f16.f16 {%r7118,%r7119,%r7120,%r7121,%r7122,%r7123,%r7124,%r7125,%r7126,%r7127,%r7128,%r7129,%r7130,%r7131,%r7132,%r7133,%r7134,%r7135,%r7136,%r7137,%r7138,%r7139,%r7140,%r7141,%r7142,%r7143,%r7144,%r7145,%r7146,%r7147,%r7148,%r7149}, {%r6151,%r6152,%r6153,%r6154}, %rd607, %p169, 1, 1, 1;
	// end inline asm
	wgmma.commit_group.sync.aligned;
	add.s64 	%rd610, %rd658, %rd650;
	add.s64 	%rd611, %rd659, %rd650;
	add.s64 	%rd612, %rd660, %rd650;
	add.s64 	%rd613, %rd661, %rd650;
	add.s32 	%r6320, %r7085, 1;
	setp.gt.s32 	%p187, %r6320, 4;
	selp.b32 	%r7085, 0, %r6320, %p187;
	shl.b32 	%r6321, %r7085, 12;
	add.s32 	%r6322, %r1414, %r6321;
	bar.sync 	0;
	add.s32 	%r6155, %r6322, %r14;
	selp.b32 	%r6156, 16, 0, %p185;
	// begin inline asm
	cp.async.cg.shared.global [ %r6155 + 0 ], [ %rd610 + 0 ], 0x10, %r6156;
	// end inline asm
	add.s32 	%r6157, %r6155, 2048;
	// begin inline asm
	cp.async.cg.shared.global [ %r6157 + 0 ], [ %rd611 + 0 ], 0x10, %r6156;
	// end inline asm
	cp.async.commit_group;
	add.s32 	%r6323, %r2059, %r6321;
	add.s32 	%r6159, %r6323, %r14;
	// begin inline asm
	cp.async.cg.shared.global [ %r6159 + 0 ], [ %rd612 + 0 ], 0x10, %r6156;
	// end inline asm
	add.s32 	%r6161, %r6159, 2048;
	// begin inline asm
	cp.async.cg.shared.global [ %r6161 + 0 ], [ %rd613 + 0 ], 0x10, %r6156;
	// end inline asm
	cp.async.commit_group;
	add.s32 	%r7150, %r7150, 1;
	add.s64 	%rd661, %rd661, %rd651;
	add.s64 	%rd660, %rd660, %rd651;
	add.s64 	%rd659, %rd659, %rd651;
	add.s64 	%rd658, %rd658, %rd651;
	setp.ne.b32 	%p188, %r999, %r7150;
	@%p188 bra 	$L__BB0_9;
$L__BB0_10:                             // %._crit_edge1208
	add.s64 	%rd632, %rd89, %rd308;
	// begin inline asm
	// wait for regs: %r7086,%r7087,%r7088,%r7089,%r7090,%r7091,%r7092,%r7093,%r7094,%r7095,%r7096,%r7097,%r7098,%r7099,%r7100,%r7101,%r7102,%r7103,%r7104,%r7105,%r7106,%r7107,%r7108,%r7109,%r7110,%r7111,%r7112,%r7113,%r7114,%r7115,%r7116,%r7117,%r7118,%r7119,%r7120,%r7121,%r7122,%r7123,%r7124,%r7125,%r7126,%r7127,%r7128,%r7129,%r7130,%r7131,%r7132,%r7133,%r7134,%r7135,%r7136,%r7137,%r7138,%r7139,%r7140,%r7141,%r7142,%r7143,%r7144,%r7145,%r7146,%r7147,%r7148,%r7149
	wgmma.wait_group.sync.aligned 0;
	// end inline asm
	cp.async.wait_group 	0;
	bar.sync 	0;
	add.s64 	%rd634, %rd632, %rd459;
	add.s64 	%rd636, %rd632, %rd461;
	add.s64 	%rd638, %rd632, %rd463;
	add.s64 	%rd640, %rd632, %rd465;
	add.s64 	%rd642, %rd632, %rd467;
	add.s64 	%rd644, %rd632, %rd469;
	add.s64 	%rd646, %rd632, %rd471;
	add.s64 	%rd648, %rd632, %rd473;
	add.s64 	%rd623, %rd634, %rd313;
	add.s64 	%rd624, %rd636, %rd313;
	add.s64 	%rd625, %rd638, %rd313;
	add.s64 	%rd626, %rd640, %rd313;
	add.s64 	%rd627, %rd642, %rd313;
	add.s64 	%rd628, %rd644, %rd313;
	add.s64 	%rd629, %rd646, %rd313;
	add.s64 	%rd630, %rd648, %rd313;
	mul.f32 	%r6524, %r7086, 0f3F317218;
	mul.f32 	%r6525, %r7087, 0f3F317218;
	cvt.rn.f16x2.f32 	%r6526, %r6525, %r6524;
	mul.f32 	%r6527, %r7088, 0f3F317218;
	mul.f32 	%r6528, %r7089, 0f3F317218;
	cvt.rn.f16x2.f32 	%r6529, %r6528, %r6527;
	mul.f32 	%r6530, %r7090, 0f3F317218;
	mul.f32 	%r6531, %r7091, 0f3F317218;
	cvt.rn.f16x2.f32 	%r6532, %r6531, %r6530;
	mul.f32 	%r6533, %r7092, 0f3F317218;
	mul.f32 	%r6534, %r7093, 0f3F317218;
	cvt.rn.f16x2.f32 	%r6535, %r6534, %r6533;
	mul.f32 	%r6536, %r7094, 0f3F317218;
	mul.f32 	%r6537, %r7095, 0f3F317218;
	cvt.rn.f16x2.f32 	%r6538, %r6537, %r6536;
	mul.f32 	%r6539, %r7096, 0f3F317218;
	mul.f32 	%r6540, %r7097, 0f3F317218;
	cvt.rn.f16x2.f32 	%r6541, %r6540, %r6539;
	mul.f32 	%r6542, %r7098, 0f3F317218;
	mul.f32 	%r6543, %r7099, 0f3F317218;
	cvt.rn.f16x2.f32 	%r6544, %r6543, %r6542;
	mul.f32 	%r6545, %r7100, 0f3F317218;
	mul.f32 	%r6546, %r7101, 0f3F317218;
	cvt.rn.f16x2.f32 	%r6547, %r6546, %r6545;
	mul.f32 	%r6548, %r7102, 0f3F317218;
	mul.f32 	%r6549, %r7103, 0f3F317218;
	cvt.rn.f16x2.f32 	%r6550, %r6549, %r6548;
	mul.f32 	%r6551, %r7104, 0f3F317218;
	mul.f32 	%r6552, %r7105, 0f3F317218;
	cvt.rn.f16x2.f32 	%r6553, %r6552, %r6551;
	mul.f32 	%r6554, %r7106, 0f3F317218;
	mul.f32 	%r6555, %r7107, 0f3F317218;
	cvt.rn.f16x2.f32 	%r6556, %r6555, %r6554;
	mul.f32 	%r6557, %r7108, 0f3F317218;
	mul.f32 	%r6558, %r7109, 0f3F317218;
	cvt.rn.f16x2.f32 	%r6559, %r6558, %r6557;
	mul.f32 	%r6560, %r7110, 0f3F317218;
	mul.f32 	%r6561, %r7111, 0f3F317218;
	cvt.rn.f16x2.f32 	%r6562, %r6561, %r6560;
	mul.f32 	%r6563, %r7112, 0f3F317218;
	mul.f32 	%r6564, %r7113, 0f3F317218;
	cvt.rn.f16x2.f32 	%r6565, %r6564, %r6563;
	mul.f32 	%r6566, %r7114, 0f3F317218;
	mul.f32 	%r6567, %r7115, 0f3F317218;
	cvt.rn.f16x2.f32 	%r6568, %r6567, %r6566;
	mul.f32 	%r6569, %r7116, 0f3F317218;
	mul.f32 	%r6570, %r7117, 0f3F317218;
	cvt.rn.f16x2.f32 	%r6571, %r6570, %r6569;
	mul.f32 	%r6572, %r7118, 0f3F317218;
	mul.f32 	%r6573, %r7119, 0f3F317218;
	cvt.rn.f16x2.f32 	%r6574, %r6573, %r6572;
	mul.f32 	%r6575, %r7120, 0f3F317218;
	mul.f32 	%r6576, %r7121, 0f3F317218;
	cvt.rn.f16x2.f32 	%r6577, %r6576, %r6575;
	mul.f32 	%r6578, %r7122, 0f3F317218;
	mul.f32 	%r6579, %r7123, 0f3F317218;
	cvt.rn.f16x2.f32 	%r6580, %r6579, %r6578;
	mul.f32 	%r6581, %r7124, 0f3F317218;
	mul.f32 	%r6582, %r7125, 0f3F317218;
	cvt.rn.f16x2.f32 	%r6583, %r6582, %r6581;
	mul.f32 	%r6584, %r7126, 0f3F317218;
	mul.f32 	%r6585, %r7127, 0f3F317218;
	cvt.rn.f16x2.f32 	%r6586, %r6585, %r6584;
	mul.f32 	%r6587, %r7128, 0f3F317218;
	mul.f32 	%r6588, %r7129, 0f3F317218;
	cvt.rn.f16x2.f32 	%r6589, %r6588, %r6587;
	mul.f32 	%r6590, %r7130, 0f3F317218;
	mul.f32 	%r6591, %r7131, 0f3F317218;
	cvt.rn.f16x2.f32 	%r6592, %r6591, %r6590;
	mul.f32 	%r6593, %r7132, 0f3F317218;
	mul.f32 	%r6594, %r7133, 0f3F317218;
	cvt.rn.f16x2.f32 	%r6595, %r6594, %r6593;
	mul.f32 	%r6596, %r7134, 0f3F317218;
	mul.f32 	%r6597, %r7135, 0f3F317218;
	cvt.rn.f16x2.f32 	%r6598, %r6597, %r6596;
	mul.f32 	%r6599, %r7136, 0f3F317218;
	mul.f32 	%r6600, %r7137, 0f3F317218;
	cvt.rn.f16x2.f32 	%r6601, %r6600, %r6599;
	mul.f32 	%r6602, %r7138, 0f3F317218;
	mul.f32 	%r6603, %r7139, 0f3F317218;
	cvt.rn.f16x2.f32 	%r6604, %r6603, %r6602;
	mul.f32 	%r6605, %r7140, 0f3F317218;
	mul.f32 	%r6606, %r7141, 0f3F317218;
	cvt.rn.f16x2.f32 	%r6607, %r6606, %r6605;
	mul.f32 	%r6608, %r7142, 0f3F317218;
	mul.f32 	%r6609, %r7143, 0f3F317218;
	cvt.rn.f16x2.f32 	%r6610, %r6609, %r6608;
	mul.f32 	%r6611, %r7144, 0f3F317218;
	mul.f32 	%r6612, %r7145, 0f3F317218;
	cvt.rn.f16x2.f32 	%r6613, %r6612, %r6611;
	mul.f32 	%r6614, %r7146, 0f3F317218;
	mul.f32 	%r6615, %r7147, 0f3F317218;
	cvt.rn.f16x2.f32 	%r6616, %r6615, %r6614;
	mul.f32 	%r6617, %r7148, 0f3F317218;
	mul.f32 	%r6618, %r7149, 0f3F317218;
	cvt.rn.f16x2.f32 	%r6619, %r6618, %r6617;
	st.shared.v4.b32 	[%r838], {%r6526, %r6532, %r6538, %r6544};
	st.shared.v4.b32 	[%r838+256], {%r6529, %r6535, %r6541, %r6547};
	st.shared.v4.b32 	[%r839], {%r6550, %r6556, %r6562, %r6568};
	st.shared.v4.b32 	[%r839+256], {%r6553, %r6559, %r6565, %r6571};
	bar.sync 	0;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r6492, %r6493, %r6494, %r6495}, [%r6476];
	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r6496, %r6497, %r6498, %r6499}, [%r6481];
	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r6500, %r6501, %r6502, %r6503}, [%r6486];
	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r6504, %r6505, %r6506, %r6507}, [%r6491];
	// end inline asm
	bar.sync 	0;
	st.shared.v4.b32 	[%r838], {%r6574, %r6580, %r6586, %r6592};
	st.shared.v4.b32 	[%r838+256], {%r6577, %r6583, %r6589, %r6595};
	st.shared.v4.b32 	[%r839], {%r6598, %r6604, %r6610, %r6616};
	st.shared.v4.b32 	[%r839+256], {%r6601, %r6607, %r6613, %r6619};
	bar.sync 	0;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r6508, %r6509, %r6510, %r6511}, [%r6476];
	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r6512, %r6513, %r6514, %r6515}, [%r6481];
	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r6516, %r6517, %r6518, %r6519}, [%r6486];
	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r6520, %r6521, %r6522, %r6523}, [%r6491];
	// end inline asm
	// begin inline asm
	st.global.v4.b32 [ %rd623 + 0 ], { %r6492, %r6493, %r6494, %r6495 };
	// end inline asm
	// begin inline asm
	st.global.v4.b32 [ %rd624 + 0 ], { %r6496, %r6497, %r6498, %r6499 };
	// end inline asm
	// begin inline asm
	st.global.v4.b32 [ %rd625 + 0 ], { %r6500, %r6501, %r6502, %r6503 };
	// end inline asm
	// begin inline asm
	st.global.v4.b32 [ %rd626 + 0 ], { %r6504, %r6505, %r6506, %r6507 };
	// end inline asm
	// begin inline asm
	st.global.v4.b32 [ %rd627 + 0 ], { %r6508, %r6509, %r6510, %r6511 };
	// end inline asm
	// begin inline asm
	st.global.v4.b32 [ %rd628 + 0 ], { %r6512, %r6513, %r6514, %r6515 };
	// end inline asm
	// begin inline asm
	st.global.v4.b32 [ %rd629 + 0 ], { %r6516, %r6517, %r6518, %r6519 };
	// end inline asm
	// begin inline asm
	st.global.v4.b32 [ %rd630 + 0 ], { %r6520, %r6521, %r6522, %r6523 };
	// end inline asm
	ret;
                                        // -- End function
}
